{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading solution (expected.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WESTCLIFF-ON-SEA</td>\n",
       "      <td>SS0_8HX</td>\n",
       "      <td>47_SECOND_AVENUE</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>1022119</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>10348000.00</td>\n",
       "      <td>9415000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHELTENHAM</td>\n",
       "      <td>GL50_3EP</td>\n",
       "      <td>BAYSHILL_ROAD</td>\n",
       "      <td>Cheltenham_Ladies_College</td>\n",
       "      <td>311722</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>32168000.00</td>\n",
       "      <td>27972000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHREWSBURY</td>\n",
       "      <td>SY3_7PQ</td>\n",
       "      <td>58_TRINITY_STREET</td>\n",
       "      <td>The_Sanata_Charitable_Trust</td>\n",
       "      <td>1132766</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>255653.00</td>\n",
       "      <td>258287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WARE</td>\n",
       "      <td>SG11_2DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>122836.00</td>\n",
       "      <td>124446.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Address (post town) Address (post code)   Address (street)  \\\n",
       "0            BROADWAY            WR12_7NL                NaN   \n",
       "1    WESTCLIFF-ON-SEA             SS0_8HX   47_SECOND_AVENUE   \n",
       "2          CHELTENHAM            GL50_3EP      BAYSHILL_ROAD   \n",
       "3          SHREWSBURY             SY3_7PQ  58_TRINITY_STREET   \n",
       "4                WARE            SG11_2DY                NaN   \n",
       "\n",
       "                  Charity Name Charity Number Period End Date Annual Income  \\\n",
       "0   Wormington_Village_Society        1155074      2018-07-31           NaN   \n",
       "1     Havens_Christian_Hospice        1022119      2016-03-31   10348000.00   \n",
       "2    Cheltenham_Ladies_College         311722      2016-07-31   32168000.00   \n",
       "3  The_Sanata_Charitable_Trust        1132766      2015-12-31     255653.00   \n",
       "4          Cantate_Youth_Choir        1039369      2013-12-31     122836.00   \n",
       "\n",
       "  Annual Spending  \n",
       "0             NaN  \n",
       "1      9415000.00  \n",
       "2     27972000.00  \n",
       "3       258287.00  \n",
       "4       124446.00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = pd.read_csv('datasets/kleister_charity/dev-0/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Address (street)\", \"Annual Income\",  \"Annual Spending\"]\n",
    "expected = expected[[\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Annual Income\", \"Annual Spending\"]]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of keys that actually have a value (are not NaN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Address (post town)    0.959091\n",
       "Address (post code)    0.968182\n",
       "Address (street)       0.886364\n",
       "Charity Name           1.000000\n",
       "Charity Number         0.993182\n",
       "Period End Date        1.000000\n",
       "Annual Income          0.986364\n",
       "Annual Spending        0.986364\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage of keys that actually have a value (are not NaN):\")\n",
    "expected.count() / len(expected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Predictions (single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>address__post_town=Wormington address__postcod...</td>\n",
       "      <td>Wormington</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>Dairymead</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>address__post_town=Westcliff-on-Sea address__p...</td>\n",
       "      <td>Westcliff-on-Sea</td>\n",
       "      <td>SSO_8HX</td>\n",
       "      <td>Stuart_House,_47_Second_Avenue</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>1089717</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>510348000.00</td>\n",
       "      <td>5882000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>address__post_town=Cheltenham address__postcod...</td>\n",
       "      <td>Cheltenham</td>\n",
       "      <td>GL50_3EP</td>\n",
       "      <td>Bayshill_Road</td>\n",
       "      <td>College</td>\n",
       "      <td>311722</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>20221215.00</td>\n",
       "      <td>27972.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>address__post_town=Shrewsbury address__postcod...</td>\n",
       "      <td>Shrewsbury</td>\n",
       "      <td>SY3_7PQ</td>\n",
       "      <td>58_Trinity_Street,_Belle_Vue</td>\n",
       "      <td>The_Sanata_Charitable_Trust</td>\n",
       "      <td>1132766</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>27560607.00</td>\n",
       "      <td>258287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>address__post_town=Ware address__postcode=SG11...</td>\n",
       "      <td>Ware</td>\n",
       "      <td>SG11_2DY</td>\n",
       "      <td>Unit_5,_Hadham_Industrial_Estates_Ltd</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>122836.00</td>\n",
       "      <td>124446.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw Address (post town)  \\\n",
       "0  address__post_town=Wormington address__postcod...          Wormington   \n",
       "1  address__post_town=Westcliff-on-Sea address__p...    Westcliff-on-Sea   \n",
       "2  address__post_town=Cheltenham address__postcod...          Cheltenham   \n",
       "3  address__post_town=Shrewsbury address__postcod...          Shrewsbury   \n",
       "4  address__post_town=Ware address__postcode=SG11...                Ware   \n",
       "\n",
       "  Address (post code)                       Address (street)  \\\n",
       "0            WR12_7NL                              Dairymead   \n",
       "1             SSO_8HX         Stuart_House,_47_Second_Avenue   \n",
       "2            GL50_3EP                          Bayshill_Road   \n",
       "3             SY3_7PQ           58_Trinity_Street,_Belle_Vue   \n",
       "4            SG11_2DY  Unit_5,_Hadham_Industrial_Estates_Ltd   \n",
       "\n",
       "                  Charity Name Charity Number Period End Date Annual Income  \\\n",
       "0   Wormington_Village_Society        1155074      2018-07-31           NaN   \n",
       "1     Havens_Christian_Hospice        1089717      2016-03-31  510348000.00   \n",
       "2                      College         311722      2016-07-31   20221215.00   \n",
       "3  The_Sanata_Charitable_Trust        1132766      2015-12-31   27560607.00   \n",
       "4          Cantate_Youth_Choir        1039369      2013-12-31     122836.00   \n",
       "\n",
       "  Annual Spending  \n",
       "0             NaN  \n",
       "1      5882000.00  \n",
       "2        27972.00  \n",
       "3       258287.00  \n",
       "4       124446.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.read_csv('datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-36-13_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in predictions[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        predictions.loc[predictions[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "predictions.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Annual Income\",  \"Annual Spending\"]\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.drop(columns=[\"raw\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Log of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(\"../logs/davinci/2022-12-15T16-36-05_davinci_temp_0.log\", \"r\")\n",
    "log_lines = [line.strip() for line in log_file.readlines() if line.strip() and line.startswith(\"20\") and \"Raw value:\" not in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34999\n"
     ]
    }
   ],
   "source": [
    "print(len(log_lines))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations (answering RH1)\n",
    "## RH1\n",
    "> Unimodal approach can reach satisfactory performance while being more cost-efficient than current state-of-the-art multi-modal approaches\n",
    "\n",
    "Where we define satisfactory performance as:\n",
    "> 80% of the values for given and findable keys are correctly found (no distinction for the other 20%, they can be either wrong or missing (which is of course also wrong)). Correctness is defined as a case-insensitive (upper-casing everything) string match with some normalisation (details below)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations and evaluating according to own definition of \"correctness\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address (post town)\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "OTTERY_ST_MARY | Ottery_St._Mary\n",
    "Lichfield | City_of_Lichfield\n",
    "Liverpool | City_of_Liverpool\n",
    "\n",
    "Normalisation:\n",
    "*  `<Solution City>` vs. `City of <Solution City>` are both correct\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition, but not a substitution) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary)\n",
    "\n",
    "### Address (post code)\n",
    "NO NORMALISATION\n",
    "\n",
    "### Address (street)\n",
    "Examples: \n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "36_BELLINGHAM_DRIVE | Unit_36_Bellingham_Drive\n",
    "34_DECIMA_STREET | Sherborne_House,_34_Decima_Street\n",
    "190_LONG_LANE | Scout_Centre,_Rear_190_Long_Lane\n",
    "13_ROSSLYN_ROAD | Room_16,_ETNA_Community_Centre,_13_Rosslyn_Road\n",
    "FURNIVAL_GATE | 2_Floor,_Midcity_House,_Furnival_Gate\n",
    "7-14_Great_Dover_Street | 7_-_14_Great_Dover_Street\n",
    "BROWNBERRIE_LANE | Leeds_Trinity_University,_Brownberrie_Lane\n",
    "\n",
    "Normalisation: \n",
    "* Delete Spaces around \"-\" in both solution and prediction\n",
    "\n",
    "Was considering generally cutting off at ,_ before or after the street but ultimately decided against it because it cannot be generally stated that having something in front or after the correct street would still make mail arrive at the destination.\n",
    "\n",
    "Also: Levenshtein edit distance of 1 doesn't make sense here as getting the number wrong (e.g. 13 instead of 1) is a clear mistake.\n",
    "\n",
    "### Charity Name\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "Cheltenham Ladies College | Cheltenham Ladies' College\n",
    "Battersea_Dogs'_and_Cats'_Home | Battersea_Dogs'_&_Cats'_Home\n",
    "Beer_Shmuel_Ltd. | Beer_Shmuel_Limited\n",
    "Catch_22_Charity_Ltd. | Catch22\n",
    "Richard_Hicks | Richard_Hicks_Charity\n",
    "King's_Schools_Taunton_Ltd. | King's_Schools_Taunton_Limited\n",
    "KEY_ENTERPRISES_(1983)_LTD. | KEY_ENTERPRISES_(1983)_LIMITED\n",
    "Louth_Playgoers_Society_Ltd. | Louth_Playgoers_Society_Limited\n",
    "Boxgrove_Village_Hall_and_Community_Centre | BOXGROVE_VILLAGE_HALL_&_COMMUNITY_CENTRE_CIO\n",
    "London_Transport_Museum | London_Transport_Museum_Ltd.\n",
    "The_Momc-Leigh_Park_Crafts_Initiative_Trust_Ltd. | THE_MOMC_-_LEIGH_PARK_CRAFTS_INITIATIVE_TRUST_LIMITED\n",
    "King_Edward_Vi's_School_At_Chelmsford | King_Edward_VI_School_at_Chelmsford\n",
    "The_Hope_Foundation_Ltd. | The_Hope_Foundation\n",
    "Nottingham_Women's_Counselling_Service | The_Nottingham_Women's_Counselling_Service\n",
    "\n",
    "Normalisation (+ give stats for how many values this applies):\n",
    "* Cut off Ltd, Ltd. and Limited from the end of both prediction and solution \n",
    "* Replace \"&\" with \"and\" in both prediction and solution\n",
    "* Delete Spaces around \"-\" in both prediction and solution\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary, King_Edward_VI's_School vs. King_Edward_VI_School)\n",
    "\n",
    "\n",
    "### Charity Number\n",
    "NO NORMALISATION\n",
    "\n",
    "### Period End Date\n",
    "NO NORMALISATION\n",
    "\n",
    "### Annual Income\n",
    "NO NORMALISATION\n",
    "\n",
    "### Annual Spending\n",
    "NO NORMALISATION\n",
    "\n",
    "### Other Normalisations\n",
    "Replaced uncommon character: ’ (U+2019) with ' (in the predictions and the solution) (applies to 4 values in the whole solution of the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 5 quotation marks in predictions.\n",
      "Replaced 4 quotation marks in expected.\n"
     ]
    }
   ],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "count = replace_quotation_mark(predictions)\n",
    "print(f\"Replaced {count} quotation marks in predictions.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro) average (over the keys) of correctly predicted values: 0.658854468475961\n",
      "(micro) average (over all predictions) of correct values: 0.6624972943722944\n",
      "Own evaluation by key:\n",
      "Address (post town)    0.789100\n",
      "Address (post code)    0.673709\n",
      "Address (street)       0.369231\n",
      "Charity Name           0.725000\n",
      "Charity Number         0.823799\n",
      "Period End Date        0.956818\n",
      "Annual Income          0.456221\n",
      "Annual Spending        0.476959\n",
      "dtype: float64\n",
      "(official) (macro)[over keys] average  of correctly predicted values: 0.6360795454545455\n",
      "(official) (micro)[over all predictions] average of correctly predicted values: 0.6360795454545455\n",
      "Official evaluation by key:\n",
      "Address (post town)    0.752273\n",
      "Address (post code)    0.670455\n",
      "Address (street)       0.338636\n",
      "Charity Name           0.597727\n",
      "Charity Number         0.825000\n",
      "Period End Date        0.956818\n",
      "Annual Income          0.463636\n",
      "Annual Spending        0.484091\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "official_evaluation = pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) # empty dataframe with NaNs that will be filled with 0s (wrong) and 1s (correct), e.g. document 42, key \"Charity Name\": 1\n",
    "own_evaluation = pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) # empty dataframe with NaNs that will be filled with 0s (wrong) and 1s (correct), e.g. document 42, key \"Charity Name\": 1\n",
    "null_evaluation = pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) # empty dataframe with zeros that will be filled with the number of true positives, false positives, false negatives and true negatives *w.r.t. null's*, e.g. key \"Charity Name\": 42\n",
    "\n",
    "for index, row in expected.iterrows():\n",
    "    for column in expected.columns:\n",
    "        if pd.notnull(row[column]):\n",
    "            # FP: we predicted null and it was not null\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                null_evaluation.loc[\"FP\", column] += 1\n",
    "\n",
    "            # TN: we predicted not null and it was not null\n",
    "            else:\n",
    "                null_evaluation.loc[\"TN\", column] += 1\n",
    "            \n",
    "            if is_correct(column, row[column], predictions.loc[index, column]):\n",
    "                own_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                own_evaluation.loc[index, column] = 0\n",
    "\n",
    "            if str(row[column]).upper() == str(predictions.loc[index, column]).upper():\n",
    "                official_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                official_evaluation.loc[index, column] = 0\n",
    "\n",
    "        else:\n",
    "            # TP: we predicted null and it was null\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                null_evaluation.loc[\"TP\", column] += 1\n",
    "            # FN: we predicted not null and it was null\n",
    "            else:\n",
    "                null_evaluation.loc[\"FN\", column] += 1\n",
    "\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                official_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                official_evaluation.loc[index, column] = 0\n",
    "\n",
    "# only looks at the keys that are actually present in the document\n",
    "print(f\"(macro) average (over the keys) of correctly predicted values: {own_evaluation.mean(axis=0, skipna=True).mean()}\")\n",
    "print(f\"(micro) average (over all predictions) of correct values: {own_evaluation.mean(axis=1, skipna=True).mean()}\")\n",
    "\n",
    "print(f\"Own evaluation by key:\\n{own_evaluation.mean(axis=0, skipna=True)}\")\n",
    "\n",
    "# official evaluation\n",
    "print(f\"(official) (macro)[over keys] average  of correctly predicted values: {official_evaluation.mean(axis=0, skipna=True).mean()}\")\n",
    "print(f\"(official) (micro)[over all predictions] average of correctly predicted values: {official_evaluation.mean(axis=1, skipna=True).mean()}\")\n",
    "\n",
    "print(f\"Official evaluation by key:\\n{official_evaluation.mean(axis=0, skipna=True)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null evaluation:\n",
      "    Address (post town)  Address (post code)  Address (street)  Charity Name  \\\n",
      "TP                  1.0                  8.0               5.0           0.0   \n",
      "FP                  1.0                  5.0               6.0           1.0   \n",
      "FN                 17.0                  6.0              45.0           0.0   \n",
      "TN                421.0                421.0             384.0         439.0   \n",
      "\n",
      "    Charity Number  Period End Date  Annual Income  Annual Spending  \n",
      "TP             3.0              0.0            6.0              6.0  \n",
      "FP            10.0              2.0           15.0             15.0  \n",
      "FN             0.0              0.0            0.0              0.0  \n",
      "TN           427.0            438.0          419.0            419.0  \n",
      "(micro)[over all predictions] Precision for null: 0.34523809523809523\n",
      "(micro)[over all predictions] Recall for null: 0.29896907216494845\n",
      "(micro)[over all predictions] F1 score for null: 0.32044198895027626\n",
      "                     precision    recall        f1\n",
      "Address (post town)   0.500000  0.055556  0.100000\n",
      "Address (post code)   0.615385  0.571429  0.592593\n",
      "Address (street)      0.454545  0.100000  0.163934\n",
      "Charity Name          0.000000       NaN       NaN\n",
      "Charity Number        0.230769  1.000000  0.375000\n",
      "Period End Date       0.000000       NaN       NaN\n",
      "Annual Income         0.285714  1.000000  0.444444\n",
      "Annual Spending       0.285714  1.000000  0.444444\n",
      "(macro)[over the keys] Precision for null: 0.296515984015984\n",
      "(macro)[over the keys] Recall for null: 0.6211640211640211\n",
      "(macro)[over the keys] F1 score for null: 0.3534026512851649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t1/8_xptc456_9bnyk0lkdrjql00000gn/T/ipykernel_23450/1533612958.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n"
     ]
    }
   ],
   "source": [
    "# How often does it say null for a key (not in subdocs, but for whole document) (wrongly vs. correctly) → F_1_{null}\n",
    "print(f\"Null evaluation:\\n{null_evaluation}\")\n",
    "\n",
    "# micro f1 score for null\n",
    "precision = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FP\", :].sum()) # micro average\n",
    "recall = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FN\", :].sum()) # micro average\n",
    "f1 = 2 * precision * recall / (precision + recall) # micro average\n",
    "print(f\"(micro)[over all predictions] Precision for null: {precision}\")\n",
    "print(f\"(micro)[over all predictions] Recall for null: {recall}\")\n",
    "print(f\"(micro)[over all predictions] F1 score for null: {f1}\")\n",
    "\n",
    "# macro f1 score for null\n",
    "# per key and then average\n",
    "null_scores_by_key = {}\n",
    "for key in null_evaluation.columns:\n",
    "    precision = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FP\", key])\n",
    "    recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    null_scores_by_key[key] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "null_scores_by_key = pd.DataFrame(null_scores_by_key).T\n",
    "print(null_scores_by_key)\n",
    "print(f\"(macro)[over the keys] Precision for null: {null_scores_by_key['precision'].mean()}\")\n",
    "print(f\"(macro)[over the keys] Recall for null: {null_scores_by_key['recall'].mean()}\")\n",
    "print(f\"(macro)[over the keys] F1 score for null: {null_scores_by_key['f1'].mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that micro and macro average of the  are quite close together because as we saw in the beginning, almost all keys are given in the data set. There is no big \"class\" (key) imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifications, collisions, lenient accuracy and looking at repetetiveness (all on subdocument level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Address (street) was not predicted at all (empty string prediction).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivo/Library/Caches/pypoetry/virtualenvs/uni-kie-JrmAaldC-py3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/ivo/Library/Caches/pypoetry/virtualenvs/uni-kie-JrmAaldC-py3.8/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Charity Number was not predicted at all (empty string prediction).\n",
      "Key Annual Income was not predicted at all (empty string prediction).\n",
      "Key Period End Date was not predicted at all (empty string prediction).\n",
      "Key Annual Spending was not predicted at all (empty string prediction).\n",
      "Key Charity Name was not predicted at all (empty string prediction).\n",
      "Key Charity Number was not predicted at all (empty string prediction).\n",
      "Key Annual Income was not predicted at all (empty string prediction).\n",
      "Key Annual Spending was not predicted at all (empty string prediction).\n"
     ]
    }
   ],
   "source": [
    "prediction_stats_dict = []\n",
    "subdoc_predictions_dict = []\n",
    "empty_key_dict = {\n",
    "    \"Address (post town)\": None,\n",
    "    \"Address (post code)\": None,\n",
    "    \"Address (street)\": None,\n",
    "    \"Charity Name\": None,\n",
    "    \"Charity Number\": None,\n",
    "    \"Annual Income\": None,\n",
    "    \"Period End Date\": None,\n",
    "    \"Annual Spending\": None,\n",
    "}\n",
    "\n",
    "\n",
    "for line in log_lines:\n",
    "    if \"Predicting document\" in line: # this is the beginning of a prediction\n",
    "        # create a new dictionary for this document\n",
    "        prediction_stats_dict.append({\n",
    "            \"num_subdocs\": None,\n",
    "            \"num_unifications\": 0,\n",
    "            \"collision_per_key\": empty_key_dict.copy(),\n",
    "            \"full_collision_per_key\": empty_key_dict.copy(),\n",
    "            \"num_unified_values_per_key\": empty_key_dict.copy(),\n",
    "            \"correct_in_any_subdoc_per_key\": empty_key_dict.copy(),\n",
    "            \"collision_percentage\": None,\n",
    "            \"full_collision_percentage\": None,\n",
    "            \"correct_in_any_subdoc_percentage\": None,\n",
    "        })\n",
    "        subdoc_predictions_dict.append(empty_key_dict.copy())\n",
    "\n",
    "    elif \"Final prediction for document\" in line: # this is the end of a prediction\n",
    "        # calculate the percentages\n",
    "        prediction_stats_dict[-1][\"collision_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"collision_per_key\"].values() if x is not None])\n",
    "        prediction_stats_dict[-1][\"full_collision_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"full_collision_per_key\"].values() if x is not None])\n",
    "        prediction_stats_dict[-1][\"correct_in_any_subdoc_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"].values() if x is not None])\n",
    " \n",
    "    elif \"No subdocs necessary\" in line:\n",
    "        prediction_stats_dict[-1][\"num_subdocs\"] = 1\n",
    "\n",
    "    elif \"Split document into\" in line:\n",
    "        num_subdocs = int(re.search(\"into (\\d+) subdocuments\", line).group(1))\n",
    "        prediction_stats_dict[-1][\"num_subdocs\"] = num_subdocs\n",
    "\n",
    "    elif \"- parse_model_output() - Key:\" in line:\n",
    "        key = re.search(\"- parse_model_output\\(\\) - Key: (.*):\", line).group(1)\n",
    "        # the prediction is always in the next line (unless the key was not predicted at all) or the prediction is an empty string\n",
    "        try:\n",
    "            prediction = re.search(\"- parse_model_output\\(\\) - Stripped value: (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "            \n",
    "\n",
    "        except AttributeError:\n",
    "            print(f\"Key {key} was not predicted at all (empty string prediction).\")\n",
    "            prediction = \"[METADATA]: EMPTY_STRING_PREDICTION\"\n",
    "            \n",
    "        if subdoc_predictions_dict[-1][key] is None:\n",
    "                subdoc_predictions_dict[-1][key] = [prediction]\n",
    "        else:\n",
    "            subdoc_predictions_dict[-1][key].append(prediction)\n",
    "\n",
    "    elif \"Unification necessary for key\" in line:\n",
    "        key = re.search(\"Unification necessary for key (.*)\", line).group(1)\n",
    "        prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "        values = re.search(\"Unifying \\d+ \\(lowered\\) values (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "\n",
    "        # values is the string representation of a list, so we can use eval to turn it into a list\n",
    "        values = eval(values)\n",
    "        unified_values = int(re.search(\"Unifying (\\d+) \\(lowered\\) values\", log_lines[log_lines.index(line)+1]).group(1))\n",
    "\n",
    "        assert unified_values == len(values) # sanity check\n",
    "\n",
    "        # if there is more than 1 value, then it's a unification\n",
    "        if len(values) > 1:\n",
    "            prediction_stats_dict[-1][\"num_unifications\"] += 1\n",
    "\n",
    "            # if there are more than 1 different values, then it's a collision\n",
    "            if len(set(values)) > 1:\n",
    "                prediction_stats_dict[-1][\"collision_per_key\"][key] = True\n",
    "\n",
    "            # if the length of the set is equal to the length of the list, then it's a full collision\n",
    "            if len(set(values)) == len(values):\n",
    "                prediction_stats_dict[-1][\"full_collision_per_key\"][key] = True\n",
    "\n",
    "        if unified_values == 1: # not a unification\n",
    "            prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = 0\n",
    "        else:\n",
    "            prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = unified_values\n",
    "\n",
    "\n",
    "        # which document are we in?\n",
    "        doc_num = len(prediction_stats_dict) - 1\n",
    "\n",
    "        # get the correct value for this key\n",
    "        correct_value = expected.iloc[doc_num][key]\n",
    "\n",
    "        # if it's NaN, then we were not supposed to predict anything for this key but we did (in at least one subdoc)\n",
    "        if pd.isna(correct_value):\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False # TODO: ask if this is correct\n",
    "            continue\n",
    "\n",
    "        # we have to transform the values in the list to the same format as the correct value\n",
    "        values = [x.replace(\" \", \"_\").replace(\":\", \"_\").upper() for x in values]\n",
    "\n",
    "        # also transform the correct value to uppercase\n",
    "        correct_value = str(correct_value).upper()\n",
    "\n",
    "        # if the correct value is in the list of values, then it's correct in at least one subdoc\n",
    "        if correct_value in values:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "\n",
    "        else:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "    elif \"Key not found in any subdoc\" in line: # null was predicted in all subdocs\n",
    "        key = re.search(\"Key not found in any subdoc (.*)\", line).group(1)\n",
    "        prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = 0\n",
    "\n",
    "        # get the correct value for this key\n",
    "        correct_value = expected.iloc[len(prediction_stats_dict) - 1][key]\n",
    "\n",
    "        if pd.isna(correct_value):\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "        else:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -> all docs with x number of subdocs should have x predictions for each key (also for the keys that have an empty string prediction\n",
    "# because we added [METADATA]: EMPTY_STRING_PREDICTION to the list of predictions\n",
    "for i in range (len(prediction_stats_dict)):\n",
    "    for key in subdoc_predictions_dict[i].keys():\n",
    "        assert len(subdoc_predictions_dict[i][key]) == prediction_stats_dict[i][\"num_subdocs\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdocs</th>\n",
       "      <th>num_unifications</th>\n",
       "      <th>collision_per_key</th>\n",
       "      <th>full_collision_per_key</th>\n",
       "      <th>num_unified_values_per_key</th>\n",
       "      <th>correct_in_any_subdoc_per_key</th>\n",
       "      <th>collision_percentage</th>\n",
       "      <th>full_collision_percentage</th>\n",
       "      <th>correct_in_any_subdoc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': 2, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': 5, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': 0, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': 0, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdocs  num_unifications  \\\n",
       "0            1                 0   \n",
       "1            9                 8   \n",
       "2            8                 8   \n",
       "3            2                 2   \n",
       "4            3                 4   \n",
       "\n",
       "                                   collision_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': True, 'Address (post c...   \n",
       "2  {'Address (post town)': True, 'Address (post c...   \n",
       "3  {'Address (post town)': False, 'Address (post ...   \n",
       "4  {'Address (post town)': False, 'Address (post ...   \n",
       "\n",
       "                              full_collision_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': True, 'Address (post c...   \n",
       "2  {'Address (post town)': False, 'Address (post ...   \n",
       "3  {'Address (post town)': False, 'Address (post ...   \n",
       "4  {'Address (post town)': False, 'Address (post ...   \n",
       "\n",
       "                          num_unified_values_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': 2, 'Address (post code...   \n",
       "2  {'Address (post town)': 5, 'Address (post code...   \n",
       "3  {'Address (post town)': 0, 'Address (post code...   \n",
       "4  {'Address (post town)': 0, 'Address (post code...   \n",
       "\n",
       "                       correct_in_any_subdoc_per_key  collision_percentage  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...                   NaN   \n",
       "1  {'Address (post town)': True, 'Address (post c...                 0.750   \n",
       "2  {'Address (post town)': True, 'Address (post c...                 0.875   \n",
       "3  {'Address (post town)': True, 'Address (post c...                 0.250   \n",
       "4  {'Address (post town)': True, 'Address (post c...                 0.250   \n",
       "\n",
       "   full_collision_percentage  correct_in_any_subdoc_percentage  \n",
       "0                        NaN                               NaN  \n",
       "1                      0.625                             0.750  \n",
       "2                      0.125                             0.750  \n",
       "3                      0.250                             0.750  \n",
       "4                      0.250                             0.875  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_stats = pd.DataFrame(prediction_stats_dict)\n",
    "prediction_stats.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenient Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"correct_in_any_subdoc_percentage\" does not consider the situation where there are no subdocs. In the case that not in any subdocs a value was predicted (all null) we check if the correct solution is in fact null and then consider that in any subdoc the corect value (which is null) was found (in reality it was correctly identified in all). We cannot do the opposite (check if it was null in any subdoc) and then say it was correctly identified (if it was indeed null) because of the subdoc structure where any given subdoc is not guaranteed to have all key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(official) (macro)[average over documents] Lenient accuracy: 0.709\n",
      "(official) Correct value found in any subdoc per key:\n",
      "   Address (post town)  Address (post code)  Address (street)  Charity Name  \\\n",
      "0             0.830409             0.704678          0.374269      0.619883   \n",
      "\n",
      "   Charity Number  Annual Income  Period End Date  Annual Spending  \n",
      "0        0.964912       0.587719         0.988304         0.602339  \n",
      "(official) (macro)[average over the keys] Lenient accuracy: 0.709\n"
     ]
    }
   ],
   "source": [
    "print(\"(official) (macro)[average over documents] Lenient accuracy:\", round(prediction_stats[\"correct_in_any_subdoc_percentage\"].mean(), 3))\n",
    "\n",
    "# a single entry is a dictionary with the correctness for each key (True, False, or None)\n",
    "correctness_per_key = prediction_stats[\"correct_in_any_subdoc_per_key\"].tolist()\n",
    "\n",
    "# create empty df\n",
    "lenient_accuracy_by_key = pd.DataFrame()\n",
    "\n",
    "print(\"(official) Correct value found in any subdoc per key:\")\n",
    "for key in correctness_per_key[0].keys():\n",
    "    correctness = [x[key] for x in correctness_per_key]\n",
    "    avg_correctness = np.mean([x for x in correctness if x is not None])\n",
    "    lenient_accuracy_by_key[key] = [avg_correctness]\n",
    "\n",
    "print(lenient_accuracy_by_key)\n",
    "print(f\"(official) (macro)[average over the keys] Lenient accuracy: {round(lenient_accuracy_by_key.mean(axis=1).mean(), 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the relative differences by key between the lenient accuracy and the actual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative improvement in accuracy (lenient vs. strict) (official evaluation):\n",
      "Address (post town): 10.39%\n",
      "Address (post code): 5.1%\n",
      "Address (street): 10.52%\n",
      "Charity Name: 3.71%\n",
      "Charity Number: 16.96%\n",
      "Period End Date: 3.29%\n",
      "Annual Income: 26.76%\n",
      "Annual Spending: 24.43%\n",
      "(macro) (official) Relative improvement in accuracy (lenient vs. strict) (average over the keys): 11.47 %\n",
      "(macro) (official) Absolute improvement in accuracy (lenient vs. strict) (average over the keys): 0.073\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_key = official_evaluation.mean(axis=0, skipna=True)\n",
    "print(\"Relative improvement in accuracy (lenient vs. strict) (official evaluation):\")\n",
    "for key in accuracy_by_key.keys():\n",
    "    print(f\"{key}: {round((lenient_accuracy_by_key[key].values[0] - accuracy_by_key[key]) / accuracy_by_key[key] * 100, 2)}%\")\n",
    "\n",
    "print(\"(macro) (official) Relative improvement in accuracy (lenient vs. strict) (average over the keys):\", round((lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean()) / accuracy_by_key.mean() * 100, 2), \"%\")\n",
    "print(\"(macro) (official) Absolute improvement in accuracy (lenient vs. strict) (average over the keys):\", round(lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean(), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and presence penalty motivation for not looking at those parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check how often the same value for different keys is predicted to see if it's generally correct to assume that values shouldn't be repeated (which could be combated by e.g. increasing the freq. and presence penalties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 rows that have the same value for different keys, that's 2.27% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "def has_two_values_for_different_keys(row):\n",
    "    return len(row) == len(set(row))\n",
    "\n",
    "num_of_same_value_rows = 0\n",
    "for i in range(len(expected)):\n",
    "    if not has_two_values_for_different_keys(expected.iloc[i]):\n",
    "        num_of_same_value_rows += 1\n",
    "\n",
    "print(f\"Found {num_of_same_value_rows} rows that have the same value for different keys, that's {round(num_of_same_value_rows / len(expected) * 100, 2)}% of the dataset.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, there shouldn't be the same value for different keys.\n",
    "A repetition of the same value would also be correct in the case of a document having more than 1 null value, where a correct generation would contain `\"null\"` multiple times. Let's look at how often this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that have 2 or more NaN values for any two keys:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SHEFFIELD</td>\n",
       "      <td>S7_1FE</td>\n",
       "      <td>133_ABBEYDALE_ROAD</td>\n",
       "      <td>Families_Relief</td>\n",
       "      <td>1168193</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St_Marina_Coptic_Orthodox_Church-Bristol</td>\n",
       "      <td>1149736</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>72998.00</td>\n",
       "      <td>31077.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>MAIDSTONE</td>\n",
       "      <td>ME16_8RL</td>\n",
       "      <td>5A_TONBRIDGE_ROAD</td>\n",
       "      <td>Lighthouse_Global_Network</td>\n",
       "      <td>1163804</td>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>TELFORD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randlay_Roundabouts_Pre-School</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-08-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Age_Concern_Harlow</td>\n",
       "      <td>264451</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>36071.00</td>\n",
       "      <td>31410.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Davis_Estate_Community_Centre</td>\n",
       "      <td>294242</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>30116.00</td>\n",
       "      <td>31244.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The_Childrens_Welfare_and_Research_Foundation</td>\n",
       "      <td>1101160</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>115677.00</td>\n",
       "      <td>72333.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sutton_Family_Church</td>\n",
       "      <td>1104168</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>143120.55</td>\n",
       "      <td>108294.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The_Silvers_Workshop</td>\n",
       "      <td>1159283</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>6445.00</td>\n",
       "      <td>5249.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rotary_Club_Of_Wigan_Trust_Fund</td>\n",
       "      <td>1037625</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>22433.54</td>\n",
       "      <td>20160.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lincolnshire_Greyhound_Trust</td>\n",
       "      <td>1103656</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>58843.00</td>\n",
       "      <td>62408.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paddock_Wood_Athletic_Club</td>\n",
       "      <td>1139343</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>94516.00</td>\n",
       "      <td>94110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Christian_Vision</td>\n",
       "      <td>1031031</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>43929201.00</td>\n",
       "      <td>14408439.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>ROMFORD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262_PETTITS_LANE_NORTH</td>\n",
       "      <td>Kingsheart_Church,_Romford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-05</td>\n",
       "      <td>53063.78</td>\n",
       "      <td>67195.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TN12_6XW</td>\n",
       "      <td>1_GOLDFINCH_CLOSE</td>\n",
       "      <td>Paddock_Wood_Athletic_Club</td>\n",
       "      <td>1139343</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Address (post town) Address (post code)        Address (street)  \\\n",
       "0              BROADWAY            WR12_7NL                     NaN   \n",
       "71            SHEFFIELD              S7_1FE      133_ABBEYDALE_ROAD   \n",
       "83                  NaN                 NaN                     NaN   \n",
       "100           MAIDSTONE            ME16_8RL       5A_TONBRIDGE_ROAD   \n",
       "171             TELFORD                 NaN                     NaN   \n",
       "176                 NaN                 NaN                     NaN   \n",
       "204                 NaN                 NaN                     NaN   \n",
       "210                 NaN                 NaN                     NaN   \n",
       "211                 NaN                 NaN                     NaN   \n",
       "221                 NaN                 NaN                     NaN   \n",
       "236                 NaN                 NaN                     NaN   \n",
       "262                 NaN                 NaN                     NaN   \n",
       "264                 NaN                 NaN                     NaN   \n",
       "332                 NaN                 NaN                     NaN   \n",
       "436             ROMFORD                 NaN  262_PETTITS_LANE_NORTH   \n",
       "439                 NaN            TN12_6XW       1_GOLDFINCH_CLOSE   \n",
       "\n",
       "                                      Charity Name Charity Number  \\\n",
       "0                       Wormington_Village_Society        1155074   \n",
       "71                                 Families_Relief        1168193   \n",
       "83        St_Marina_Coptic_Orthodox_Church-Bristol        1149736   \n",
       "100                      Lighthouse_Global_Network        1163804   \n",
       "171                 Randlay_Roundabouts_Pre-School            NaN   \n",
       "176                             Age_Concern_Harlow         264451   \n",
       "204                  Davis_Estate_Community_Centre         294242   \n",
       "210  The_Childrens_Welfare_and_Research_Foundation        1101160   \n",
       "211                           Sutton_Family_Church        1104168   \n",
       "221                           The_Silvers_Workshop        1159283   \n",
       "236                Rotary_Club_Of_Wigan_Trust_Fund        1037625   \n",
       "262                   Lincolnshire_Greyhound_Trust        1103656   \n",
       "264                     Paddock_Wood_Athletic_Club        1139343   \n",
       "332                               Christian_Vision        1031031   \n",
       "436                     Kingsheart_Church,_Romford            NaN   \n",
       "439                     Paddock_Wood_Athletic_Club        1139343   \n",
       "\n",
       "    Period End Date Annual Income Annual Spending  \n",
       "0        2018-07-31           NaN             NaN  \n",
       "71       2017-12-31           NaN             NaN  \n",
       "83       2017-03-31      72998.00        31077.00  \n",
       "100      2016-05-31           NaN             NaN  \n",
       "171      2014-08-31           NaN             NaN  \n",
       "176      2015-03-31      36071.00        31410.21  \n",
       "204      2018-03-31      30116.00        31244.00  \n",
       "210      2016-06-30     115677.00        72333.00  \n",
       "211      2013-12-31     143120.55       108294.79  \n",
       "221      2016-04-06       6445.00         5249.00  \n",
       "236      2017-06-30      22433.54        20160.83  \n",
       "262      2014-12-31      58843.00        62408.12  \n",
       "264      2013-12-31      94516.00        94110.00  \n",
       "332      2016-12-31   43929201.00     14408439.00  \n",
       "436      2015-04-05      53063.78        67195.24  \n",
       "439      2017-12-31           NaN             NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Documents that have 2 or more NaN values for any two keys:\")\n",
    "expected[expected.isna().sum(axis=1) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of documents that have 2 or more NaN values for any two keys: 3.64%\n"
     ]
    }
   ],
   "source": [
    "more_than_one_missing_value_in_solution_pct = round(len(expected[expected.isna().sum(axis=1) >= 2]) / len(expected) * 100, 2)\n",
    "print(f\"Percentage of documents that have 2 or more NaN values for any two keys: {more_than_one_missing_value_in_solution_pct}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, we shouldn't have more than 1 null in our generations, so we can now generally assume that repetition of the same value in the generations is incorrect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check how often we have repetition (i.e. the same value (including null's) for different keys) in the (subdoc) predictions. If this is very high then it would make sense to look into tuning the presence and frequency penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address (post town)': ['Wormington'], 'Address (post code)': ['WR12 7NL'], 'Address (street)': ['Dairymead'], 'Charity Name': ['Wormington Village Society'], 'Charity Number': ['1155074'], 'Annual Income': ['null'], 'Period End Date': ['31st July 2018'], 'Annual Spending': ['null']}\n",
      "Average percentage of unique predictions (ignoring null) per subdoc (micro average): 0.9773620434275629\n",
      "Average percentage of unique predictions per subdoc (micro average): 0.7894230769230769\n",
      "Average percentage of unique predictions (ignoring null) per subdoc (macro average over the subdocs for each doc): 0.9847101804032609\n",
      "Average percentage of unique predictions per subdoc (macro average over the subdocs for each doc): 0.8443978855866076\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(subdoc_predictions_dict)):\n",
    "    prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"] = {}\n",
    "    prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"] = {}\n",
    "    for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"]):\n",
    "        # get the predictions for each key\n",
    "        predictions = [subdoc_predictions_dict[i][key][subdoc] for key in subdoc_predictions_dict[i].keys()]\n",
    "        # get the number of unique predictions\n",
    "        num_unique_predictions = len(set(predictions))\n",
    "        # add the percentage of unique predictions to the dictionary\n",
    "        prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] = num_unique_predictions / len(predictions)\n",
    "\n",
    "        # get the number of unique predictions ignoring null values\n",
    "        num_unique_predictions_ignore_null = len(set([x for x in predictions if x != \"null\"]))\n",
    "\n",
    "        num_predictions_ignore_null = len([x for x in predictions if x != \"null\"])\n",
    "        # this is 0 if all the predictions are null\n",
    "        # in this case, we set the number of unique predictions to nan\n",
    "        if num_unique_predictions_ignore_null == 0:\n",
    "            prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = np.nan\n",
    "            continue\n",
    "        # add the percentage of unique predictions (ignoring null) to the dictionary\n",
    "        prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = num_unique_predictions_ignore_null / num_predictions_ignore_null\n",
    "\n",
    "print(subdoc_predictions_dict[0])\n",
    "\n",
    "print(\"Average percentage of unique predictions (ignoring null) per subdoc (micro average):\", np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for i in range(len(subdoc_predictions_dict)) for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "print(\"Average percentage of unique predictions per subdoc (micro average):\", np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for i in range(len(subdoc_predictions_dict)) for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "\n",
    "print(\"Average percentage of unique predictions (ignoring null) per subdoc (macro average over the subdocs for each doc):\", np.nanmean([np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(subdoc_predictions_dict))]))\n",
    "print(\"Average percentage of unique predictions per subdoc (macro average over the subdocs for each doc):\", np.nanmean([np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(subdoc_predictions_dict))]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to be expected that there is quite some repetition when not ignoring null values because many times the subdoc will only contain few or even none of the keys. When we ignore null values, repetition is almost non-existent, hence we can reasonably assume that tuning the frequency and presence penalty parameters would not improve results by much. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_stats_of_subdocs = prediction_stats[prediction_stats[\"num_subdocs\"] > 1]\n",
    "\n",
    "# add column \"no_collision_percentage\" which is the percentage of keys that had no collision\n",
    "prediction_stats_of_subdocs.loc[:, \"no_collision_percentage\"] = 1 - prediction_stats_of_subdocs.loc[:, \"collision_percentage\"]\n",
    "\n",
    "for i, row in prediction_stats_of_subdocs.iterrows():\n",
    "    num_keys = len([x for x in row[\"collision_per_key\"].values() if x is not None])\n",
    "    num_no_collisions = len([x for x in row[\"collision_per_key\"].values() if x is False])\n",
    "    try:\n",
    "        prediction_stats_of_subdocs.loc[i, \"no_collision_percentage_calculated\"] = num_no_collisions / num_keys\n",
    "    except ZeroDivisionError:\n",
    "        prediction_stats_of_subdocs.loc[i, \"no_collision_percentage_calculated\"] = np.nan\n",
    "\n",
    "assert np.allclose(prediction_stats_of_subdocs[\"collision_percentage\"], 1 - prediction_stats_of_subdocs[\"no_collision_percentage_calculated\"], equal_nan=True) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents that were split into more than 1 subdocument: {len(prediction_stats_of_subdocs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs_with_no_subdocs = len(prediction_stats[prediction_stats[\"num_subdocs\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = prediction_stats_of_subdocs['num_subdocs'].value_counts().sort_index().index\n",
    "y = prediction_stats_of_subdocs['num_subdocs'].value_counts().sort_index().values\n",
    "\n",
    "max_subdocs = max(x)\n",
    "\n",
    "x = np.append(x, 1)\n",
    "y = np.append(y, num_of_docs_with_no_subdocs)\n",
    "\n",
    "# add a 0 for each number of subdocs up to the maximum number of subdocs, so that there is a value for each number of subdocs\n",
    "for i in range(2, max_subdocs+1):\n",
    "    if i not in x:\n",
    "        x = np.append(x, i)\n",
    "        y = np.append(y, 0)\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(0.5, max_subdocs)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments needed\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Histogram of Number of Needed Subdocuments\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/subdoc_hist.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_values_per_key = {}\n",
    "\n",
    "for i in range(len(prediction_stats_of_subdocs)):\n",
    "    for key, value in prediction_stats_of_subdocs.iloc[i]['num_unified_values_per_key'].items():\n",
    "        if value is not None and value > 0:\n",
    "            if key in unified_values_per_key:\n",
    "                unified_values_per_key[key].append(value)\n",
    "            else:\n",
    "                unified_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "occurence_dict = {}\n",
    "for key, values in unified_values_per_key.items():\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "# intersperse keys that are missing with value 0\n",
    "for i in range(2, max(occurence_dict.keys())+1):\n",
    "    if i not in occurence_dict:\n",
    "        occurence_dict[i] = 0\n",
    "\n",
    "# sort the dictionary by the key (number of unifications)\n",
    "occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "x = list(occurence_dict.keys())\n",
    "y = list(occurence_dict.values())\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(1.5, max(x))\n",
    "ax.set(xlabel=\"Number of unified values\")\n",
    "ax.set_title(\"Histogram of Number of Unified Values per Key\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/unified_values_vs_occurence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_values_per_key = {}\n",
    "\n",
    "for i in range(len(prediction_stats_of_subdocs)):\n",
    "    for key, value in prediction_stats_of_subdocs.iloc[i]['num_unified_values_per_key'].items():\n",
    "        if value is not None and value > 0:\n",
    "            if key in unified_values_per_key:\n",
    "                unified_values_per_key[key].append(value)\n",
    "            else:\n",
    "                unified_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# one key = one bar plot (histogram)\n",
    "for key, values in unified_values_per_key.items():\n",
    "    occurence_dict = {}\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "    # intersperse keys that are missing with value 0\n",
    "    for i in range(2, max(occurence_dict.keys())+1):\n",
    "        if i not in occurence_dict:\n",
    "            occurence_dict[i] = 0\n",
    "\n",
    "    # sort the dictionary by the key (number of unifications)\n",
    "    occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "    x = list(occurence_dict.keys())\n",
    "    y = list(occurence_dict.values())\n",
    "\n",
    "    ax = sns.scatterplot(x=x, y=y, ax=ax, label=key)\n",
    "\n",
    "ax.set(xlabel=\"Number of unified values\", ylabel=\"Count\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Number of unified values by key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/unified_values_by_key_vs_count.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(prediction_stats_of_subdocs['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "y = [] # number of unified values per key (average over all keys)\n",
    "\n",
    "y = [[] for _ in range(max(x)+1)]\n",
    "\n",
    "for i in range(len(prediction_stats_of_subdocs)):\n",
    "    num_subdocs = prediction_stats_of_subdocs.iloc[i]['num_subdocs']\n",
    "    if num_subdocs == 1:\n",
    "        continue\n",
    "\n",
    "\n",
    "    num_unified_values = 0\n",
    "    num_keys = 0\n",
    "    for key, value in prediction_stats_of_subdocs.iloc[i]['num_unified_values_per_key'].items():\n",
    "        if value is not None and value > 0:\n",
    "            num_unified_values += value\n",
    "            num_keys += 1\n",
    "    if num_keys > 0:\n",
    "        y[num_subdocs].append(num_unified_values / num_keys)\n",
    "    else:\n",
    "        y[num_subdocs].append(0)\n",
    "\n",
    "y = [np.mean(y[i]) for i in range(len(y))] # average number of unified values per key for each number of subdocuments\n",
    "\n",
    "ax.plot([0, max(x) + 1], [0, max(x) + 1], color='black', linestyle='--', label='Upper bound')\n",
    "\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line')\n",
    "\n",
    "ax.set(xticks=x, yticks=x)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(right=max_subdocs+0.5)\n",
    "ax.set_ylim(top=max_subdocs+0.5)\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of unified values per key\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title(\"Number of subdocuments vs. average number of unified values per key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/num_subdocs_vs_unified_values_per_key.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of keys that were unified in some way: {sum(prediction_stats_of_subdocs['num_unifications'])}\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for key, values in unified_values_per_key.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(values)/len(values))\n",
    "\n",
    "# round to 2 decimal places\n",
    "y = [round(y[i], 2) for i in range(len(y))]\n",
    "\n",
    "# create a table with the keys and the average number of unified values per key\n",
    "key_stats = pd.DataFrame({'key': x, 'avg_num_unified_values': y})\n",
    "\n",
    "# print the table\n",
    "print(key_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification was a collision)\")\n",
    "ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (no collisions)\")\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=prediction_stats_of_subdocs, ax=ax, label=\"Collision percentage\")\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=prediction_stats_of_subdocs, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "avg_collision_percentage = np.nanmean(prediction_stats_of_subdocs['collision_percentage'])\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(prediction_stats_of_subdocs['full_collision_percentage'])\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set_xlim(left=2, right=max(prediction_stats_of_subdocs['num_subdocs']))\n",
    "ax.set(xticks=np.arange(2, max_subdocs+1, 1))\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"plots/collisions_wrt_subdocs_hist.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 subdocument == no subdocs (or in other words: the 1 subdoc is the whole document)\n",
    "\n",
    "Note: best case assumes:\n",
    "* perfect OCR\n",
    "* no mistakes in the reports (no typos, no conflicting information on different pages)\n",
    "\n",
    "of course with num_subdocs=2 every collision is a full_collision :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification was a collision)\")\n",
    "\n",
    "collision_percentage_by_key = {}\n",
    "full_collision_percentage_by_key = {}\n",
    "\n",
    "# line plot of the collision percentage by key\n",
    "for key in prediction_stats_of_subdocs['num_unified_values_per_key'].iloc[0].keys():\n",
    "    for i in range(len(prediction_stats_of_subdocs)):\n",
    "        if key not in collision_percentage_by_key:\n",
    "            collision_percentage_by_key[key] = []\n",
    "\n",
    "        if key not in full_collision_percentage_by_key:\n",
    "            full_collision_percentage_by_key[key] = []\n",
    "\n",
    "        collision_percentage_by_key[key].append(prediction_stats_of_subdocs['collision_per_key'].iloc[i][key])\n",
    "        full_collision_percentage_by_key[key].append(prediction_stats_of_subdocs['full_collision_per_key'].iloc[i][key])\n",
    "\n",
    "# filter out None values\n",
    "collision_percentage_by_key = {key: [x for x in collision_percentage_by_key[key] if x is not None] for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: [x for x in full_collision_percentage_by_key[key] if x is not None] for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# calculate average and ignore nan values and round to 2 decimal places\n",
    "collision_percentage_by_key = {key: round(np.nanmean(collision_percentage_by_key[key]), 2) for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: round(np.nanmean(full_collision_percentage_by_key[key]), 2) for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# add those values to the dataframe\n",
    "key_stats['collision_percentage'] = key_stats['key'].map(collision_percentage_by_key)\n",
    "key_stats['full_collision_percentage'] = key_stats['key'].map(full_collision_percentage_by_key)\n",
    "\n",
    "# create a barplot that has grouped bars\n",
    "\n",
    "# we want to use the \"hue\" parameter to group the bars by collision vs. full collision\n",
    "# thus we have to transform the dataframe to have a column for each of the two types of collisions\n",
    "# and a column for the key\n",
    "key_stats_trf = key_stats.melt(id_vars=['key'], value_vars=['collision_percentage', 'full_collision_percentage'], var_name='collision_type', value_name='collision_pct')\n",
    "\n",
    "# rename the collision types to something more readable\n",
    "key_stats_trf['collision_type'] = key_stats_trf['collision_type'].map({'collision_percentage': 'Collision percentage', 'full_collision_percentage': 'Full Collision percentage'})\n",
    "\n",
    "# create the barplot\n",
    "ax = sns.barplot(x=\"key\", y=\"collision_pct\", hue=\"collision_type\", data=key_stats_trf, ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "avg_collision_percentage = np.nanmean(list(collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(list(full_collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set(xlabel=\"Key\", ylabel=\"(Full) collision percentage\", title=\"Collision Percentage by Key\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/collision_percentage_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the part of collisions that are full collisions\n",
    "key_stats['full_collision_over_collision'] = round(key_stats['full_collision_percentage'] / key_stats['collision_percentage'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataframe key_stats with a heatmap\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# create a heatmap ignore 'key' column\n",
    "sns.heatmap(key_stats.iloc[:, 2:], annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax)\n",
    "\n",
    "ax.set(xlabel=\"Key\", ylabel=\"Collision type\", title=\"Collision statistics by key\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "In order to contextualize the performance of the baselines (general and specific), we want to check how often the key search (with the given fuzziness) yields any result. In case of the general baseline, if no match is found then no key-value is extracted, in case of the specific baseline there is the addition of synonyms for some of the keys which increases the likelihood of finding a key in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\"datasets/kleister_charity/dev-0/in_extended.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_key_to_gold_key = {\n",
    "    \"Address (post code)\": \"address__postcode\",\n",
    "    \"Address (street)\": \"address__street_line\",\n",
    "    \"Address (post town)\": \"address__post_town\",\n",
    "    \"Charity Name\": \"charity_name\",\n",
    "    \"Charity Number\": \"charity_number\",\n",
    "    \"Annual Income\": \"income_annually_in_british_pounds\",\n",
    "    \"Period End Date\": \"report_date\",\n",
    "    \"Annual Spending\": \"spending_annually_in_british_pounds\",\n",
    "}\n",
    "prompt_keys = list(prompt_key_to_gold_key.keys())\n",
    "gold_keys = list(prompt_key_to_gold_key.values())\n",
    "\n",
    "# for specific baseline we also use these synonyms\n",
    "synonyms = {\n",
    "    \"Charity Name\": [\"Charity Name\"],\n",
    "    \"Charity Number\": [\n",
    "        \"Charity Number\",\n",
    "        \"Charity Registration No\",\n",
    "        \"Charity No\",\n",
    "    ],\n",
    "    \"Annual Income\": [\"Annual Income\", \"Income\", \"Total Income\"],\n",
    "    \"Period End Date\": [\"Period End Date\", \"Period End\", \"Year Ended\"],\n",
    "    \"Annual Spending\": [\n",
    "        \"Annual Spending\",\n",
    "        \"Spending\",\n",
    "        \"Total Spending\",\n",
    "        \"Expenditure\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create dict that will record how often the respective key was actually found\n",
    "key_to_count = {key: 0 for key in prompt_keys}\n",
    "\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has to match the respective configuration of the baseline model that is evaluated\n",
    "type_of_baseline = \"specific\" # or \"specific\" \n",
    "error_percentage = 0.18\n",
    "\n",
    "def get_best_match_span(text: str, key: str):\n",
    "    \"\"\"\n",
    "    Returns the best match for the key in the text with some fuzziness\n",
    "    (i.e. we limit the levenshtein distance) of the best match.\n",
    "\n",
    "    (?b) -> BESTMATCH\n",
    "    (?i) -> IGNORECASE\n",
    "    {e<n} -> up to n errors (subs, inserts, dels). if more -> None\n",
    "    (1) -> the span of the best match\n",
    "    \"\"\"\n",
    "    key_length = len(key)\n",
    "    max_errors = round(key_length * error_percentage)\n",
    "    match_span = regex.search(f\"(?b)(?i)({key}){{e<{max_errors}}}\", text)\n",
    "\n",
    "    if match_span:\n",
    "        return match_span.span(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(in_df)):\n",
    "    text = in_df.loc[in_df[\"filename\"] == in_df.iloc[i][\"filename\"], \"text_best_cleaned\"].values[0]\n",
    "\n",
    "    if type_of_baseline == \"general\":\n",
    "        # check for a match for each key\n",
    "        for i, key in enumerate(prompt_keys):\n",
    "            total += 1\n",
    "            match_span = get_best_match_span(text, key)\n",
    "\n",
    "            if match_span is None:\n",
    "                continue\n",
    "            else:\n",
    "                key_to_count[key] += 1\n",
    "    \n",
    "    elif type_of_baseline == \"specific\":\n",
    "        # check for a match for each synonym\n",
    "        for i, key in enumerate(list(synonyms.keys())):\n",
    "            total += 1\n",
    "            for synonym in synonyms[key]:\n",
    "                match_span = get_best_match_span(text, synonym)\n",
    "    \n",
    "                if match_span is None: # no match for this synonym\n",
    "                    continue\n",
    "                else:\n",
    "                    key_to_count[key] += 1 # found a match for this key\n",
    "                    break # no need to check the other synonyms for this key as we are only interested if any of them matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_of_baseline == \"specific\":\n",
    "    num_keys_considered = len(synonyms.keys())\n",
    "\n",
    "    # remove the keys that we don't use synonyms for\n",
    "    for key in prompt_keys:\n",
    "        if key not in synonyms.keys():\n",
    "            key_to_count.pop(key)\n",
    "else:\n",
    "    num_keys_considered = len(prompt_keys)\n",
    "\n",
    "\n",
    "key_to_count_percentage = {key: count / total * num_keys_considered * 100 for key, count in key_to_count.items()}\n",
    "print(key_to_count_percentage)\n",
    "\n",
    "# macro average\n",
    "print(f\"(macro)[over keys] average Percentage of how often the keys can be found in the docoument: {sum(key_to_count_percentage.values()) / len(key_to_count_percentage)}\")\n",
    "\n",
    "# micro average\n",
    "print(f\"(micro)[over all predictions] average Percentage of how often the keys can be found in the document: {sum(key_to_count.values()) / total * 100}\")\n",
    "\n",
    "# they are the same which makes sense because all classes (keys) are checked the same amount of times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
