{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from Levenshtein import distance\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has to match the respective configuration of the baseline model that is evaluated\n",
    "TYPE_OF_BASELINE = \"GENERAL\" # \"SPECIFIC\" or \"GENERAL\"\n",
    "ERROR_PERCENTAGE = 0.18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading OCR input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\"datasets/kleister_charity/test-A/in_extended.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "In order to contextualize the performance of the baselines (general and specific), we want to check how often the key search (with the given fuzziness) yields any result. In case of the general baseline, if no match is found then no key-value is extracted, in case of the specific baseline there is the addition of synonyms for some of the keys which increases the likelihood of finding a key in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_key_to_gold_key = {\n",
    "    \"Address (post code)\": \"address__postcode\",\n",
    "    \"Address (street)\": \"address__street_line\",\n",
    "    \"Address (post town)\": \"address__post_town\",\n",
    "    \"Charity Name\": \"charity_name\",\n",
    "    \"Charity Number\": \"charity_number\",\n",
    "    \"Annual Income\": \"income_annually_in_british_pounds\",\n",
    "    \"Period End Date\": \"report_date\",\n",
    "    \"Annual Spending\": \"spending_annually_in_british_pounds\",\n",
    "}\n",
    "prompt_keys = list(prompt_key_to_gold_key.keys())\n",
    "gold_keys = list(prompt_key_to_gold_key.values())\n",
    "\n",
    "# for specific baseline we also use these synonyms\n",
    "synonyms = {\n",
    "    \"Charity Name\": [\"Charity Name\"],\n",
    "    \"Charity Number\": [\n",
    "        \"Charity Number\",\n",
    "        \"Charity Registration No\",\n",
    "        \"Charity No\",\n",
    "    ],\n",
    "    \"Annual Income\": [\"Annual Income\", \"Income\", \"Total Income\"],\n",
    "    \"Period End Date\": [\"Period End Date\", \"Period End\", \"Year Ended\"],\n",
    "    \"Annual Spending\": [\n",
    "        \"Annual Spending\",\n",
    "        \"Spending\",\n",
    "        \"Total Spending\",\n",
    "        \"Expenditure\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create dict that will record how often the respective key was actually found\n",
    "key_to_count = {key: 0 for key in prompt_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_span(text: str, key: str):\n",
    "    \"\"\"\n",
    "    Returns the best match for the key in the text with some fuzziness\n",
    "    (i.e. we limit the levenshtein distance) of the best match.\n",
    "\n",
    "    (?b) -> BESTMATCH\n",
    "    (?i) -> IGNORECASE\n",
    "    {e<n} -> up to n errors (subs, inserts, dels). if more -> None\n",
    "    (1) -> the span of the best match\n",
    "    \"\"\"\n",
    "    key_length = len(key)\n",
    "    max_errors = round(key_length * ERROR_PERCENTAGE)\n",
    "    match_span = regex.search(f\"(?b)(?i)({key}){{e<{max_errors}}}\", text)\n",
    "\n",
    "    if match_span:\n",
    "        return match_span.span(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(len(in_df)):\n",
    "    text = in_df.loc[in_df[\"filename\"] == in_df.iloc[i][\"filename\"], \"text_best_cleaned\"].values[0]\n",
    "\n",
    "    if TYPE_OF_BASELINE == \"GENERAL\":\n",
    "        # check for a match for each key\n",
    "        for i, key in enumerate(prompt_keys):\n",
    "            total += 1\n",
    "            match_span = get_best_match_span(text, key)\n",
    "\n",
    "            if match_span is None:\n",
    "                continue\n",
    "            else:\n",
    "                key_to_count[key] += 1\n",
    "    \n",
    "    elif TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "        # check for a match for each synonym\n",
    "        for i, key in enumerate(list(synonyms.keys())):\n",
    "            total += 1\n",
    "            for synonym in synonyms[key]:\n",
    "                match_span = get_best_match_span(text, synonym)\n",
    "    \n",
    "                if match_span is None: # no match for this synonym\n",
    "                    continue\n",
    "                else:\n",
    "                    key_to_count[key] += 1 # found a match for this key\n",
    "                    break # no need to check the other synonyms for this key as we are only interested if any of them matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address (post code)': 0.006568144499178982, 'Address (street)': 0.003284072249589491, 'Address (post town)': 0.0, 'Charity Name': 0.16748768472906403, 'Charity Number': 0.7142857142857143, 'Annual Income': 0.019704433497536946, 'Period End Date': 0.06403940886699508, 'Annual Spending': 0.008210180623973728}\n",
      "(macro)[over keys] average Percentage of how often the keys can be found in the docoument: 0.12294745484400657\n",
      "(micro)[over all predictions] average Percentage of how often the keys can be found in the document: 0.12294745484400657\n"
     ]
    }
   ],
   "source": [
    "if TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "    num_keys_considered = len(synonyms.values())\n",
    "    \n",
    "    # remove the keys that we don't use synonyms for\n",
    "    for key in prompt_keys:\n",
    "        if key not in synonyms.keys():\n",
    "            key_to_count.pop(key)\n",
    "else:\n",
    "    num_keys_considered = len(prompt_keys)\n",
    "\n",
    "\n",
    "key_to_count_ratio = {key: count / total * num_keys_considered for key, count in key_to_count.items()}\n",
    "print(key_to_count_ratio)\n",
    "\n",
    "print(f\"(macro)[over keys] average Percentage of how often the keys can be found in the docoument: {sum(key_to_count_ratio.values()) / len(key_to_count_ratio)}\")\n",
    "print(f\"(micro)[over all predictions] average Percentage of how often the keys can be found in the document: {sum(key_to_count.values()) / total}\")\n",
    "\n",
    "# they are the same which makes sense because all classes (keys) are checked the same amount of times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results with own definition of correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE_OF_BASELINE == \"GENERAL\":\n",
    "    PREDICTION_RUNS_PATHS = ['datasets/kleister_charity/test-A/predictions/baselines/2023-01-30T20-55-26_BaselinePipeline(pdf_to_text_model=KleisterCharityWrapper, model=Baseline(error_percentage=0.18, allowed_entity_range=40), parser=KleisterCharityParser, ner_tagger=en_core_web_sm).tsv']\n",
    "\n",
    "elif TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "        PREDICTION_RUNS_PATHS = ['datasets/kleister_charity/test-A/predictions/baselines/2023-01-30T20-35-54_BaselinePipeline(pdf_to_text_model=KleisterCharityWrapper, model=SpecificBaseline(error_percentage=0.18, allowed_entity_range=40), parser=KleisterCharityParser, ner_tagger=en_core_web_sm).tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAMPTON</td>\n",
       "      <td>TW12_3HD</td>\n",
       "      <td>HANWORTH_ROAD</td>\n",
       "      <td>Hampton_School</td>\n",
       "      <td>1120005</td>\n",
       "      <td>25027407.00</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>22739373.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HASSOCKS</td>\n",
       "      <td>BN6_9JS</td>\n",
       "      <td>COLLEGE_LANE</td>\n",
       "      <td>Hurstpierpoint_College_Ltd.</td>\n",
       "      <td>1076498</td>\n",
       "      <td>20675000.00</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>18255000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRISTOL</td>\n",
       "      <td>BS2_0PT</td>\n",
       "      <td>AVON_STREET</td>\n",
       "      <td>The_Rowland_Betty_Memorial_Trust</td>\n",
       "      <td>1153211</td>\n",
       "      <td>275173.00</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>298446.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HITCHIN</td>\n",
       "      <td>SG4_8NR</td>\n",
       "      <td>PASTURE_LANE</td>\n",
       "      <td>Breachwood_Green_Pre-School</td>\n",
       "      <td>1038926</td>\n",
       "      <td>34293.00</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>37970.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BISHOP'S_STORTFORD</td>\n",
       "      <td>CM23_3LJ</td>\n",
       "      <td>TWYFORD_ROAD</td>\n",
       "      <td>Bishop's_Stortford_Baptist_Church</td>\n",
       "      <td>1159484</td>\n",
       "      <td>342604.82</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>309341.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Address (post town) Address (post code) Address (street)  \\\n",
       "0             HAMPTON            TW12_3HD    HANWORTH_ROAD   \n",
       "1            HASSOCKS             BN6_9JS     COLLEGE_LANE   \n",
       "2             BRISTOL             BS2_0PT      AVON_STREET   \n",
       "3             HITCHIN             SG4_8NR     PASTURE_LANE   \n",
       "4  BISHOP'S_STORTFORD            CM23_3LJ     TWYFORD_ROAD   \n",
       "\n",
       "                        Charity Name Charity Number Annual Income  \\\n",
       "0                     Hampton_School        1120005   25027407.00   \n",
       "1        Hurstpierpoint_College_Ltd.        1076498   20675000.00   \n",
       "2   The_Rowland_Betty_Memorial_Trust        1153211     275173.00   \n",
       "3        Breachwood_Green_Pre-School        1038926      34293.00   \n",
       "4  Bishop's_Stortford_Baptist_Church        1159484     342604.82   \n",
       "\n",
       "  Period End Date Annual Spending  \n",
       "0      2016-08-31     22739373.00  \n",
       "1      2015-08-31     18255000.00  \n",
       "2      2018-02-03       298446.00  \n",
       "3      2017-12-31        37970.00  \n",
       "4      2017-12-31       309341.57  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = pd.read_csv('datasets/kleister_charity/test-A/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n",
    "expected = expected[COLUMN_ORDER]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_dfs = []\n",
    "for prediction_run_path in PREDICTION_RUNS_PATHS:\n",
    "    prediction_run_df = pd.read_csv(prediction_run_path, sep='\\t', header=None, names=['raw'], skip_blank_lines=False)\n",
    "\n",
    "    for raw_prediction in prediction_run_df[\"raw\"]:\n",
    "        if raw_prediction is np.nan:\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction] = np.nan\n",
    "            continue\n",
    "        key_value_pairs = raw_prediction.split(\" \")\n",
    "        for key_value in key_value_pairs:\n",
    "            key, value = key_value.split(\"=\", 1)\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction, key] = value\n",
    "\n",
    "    num_columns = len(prediction_run_df.columns)\n",
    "    if TYPE_OF_BASELINE == 'GENERAL':\n",
    "        prediction_column_order = [\"raw\", \"Charity Number\", \"Charity Name\", \"Period End Date\", \"Address (post code)\", \"Address (street)\", \"Annual Income\", \"Annual Spending\", \"Address (post town)\"]\n",
    "    \n",
    "    # TODO: be very careful with this, the order of the columns is very important and unfortunately not always the same\n",
    "    # column_order = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n",
    "    elif TYPE_OF_BASELINE == 'SPECIFIC':\n",
    "        prediction_column_order = [\"raw\", \"Address (post code)\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\", \"Address (street)\", \"Address (post town)\", \"Charity Name\"]\n",
    "\n",
    "\n",
    "    # rename columns\n",
    "    prediction_run_df.columns = prediction_column_order[:num_columns]\n",
    "\n",
    "    # add any missing columns and fill them with NaN (flan-t5 almost always only predicts first key)\n",
    "    for column in prediction_column_order[num_columns:]:\n",
    "        prediction_run_df[column] = np.nan\n",
    "    \n",
    "    prediction_run_df = prediction_run_df[COLUMN_ORDER]\n",
    "    prediction_run_df = prediction_run_df.drop(columns=[\"raw\"])\n",
    "    prediction_runs_dfs.append(prediction_run_df)\n",
    "\n",
    "assert len(prediction_runs_dfs) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1076498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1153211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1038926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Address (post town) Address (post code) Address (street) Charity Name  \\\n",
       "0                  NaN                 NaN              NaN          NaN   \n",
       "1                  NaN                 NaN              NaN          NaN   \n",
       "2                  NaN                 NaN              NaN          NaN   \n",
       "3                  NaN                 NaN              NaN          NaN   \n",
       "4                  NaN                 NaN              NaN          NaN   \n",
       "\n",
       "  Charity Number Annual Income Period End Date  Annual Spending  \n",
       "0        1120005           NaN             NaN              NaN  \n",
       "1        1076498           NaN             NaN              NaN  \n",
       "2        1153211           NaN             NaN              NaN  \n",
       "3        1038926           NaN             NaN              NaN  \n",
       "4            NaN           NaN             NaN              NaN  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_runs_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 1 quotation marks in prediction run 0.\n",
      "Replaced 1 quotation marks in expected.\n"
     ]
    }
   ],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    count = replace_quotation_mark(prediction_run_df)\n",
    "    print(f\"Replaced {count} quotation marks in prediction run {i}.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(1)]\n",
    "own_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(1)]\n",
    "null_evaluations = [pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) for _ in range(1)]\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    for index, row in expected.iterrows():\n",
    "        for column in expected.columns:\n",
    "            if pd.notnull(row[column]): # because during parsing we look at the generations and if all subdocs are \"null\" or empty strings, it will not appear in the output and hence be NaN\n",
    "                # FP: we predicted null and it was not null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"FP\", column] += 1\n",
    "\n",
    "                # TN: we predicted not null (i.e. we predicted something) and it was not null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"TN\", column] += 1\n",
    "                if is_correct(column, row[column], prediction_run_df.loc[index, column]):\n",
    "                    own_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    own_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "                if str(row[column]).upper() == str(prediction_run_df.loc[index, column]).upper():\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "            else: # we don't care about the prediction in our own evaluation if the expected value is null\n",
    "                # TP: we predicted null and it was null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"TP\", column] += 1\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "\n",
    "                # FN: we predicted not null and it was null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"FN\", column] += 1\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro)[over runs] (own) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.000000  0.000000  0.000000       0.0\n",
      "Address (post code)  0.000000  0.000000  0.000000       0.0\n",
      "Address (street)     0.000000  0.000000  0.000000       0.0\n",
      "Charity Name         0.027915  0.027915  0.027915       0.0\n",
      "Charity Number       0.583056  0.583056  0.583056       0.0\n",
      "Annual Income        0.000000  0.000000  0.000000       0.0\n",
      "Period End Date      0.001642  0.001642  0.001642       0.0\n",
      "Annual Spending      0.000000  0.000000  0.000000       0.0\n",
      "(macro)[over runs and keys] (own) average of correctly predicted values: 0.077\n",
      "(macro)[over runs and keys] (own) range of correctly predicted values: 0.0\n",
      "(micro)[over all key-value pairs] (own) average of correctly predicted values: 0.078\n",
      "(micro)[over all key-value pairs] (own) sample standard deviation of correctly predicted values: 0.0\n",
      "(macro)[over runs] (official) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.044335  0.044335  0.044335       0.0\n",
      "Address (post code)  0.029557  0.029557  0.029557       0.0\n",
      "Address (street)     0.080460  0.080460  0.080460       0.0\n",
      "Charity Name         0.027915  0.027915  0.027915       0.0\n",
      "Charity Number       0.587849  0.587849  0.587849       0.0\n",
      "Annual Income        0.021346  0.021346  0.021346       0.0\n",
      "Period End Date      0.001642  0.001642  0.001642       0.0\n",
      "Annual Spending      0.029557  0.029557  0.029557       0.0\n",
      "(macro)[over runs and keys] (official) average of correctly predicted values: 0.103\n",
      "(macro)[over runs and keys] (official) range of correctly predicted values: 0.0\n",
      "(micro)[over all key-value pairs] (official) average of correctly predicted values: 0.103\n",
      "(micro)[over all key-value pairs] (official) sample standard deviation of correctly predicted values: 0.0\n"
     ]
    }
   ],
   "source": [
    "# own evaluation: only looks at the keys that are actually present in the document\n",
    "\n",
    "# we combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the own evaluations by key of each run\n",
    "avg_own_evaluation_by_key = pd.concat([own_evaluation.mean(axis=0, skipna=True) for own_evaluation in own_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (own) evaluation by key:\\n{avg_own_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (own) average of correctly predicted values: {round(avg_own_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (own) range of correctly predicted values: {round(avg_own_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")\n",
    "\n",
    "micro_averaged_accuracy = []\n",
    "for i, own_evaluation in enumerate(own_evaluations):\n",
    "    micro_averaged_accuracy.append(own_evaluation.sum().sum() / own_evaluation.count().sum())\n",
    "\n",
    "print(f\"(micro)[over all key-value pairs] (own) average of correctly predicted values: {round(np.mean(micro_averaged_accuracy), 3)}\")\n",
    "print(f\"(micro)[over all key-value pairs] (own) sample standard deviation of correctly predicted values: {round(np.std(micro_averaged_accuracy), 3)}\")\n",
    "\n",
    "# official evaluation (same as above but with the official evaluation)\n",
    "avg_official_evaluation_by_key = pd.concat([official_evaluation.mean(axis=0, skipna=True) for official_evaluation in official_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (official) evaluation by key:\\n{avg_official_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (official) average of correctly predicted values: {round(avg_official_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (official) range of correctly predicted values: {round(avg_official_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")\n",
    "\n",
    "micro_averaged_accuracy = []\n",
    "for i, official_evaluation in enumerate(official_evaluations):\n",
    "    micro_averaged_accuracy.append(official_evaluation.sum().sum() / official_evaluation.count().sum())\n",
    "\n",
    "print(f\"(micro)[over all key-value pairs] (official) average of correctly predicted values: {round(np.mean(micro_averaged_accuracy), 3)}\")\n",
    "print(f\"(micro)[over all key-value pairs] (official) sample standard deviation of correctly predicted values: {round(np.std(micro_averaged_accuracy), 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
