{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from Levenshtein import distance\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has to match the respective configuration of the baseline model that is evaluated\n",
    "TYPE_OF_BASELINE = \"GENERAL\" # \"SPECIFIC\" or \"GENERAL\"\n",
    "ERROR_PERCENTAGE = 0.18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading OCR input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\"datasets/kleister_charity/dev-0/in_extended.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "In order to contextualize the performance of the baselines (general and specific), we want to check how often the key search (with the given fuzziness) yields any result. In case of the general baseline, if no match is found then no key-value is extracted, in case of the specific baseline there is the addition of synonyms for some of the keys which increases the likelihood of finding a key in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_key_to_gold_key = {\n",
    "    \"Address (post code)\": \"address__postcode\",\n",
    "    \"Address (street)\": \"address__street_line\",\n",
    "    \"Address (post town)\": \"address__post_town\",\n",
    "    \"Charity Name\": \"charity_name\",\n",
    "    \"Charity Number\": \"charity_number\",\n",
    "    \"Annual Income\": \"income_annually_in_british_pounds\",\n",
    "    \"Period End Date\": \"report_date\",\n",
    "    \"Annual Spending\": \"spending_annually_in_british_pounds\",\n",
    "}\n",
    "prompt_keys = list(prompt_key_to_gold_key.keys())\n",
    "gold_keys = list(prompt_key_to_gold_key.values())\n",
    "\n",
    "# for specific baseline we also use these synonyms\n",
    "synonyms = {\n",
    "    \"Charity Name\": [\"Charity Name\"],\n",
    "    \"Charity Number\": [\n",
    "        \"Charity Number\",\n",
    "        \"Charity Registration No\",\n",
    "        \"Charity No\",\n",
    "    ],\n",
    "    \"Annual Income\": [\"Annual Income\", \"Income\", \"Total Income\"],\n",
    "    \"Period End Date\": [\"Period End Date\", \"Period End\", \"Year Ended\"],\n",
    "    \"Annual Spending\": [\n",
    "        \"Annual Spending\",\n",
    "        \"Spending\",\n",
    "        \"Total Spending\",\n",
    "        \"Expenditure\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create dict that will record how often the respective key was actually found\n",
    "key_to_count = {key: 0 for key in prompt_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match_span(text: str, key: str):\n",
    "    \"\"\"\n",
    "    Returns the best match for the key in the text with some fuzziness\n",
    "    (i.e. we limit the levenshtein distance) of the best match.\n",
    "\n",
    "    (?b) -> BESTMATCH\n",
    "    (?i) -> IGNORECASE\n",
    "    {e<n} -> up to n errors (subs, inserts, dels). if more -> None\n",
    "    (1) -> the span of the best match\n",
    "    \"\"\"\n",
    "    key_length = len(key)\n",
    "    max_errors = round(key_length * ERROR_PERCENTAGE)\n",
    "    match_span = regex.search(f\"(?b)(?i)({key}){{e<{max_errors}}}\", text)\n",
    "\n",
    "    if match_span:\n",
    "        return match_span.span(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(len(in_df)):\n",
    "    text = in_df.loc[in_df[\"filename\"] == in_df.iloc[i][\"filename\"], \"text_best_cleaned\"].values[0]\n",
    "\n",
    "    if TYPE_OF_BASELINE == \"GENERAL\":\n",
    "        # check for a match for each key\n",
    "        for i, key in enumerate(prompt_keys):\n",
    "            total += 1\n",
    "            match_span = get_best_match_span(text, key)\n",
    "\n",
    "            if match_span is None:\n",
    "                continue\n",
    "            else:\n",
    "                key_to_count[key] += 1\n",
    "    \n",
    "    elif TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "        # check for a match for each synonym\n",
    "        for i, key in enumerate(list(synonyms.keys())):\n",
    "            total += 1\n",
    "            for synonym in synonyms[key]:\n",
    "                match_span = get_best_match_span(text, synonym)\n",
    "    \n",
    "                if match_span is None: # no match for this synonym\n",
    "                    continue\n",
    "                else:\n",
    "                    key_to_count[key] += 1 # found a match for this key\n",
    "                    break # no need to check the other synonyms for this key as we are only interested if any of them matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address (post code)': 0.0022727272727272726, 'Address (street)': 0.004545454545454545, 'Address (post town)': 0.0, 'Charity Name': 0.225, 'Charity Number': 0.75, 'Annual Income': 0.04772727272727273, 'Period End Date': 0.10681818181818181, 'Annual Spending': 0.00909090909090909}\n",
      "(macro)[over keys] average Percentage of how often the keys can be found in the docoument: 0.14318181818181816\n",
      "(micro)[over all predictions] average Percentage of how often the keys can be found in the document: 0.1431818181818182\n"
     ]
    }
   ],
   "source": [
    "if TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "    num_keys_considered = len(synonyms.values())\n",
    "    \n",
    "    # remove the keys that we don't use synonyms for\n",
    "    for key in prompt_keys:\n",
    "        if key not in synonyms.keys():\n",
    "            key_to_count.pop(key)\n",
    "else:\n",
    "    num_keys_considered = len(prompt_keys)\n",
    "\n",
    "\n",
    "key_to_count_ratio = {key: count / total * num_keys_considered for key, count in key_to_count.items()}\n",
    "print(key_to_count_ratio)\n",
    "\n",
    "print(f\"(macro)[over keys] average Percentage of how often the keys can be found in the docoument: {sum(key_to_count_ratio.values()) / len(key_to_count_ratio)}\")\n",
    "print(f\"(micro)[over all predictions] average Percentage of how often the keys can be found in the document: {sum(key_to_count.values()) / total}\")\n",
    "\n",
    "# they are the same which makes sense because all classes (keys) are checked the same amount of times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results with own definition of correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE_OF_BASELINE == \"GENERAL\":\n",
    "    PREDICTION_RUNS_PATHS = ['datasets/kleister_charity/dev-0/predictions/baselines/BaselinePipeline(pdf_to_text_model=KleisterCharityWrapper, model=Baseline(error_percentage=0.18, allowed_entity_range=40), parser=KleisterCharityParser, ner_tagger=en_core_web_sm)_2022-11-24T00-37-43.tsv']\n",
    "\n",
    "elif TYPE_OF_BASELINE == \"SPECIFIC\":\n",
    "    PREDICTION_RUNS_PATHS = ['datasets/kleister_charity/dev-0/predictions/baselines/2023-01-29T18-14-19_BaselinePipeline(pdf_to_text_model=KleisterCharityWrapper, model=SpecificBaseline(error_percentage=0.18, allowed_entity_range=40), parser=KleisterCharityParser, ner_tagger=en_core_web_sm).tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WESTCLIFF-ON-SEA</td>\n",
       "      <td>SS0_8HX</td>\n",
       "      <td>47_SECOND_AVENUE</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>1022119</td>\n",
       "      <td>10348000.00</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>9415000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHELTENHAM</td>\n",
       "      <td>GL50_3EP</td>\n",
       "      <td>BAYSHILL_ROAD</td>\n",
       "      <td>Cheltenham_Ladies_College</td>\n",
       "      <td>311722</td>\n",
       "      <td>32168000.00</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>27972000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHREWSBURY</td>\n",
       "      <td>SY3_7PQ</td>\n",
       "      <td>58_TRINITY_STREET</td>\n",
       "      <td>The_Sanata_Charitable_Trust</td>\n",
       "      <td>1132766</td>\n",
       "      <td>255653.00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>258287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WARE</td>\n",
       "      <td>SG11_2DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>122836.00</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>124446.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Address (post town) Address (post code)   Address (street)  \\\n",
       "0            BROADWAY            WR12_7NL                NaN   \n",
       "1    WESTCLIFF-ON-SEA             SS0_8HX   47_SECOND_AVENUE   \n",
       "2          CHELTENHAM            GL50_3EP      BAYSHILL_ROAD   \n",
       "3          SHREWSBURY             SY3_7PQ  58_TRINITY_STREET   \n",
       "4                WARE            SG11_2DY                NaN   \n",
       "\n",
       "                  Charity Name Charity Number Annual Income Period End Date  \\\n",
       "0   Wormington_Village_Society        1155074           NaN      2018-07-31   \n",
       "1     Havens_Christian_Hospice        1022119   10348000.00      2016-03-31   \n",
       "2    Cheltenham_Ladies_College         311722   32168000.00      2016-07-31   \n",
       "3  The_Sanata_Charitable_Trust        1132766     255653.00      2015-12-31   \n",
       "4          Cantate_Youth_Choir        1039369     122836.00      2013-12-31   \n",
       "\n",
       "  Annual Spending  \n",
       "0             NaN  \n",
       "1      9415000.00  \n",
       "2     27972000.00  \n",
       "3       258287.00  \n",
       "4       124446.00  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = pd.read_csv('datasets/kleister_charity/dev-0/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Address (street)\", \"Annual Income\",  \"Annual Spending\"]\n",
    "expected = expected[COLUMN_ORDER]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_dfs = []\n",
    "for prediction_run_path in PREDICTION_RUNS_PATHS:\n",
    "    prediction_run_df = pd.read_csv(prediction_run_path, sep='\\t', header=None, names=['raw'], skip_blank_lines=False)\n",
    "\n",
    "    for raw_prediction in prediction_run_df[\"raw\"]:\n",
    "        if raw_prediction is np.nan:\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction] = np.nan\n",
    "            continue\n",
    "        key_value_pairs = raw_prediction.split(\" \")\n",
    "        for key_value in key_value_pairs:\n",
    "            key, value = key_value.split(\"=\", 1)\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction, key] = value\n",
    "\n",
    "    num_columns = len(prediction_run_df.columns)\n",
    "\n",
    "    if TYPE_OF_BASELINE == 'GENERAL':\n",
    "        prediction_column_order = [\"raw\", \"Charity Name\", \"Charity Number\", \"Address (post code)\", \"Annual Income\", \"Period End Date\", \"Address (street)\", \"Annual Spending\", \"Address (post town)\"]\n",
    "    \n",
    "    # TODO: be very careful with this, the order of the columns is very important and unfortunately not always the same\n",
    "    # column_order = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n",
    "    elif TYPE_OF_BASELINE == 'SPECIFIC':\n",
    "        prediction_column_order = [\"raw\",  \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Address (post town)\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n",
    "\n",
    "\n",
    "    # rename columns\n",
    "    prediction_run_df.columns = prediction_column_order[:num_columns]\n",
    "\n",
    "    # add any missing columns and fill them with NaN (flan-t5 almost always only predicts first key)\n",
    "    for column in prediction_column_order[num_columns:]:\n",
    "        prediction_run_df[column] = np.nan\n",
    "    \n",
    "    prediction_run_df = prediction_run_df[COLUMN_ORDER]\n",
    "    prediction_run_df = prediction_run_df.drop(columns=[\"raw\"])\n",
    "    prediction_runs_dfs.append(prediction_run_df)\n",
    "\n",
    "assert len(prediction_runs_dfs) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society_Charity</td>\n",
       "      <td>1155074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1022119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1132766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1039369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Address (post town) Address (post code) Address (street)  \\\n",
       "0                  NaN                 NaN              NaN   \n",
       "1                  NaN                 NaN              NaN   \n",
       "2                  NaN                 NaN              NaN   \n",
       "3                  NaN                 NaN              NaN   \n",
       "4                  NaN                 NaN              NaN   \n",
       "\n",
       "                         Charity Name Charity Number Annual Income  \\\n",
       "0  Wormington_Village_Society_Charity        1155074           NaN   \n",
       "1                                 NaN        1022119           NaN   \n",
       "2                                 NaN         311722           NaN   \n",
       "3                                 NaN        1132766           NaN   \n",
       "4                                 NaN        1039369           NaN   \n",
       "\n",
       "  Period End Date  Annual Spending  \n",
       "0             NaN              NaN  \n",
       "1             NaN              NaN  \n",
       "2             NaN              NaN  \n",
       "3             NaN              NaN  \n",
       "4             NaN              NaN  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_runs_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 0 quotation marks in prediction run 0.\n",
      "Replaced 4 quotation marks in expected.\n"
     ]
    }
   ],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    count = replace_quotation_mark(prediction_run_df)\n",
    "    print(f\"Replaced {count} quotation marks in prediction run {i}.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(1)]\n",
    "own_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(1)]\n",
    "null_evaluations = [pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) for _ in range(1)]\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    for index, row in expected.iterrows():\n",
    "        for column in expected.columns:\n",
    "            if pd.notnull(row[column]): # because during parsing we look at the generations and if all subdocs are \"null\" or empty strings, it will not appear in the output and hence be NaN\n",
    "                # FP: we predicted null and it was not null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"FP\", column] += 1\n",
    "\n",
    "                # TN: we predicted not null (i.e. we predicted something) and it was not null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"TN\", column] += 1\n",
    "                if is_correct(column, row[column], prediction_run_df.loc[index, column]):\n",
    "                    own_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    own_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "                if str(row[column]).upper() == str(prediction_run_df.loc[index, column]).upper():\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "            else: # we don't care about the prediction in our own evaluation if the expected value is null\n",
    "                # TP: we predicted null and it was null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"TP\", column] += 1\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "\n",
    "                # FN: we predicted not null and it was null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"FN\", column] += 1\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro)[over runs] (own) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.000000  0.000000  0.000000       0.0\n",
      "Address (post code)  0.000000  0.000000  0.000000       0.0\n",
      "Address (street)     0.000000  0.000000  0.000000       0.0\n",
      "Charity Name         0.036364  0.036364  0.036364       0.0\n",
      "Charity Number       0.562929  0.562929  0.562929       0.0\n",
      "Annual Income        0.000000  0.000000  0.000000       0.0\n",
      "Period End Date      0.002273  0.002273  0.002273       0.0\n",
      "Annual Spending      0.000000  0.000000  0.000000       0.0\n",
      "(macro)[over runs and keys] (own) average of correctly predicted values: 0.075\n",
      "(macro)[over runs and keys] (own) range of correctly predicted values: 0.0\n",
      "(micro)[over all key-value pairs] (own) average of correctly predicted values: 0.077\n",
      "(micro)[over all key-value pairs] (own) sample standard deviation of correctly predicted values: 0.0\n",
      "(macro)[over runs] (official) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.040909  0.040909  0.040909       0.0\n",
      "Address (post code)  0.031818  0.031818  0.031818       0.0\n",
      "Address (street)     0.111364  0.111364  0.111364       0.0\n",
      "Charity Name         0.036364  0.036364  0.036364       0.0\n",
      "Charity Number       0.565909  0.565909  0.565909       0.0\n",
      "Annual Income        0.013636  0.013636  0.013636       0.0\n",
      "Period End Date      0.002273  0.002273  0.002273       0.0\n",
      "Annual Spending      0.013636  0.013636  0.013636       0.0\n",
      "(macro)[over runs and keys] (official) average of correctly predicted values: 0.102\n",
      "(macro)[over runs and keys] (official) range of correctly predicted values: 0.0\n",
      "(micro)[over all key-value pairs] (official) average of correctly predicted values: 0.102\n",
      "(micro)[over all key-value pairs] (official) sample standard deviation of correctly predicted values: 0.0\n"
     ]
    }
   ],
   "source": [
    "# own evaluation: only looks at the keys that are actually present in the document\n",
    "\n",
    "# we combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the own evaluations by key of each run\n",
    "avg_own_evaluation_by_key = pd.concat([own_evaluation.mean(axis=0, skipna=True) for own_evaluation in own_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (own) evaluation by key:\\n{avg_own_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (own) average of correctly predicted values: {round(avg_own_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (own) range of correctly predicted values: {round(avg_own_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")\n",
    "\n",
    "micro_averaged_accuracy = []\n",
    "for i, own_evaluation in enumerate(own_evaluations):\n",
    "    micro_averaged_accuracy.append(own_evaluation.sum().sum() / own_evaluation.count().sum())\n",
    "\n",
    "print(f\"(micro)[over all key-value pairs] (own) average of correctly predicted values: {round(np.mean(micro_averaged_accuracy), 3)}\")\n",
    "print(f\"(micro)[over all key-value pairs] (own) sample standard deviation of correctly predicted values: {round(np.std(micro_averaged_accuracy), 3)}\")\n",
    "\n",
    "# official evaluation (same as above but with the official evaluation)\n",
    "avg_official_evaluation_by_key = pd.concat([official_evaluation.mean(axis=0, skipna=True) for official_evaluation in official_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (official) evaluation by key:\\n{avg_official_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (official) average of correctly predicted values: {round(avg_official_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (official) range of correctly predicted values: {round(avg_official_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")\n",
    "\n",
    "micro_averaged_accuracy = []\n",
    "for i, official_evaluation in enumerate(official_evaluations):\n",
    "    micro_averaged_accuracy.append(official_evaluation.sum().sum() / official_evaluation.count().sum())\n",
    "\n",
    "print(f\"(micro)[over all key-value pairs] (official) average of correctly predicted values: {round(np.mean(micro_averaged_accuracy), 3)}\")\n",
    "print(f\"(micro)[over all key-value pairs] (official) sample standard deviation of correctly predicted values: {round(np.std(micro_averaged_accuracy), 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
