{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET DESIRED EVALUATION PARAMS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 'dev' # 'dev' or 'test'\n",
    "TEMPERATURE = '1' # '0', '0.1' or '1'\n",
    "MODEL = 'neox' # 'flan-t5', 'neox' or 'davinci'\n",
    "ONE_SHOT = False # True or False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Levenshtein import distance\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT == 'dev':\n",
    "    if MODEL == 'flan-t5':\n",
    "        pass\n",
    "    elif MODEL == 'neox':\n",
    "        if ONE_SHOT:\n",
    "            pass\n",
    "        else:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-14T18-27-21_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-44-54_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-15_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-14T18-27-46_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-42_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-54_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-11T00-56-12_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-12T02-26-28_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/neox/2022-12-11T00-25-32_neox_temp_1.log',\n",
    "                    '../logs/neox/2022-12-11T00-56-12_neox_temp_1.log',\n",
    "                    '../logs/neox/2022-12-12T02-26-28_neox_temp_1.log',\n",
    "                ]\n",
    "\n",
    "    elif MODEL == 'davinci':\n",
    "        if ONE_SHOT:\n",
    "            pass\n",
    "        else:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_Davinci(max_input_tokens=1024, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                \n",
    "\n",
    "\n",
    "elif SPLIT == 'test':\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading solution (expected.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WESTCLIFF-ON-SEA</td>\n",
       "      <td>SS0_8HX</td>\n",
       "      <td>47_SECOND_AVENUE</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>1022119</td>\n",
       "      <td>10348000.00</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>9415000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHELTENHAM</td>\n",
       "      <td>GL50_3EP</td>\n",
       "      <td>BAYSHILL_ROAD</td>\n",
       "      <td>Cheltenham_Ladies_College</td>\n",
       "      <td>311722</td>\n",
       "      <td>32168000.00</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>27972000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHREWSBURY</td>\n",
       "      <td>SY3_7PQ</td>\n",
       "      <td>58_TRINITY_STREET</td>\n",
       "      <td>The_Sanata_Charitable_Trust</td>\n",
       "      <td>1132766</td>\n",
       "      <td>255653.00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>258287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WARE</td>\n",
       "      <td>SG11_2DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>122836.00</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>124446.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Address (post town) Address (post code)   Address (street)  \\\n",
       "0            BROADWAY            WR12_7NL                NaN   \n",
       "1    WESTCLIFF-ON-SEA             SS0_8HX   47_SECOND_AVENUE   \n",
       "2          CHELTENHAM            GL50_3EP      BAYSHILL_ROAD   \n",
       "3          SHREWSBURY             SY3_7PQ  58_TRINITY_STREET   \n",
       "4                WARE            SG11_2DY                NaN   \n",
       "\n",
       "                  Charity Name Charity Number Annual Income Period End Date  \\\n",
       "0   Wormington_Village_Society        1155074           NaN      2018-07-31   \n",
       "1     Havens_Christian_Hospice        1022119   10348000.00      2016-03-31   \n",
       "2    Cheltenham_Ladies_College         311722   32168000.00      2016-07-31   \n",
       "3  The_Sanata_Charitable_Trust        1132766     255653.00      2015-12-31   \n",
       "4          Cantate_Youth_Choir        1039369     122836.00      2013-12-31   \n",
       "\n",
       "  Annual Spending  \n",
       "0             NaN  \n",
       "1      9415000.00  \n",
       "2     27972000.00  \n",
       "3       258287.00  \n",
       "4       124446.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SPLIT == 'dev':\n",
    "    expected = pd.read_csv('datasets/kleister_charity/dev-0/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "elif SPLIT == 'test':\n",
    "    expected = pd.read_csv('datasets/kleister_charity/test-A/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Address (street)\", \"Annual Income\",  \"Annual Spending\"]\n",
    "expected = expected[COLUMN_ORDER]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of keys that actually have a value (are not NaN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Address (post town)    0.959091\n",
       "Address (post code)    0.968182\n",
       "Address (street)       0.886364\n",
       "Charity Name           1.000000\n",
       "Charity Number         0.993182\n",
       "Annual Income          0.986364\n",
       "Period End Date        1.000000\n",
       "Annual Spending        0.986364\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage of keys that actually have a value (are not NaN):\")\n",
    "expected.count() / len(expected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading predictions (three runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_dfs = []\n",
    "for prediction_run_path in PREDICTION_RUNS_PATHS:\n",
    "    prediction_run_df = pd.read_csv(prediction_run_path, sep='\\t', header=None, names=['raw'], skip_blank_lines=False)\n",
    "\n",
    "    for raw_prediction in prediction_run_df[\"raw\"]:\n",
    "        if raw_prediction is np.nan:\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction] = np.nan\n",
    "            continue\n",
    "        key_value_pairs = raw_prediction.split(\" \")\n",
    "        for key_value in key_value_pairs:\n",
    "            key, value = key_value.split(\"=\", 1)\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction, key] = value\n",
    "    \n",
    "    prediction_run_df.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Annual Income\", \"Annual Spending\"]\n",
    "    prediction_run_df = prediction_run_df[COLUMN_ORDER]\n",
    "    prediction_run_df = prediction_run_df.drop(columns=[\"raw\"])\n",
    "    prediction_runs_dfs.append(prediction_run_df)\n",
    "\n",
    "assert len(prediction_runs_dfs) == 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading associated logs of predictions (three runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_logs = []\n",
    "for log_path in LOG_PATHS:\n",
    "    log_file = open(log_path, \"r\")\n",
    "    log_lines = [line.strip() for line in log_file.readlines() if line.strip() and line.startswith(\"20\") and \"Raw value:\" not in line]\n",
    "    log_file.close()\n",
    "    prediction_runs_logs.append(log_lines)\n",
    "\n",
    "assert len(prediction_runs_logs) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61145"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_runs_logs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations (answering RH1)\n",
    "## RH1\n",
    "> Unimodal approach can reach satisfactory performance while being more cost-efficient than current state-of-the-art multi-modal approaches\n",
    "\n",
    "Where we define satisfactory performance as:\n",
    "> 80% of the values for given and findable keys are correctly found (no distinction for the other 20%, they can be either wrong or missing (which is of course also wrong)). Correctness is defined as a case-insensitive (upper-casing everything) string match with some normalisation (details below)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise values for three keys as listed below. We don't normalise values of `Address (post code)`, `Charity Number`, `Period End Date`, `Annual Income`, `Annual Spending`.\n",
    "\n",
    "\n",
    "### Address (post town)\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "OTTERY_ST_MARY | Ottery_St._Mary\n",
    "Lichfield | City_of_Lichfield\n",
    "Liverpool | City_of_Liverpool\n",
    "\n",
    "Normalisation:\n",
    "* `<Solution City>` vs. `City of <Solution City>` are both correct\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition, but not a substitution) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary)\n",
    "\n",
    "### Address (street)\n",
    "Examples: \n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "36_BELLINGHAM_DRIVE | Unit_36_Bellingham_Drive\n",
    "34_DECIMA_STREET | Sherborne_House,_34_Decima_Street\n",
    "190_LONG_LANE | Scout_Centre,_Rear_190_Long_Lane\n",
    "13_ROSSLYN_ROAD | Room_16,_ETNA_Community_Centre,_13_Rosslyn_Road\n",
    "FURNIVAL_GATE | 2_Floor,_Midcity_House,_Furnival_Gate\n",
    "7-14_Great_Dover_Street | 7_-_14_Great_Dover_Street\n",
    "BROWNBERRIE_LANE | Leeds_Trinity_University,_Brownberrie_Lane\n",
    "\n",
    "Normalisation: \n",
    "* Delete Spaces around \"-\" in both solution and prediction\n",
    "\n",
    "Was considering generally cutting off at ,_ before or after the street but ultimately decided against it because it cannot be generally stated that having something in front or after the correct street would still make mail arrive at the destination.\n",
    "\n",
    "Also: Levenshtein edit distance of 1 doesn't make sense here as getting the number wrong (e.g. 13 instead of 1) is a clear mistake.\n",
    "\n",
    "### Charity Name\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "Cheltenham Ladies College | Cheltenham Ladies' College\n",
    "Battersea_Dogs'_and_Cats'_Home | Battersea_Dogs'_&_Cats'_Home\n",
    "Beer_Shmuel_Ltd. | Beer_Shmuel_Limited\n",
    "Catch_22_Charity_Ltd. | Catch22\n",
    "Richard_Hicks | Richard_Hicks_Charity\n",
    "King's_Schools_Taunton_Ltd. | King's_Schools_Taunton_Limited\n",
    "KEY_ENTERPRISES_(1983)_LTD. | KEY_ENTERPRISES_(1983)_LIMITED\n",
    "Louth_Playgoers_Society_Ltd. | Louth_Playgoers_Society_Limited\n",
    "Boxgrove_Village_Hall_and_Community_Centre | BOXGROVE_VILLAGE_HALL_&_COMMUNITY_CENTRE_CIO\n",
    "London_Transport_Museum | London_Transport_Museum_Ltd.\n",
    "The_Momc-Leigh_Park_Crafts_Initiative_Trust_Ltd. | THE_MOMC_-_LEIGH_PARK_CRAFTS_INITIATIVE_TRUST_LIMITED\n",
    "King_Edward_Vi's_School_At_Chelmsford | King_Edward_VI_School_at_Chelmsford\n",
    "The_Hope_Foundation_Ltd. | The_Hope_Foundation\n",
    "Nottingham_Women's_Counselling_Service | The_Nottingham_Women's_Counselling_Service\n",
    "\n",
    "Normalisation (+ give stats for how many values this applies):\n",
    "* Cut off Ltd, Ltd. and Limited from the end of both prediction and solution \n",
    "* Replace \"&\" with \"and\" in both prediction and solution\n",
    "* Delete Spaces around \"-\" in both prediction and solution\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary, King_Edward_VI's_School vs. King_Edward_VI_School)\n",
    "\n",
    "\n",
    "### Other Normalisations\n",
    "Replaced uncommon character: ’ (U+2019) with ' (in the predictions and the solution) (applies to 4 values in the whole solution of the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 8 quotation marks in prediction run 0.\n",
      "Replaced 3 quotation marks in prediction run 1.\n",
      "Replaced 8 quotation marks in prediction run 2.\n",
      "Replaced 4 quotation marks in expected.\n"
     ]
    }
   ],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    count = replace_quotation_mark(prediction_run_df)\n",
    "    print(f\"Replaced {count} quotation marks in prediction run {i}.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating according to own definition of \"correctness\" and official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(3)]\n",
    "own_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(3)]\n",
    "null_evaluations = [pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) for _ in range(3)]\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    for index, row in expected.iterrows():\n",
    "        for column in expected.columns:\n",
    "            if pd.notnull(row[column]): # because during parsing we look at the generations and if all subdocs are \"null\" or empty strings, it will not appear in the output and hence be NaN\n",
    "                # FP: we predicted null and it was not null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"FP\", column] += 1\n",
    "\n",
    "                # TN: we predicted not null (i.e. we predicted something) and it was not null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"TN\", column] += 1\n",
    "                if is_correct(column, row[column], prediction_run_df.loc[index, column]):\n",
    "                    own_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    own_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "                if str(row[column]).upper() == str(prediction_run_df.loc[index, column]).upper():\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "            else: # we don't care about the prediction in our own evaluation if the expected value is null\n",
    "                # TP: we predicted null and it was null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"TP\", column] += 1\n",
    "                # FN: we predicted not null and it was null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"FN\", column] += 1\n",
    "\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general we take macro average over the runs as it gives equal importance to each run (on the other hand, micro would give more importance to runs that have more predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro)[over runs] (own) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.154028  0.146919  0.163507  0.016588\n",
      "Address (post code)  0.029734  0.023474  0.037559  0.014085\n",
      "Address (street)     0.026496  0.015385  0.035897  0.020513\n",
      "Charity Name         0.387879  0.365909  0.427273  0.061364\n",
      "Charity Number       0.168574  0.146453  0.199085  0.052632\n",
      "Annual Income        0.000000  0.000000  0.000000  0.000000\n",
      "Period End Date      0.000000  0.000000  0.000000  0.000000\n",
      "Annual Spending      0.001536  0.000000  0.002304  0.002304\n",
      "(macro)[over runs and keys] (own) average of correctly predicted values: 0.09603082699350793\n",
      "(macro)[over runs] (official) evaluation by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.153788  0.143182  0.168182  0.025000\n",
      "Address (post code)  0.043182  0.036364  0.056818  0.020455\n",
      "Address (street)     0.043939  0.031818  0.054545  0.022727\n",
      "Charity Name         0.324242  0.304545  0.356818  0.052273\n",
      "Charity Number       0.172727  0.150000  0.202273  0.052273\n",
      "Annual Income        0.006818  0.004545  0.009091  0.004545\n",
      "Period End Date      0.000000  0.000000  0.000000  0.000000\n",
      "Annual Spending      0.005303  0.002273  0.006818  0.004545\n",
      "(macro)[over runs and keys] (official) average of correctly predicted values: 0.09375\n"
     ]
    }
   ],
   "source": [
    "# own evaluation: only looks at the keys that are actually present in the document\n",
    "\n",
    "# combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the own evaluations by key of each run\n",
    "avg_own_evaluation_by_key = pd.concat([own_evaluation.mean(axis=0, skipna=True) for own_evaluation in own_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (own) evaluation by key:\\n{avg_own_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (own) average of correctly predicted values: {avg_own_evaluation_by_key['mean'].agg('mean')}\")\n",
    "\n",
    "# official evaluation (same as above but with the official evaluation)\n",
    "avg_official_evaluation_by_key = pd.concat([official_evaluation.mean(axis=0, skipna=True) for official_evaluation in official_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (official) evaluation by key:\\n{avg_official_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (official) average of correctly predicted values: {avg_official_evaluation_by_key['mean'].agg('mean')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that the official evaluation is actually higher than our evaluation because it rewards correctly identifying null values whereas we don't care about the key-value pairs that are expected to be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro)[over runs] Correlation coefficient between own and official evaluation (by key): 0.994\n"
     ]
    }
   ],
   "source": [
    "print(f\"(macro)[over runs] Correlation coefficient between own and official evaluation (by key): {round(np.corrcoef(avg_own_evaluation_by_key['mean'], avg_official_evaluation_by_key['mean'])[0, 1], 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we want to answer here is how good we are with predicting `null` for a key (not in subdocs, but for whole document), specifically we also look at the F1 for only `null` → `F_1_{null}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(macro)[over runs] Precision for null by key:\n",
      "                         mean       min       max\n",
      "Address (post town)  0.074106  0.052632  0.103896\n",
      "Address (post code)  0.074790  0.047059  0.105882\n",
      "Address (street)     0.115940  0.102564  0.129870\n",
      "Charity Name         0.000000  0.000000  0.000000\n",
      "Charity Number       0.043902  0.034483  0.055556\n",
      "Annual Income        0.016420  0.009901  0.022599\n",
      "Period End Date      0.000000  0.000000  0.000000\n",
      "Annual Spending      0.034972  0.000000  0.073171\n",
      "(macro)[over runs] Recall for null by key:\n",
      "                         mean       min       max\n",
      "Address (post town)  0.314815  0.222222  0.444444\n",
      "Address (post code)  0.452381  0.285714  0.642857\n",
      "Address (street)     0.180000  0.160000  0.200000\n",
      "Charity Name              NaN       NaN       NaN\n",
      "Charity Number       0.777778  0.666667  1.000000\n",
      "Annual Income        0.500000  0.333333  0.666667\n",
      "Period End Date           NaN       NaN       NaN\n",
      "Annual Spending      0.277778  0.000000  0.500000\n",
      "(macro)[over runs] F1 score for null by key:\n",
      "                         mean       min       max\n",
      "Address (post town)  0.119970  0.085106  0.168421\n",
      "Address (post code)  0.128358  0.080808  0.181818\n",
      "Address (street)     0.141035  0.125000  0.157480\n",
      "Charity Name              NaN       NaN       NaN\n",
      "Charity Number       0.083089  0.065574  0.105263\n",
      "Annual Income        0.031793  0.019231  0.043716\n",
      "Period End Date           NaN       NaN       NaN\n",
      "Annual Spending      0.092815  0.057971  0.127660\n",
      "(macro)[over runs with micro average] Precision for null: 0.04080002271576191\n",
      "(macro)[over runs with micro average] Recall for null: 0.288659793814433\n",
      "(macro)[over runs with micro average] F1 score for null: 0.07147524960896341\n",
      "(macro)[over runs with macro over keys] F1 score for null: 0.09954786346191481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t1/8_xptc456_9bnyk0lkdrjql00000gn/T/ipykernel_9109/2220019627.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n",
      "/var/folders/t1/8_xptc456_9bnyk0lkdrjql00000gn/T/ipykernel_9109/2220019627.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1 = 2 * precision * recall / (precision + recall)\n"
     ]
    }
   ],
   "source": [
    "null_scores_by_key_runs = []\n",
    "for null_evaluation in null_evaluations:\n",
    "    # micro average\n",
    "    precision = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FP\", :].sum())\n",
    "    recall = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FN\", :].sum())\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # aggregating by key for later macro average\n",
    "    null_scores_by_key = {}\n",
    "    for key in null_evaluation.columns:   \n",
    "        precision = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FP\", key]) \n",
    "        recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        null_scores_by_key[key] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    null_scores_by_key = pd.DataFrame(null_scores_by_key).T\n",
    "    null_scores_by_key_runs.append(null_scores_by_key)\n",
    "\n",
    "print(f\"(macro)[over runs] Precision for null by key:\\n{pd.concat([null_scores_by_key['precision'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "print(f\"(macro)[over runs] Recall for null by key:\\n{pd.concat([null_scores_by_key['recall'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "print(f\"(macro)[over runs] F1 score for null by key:\\n{pd.concat([null_scores_by_key['f1'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "\n",
    "\n",
    "print(f\"(macro)[over runs with micro average] Precision for null: {np.mean([null_evaluation.loc['TP', :].sum() / (null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FP', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "print(f\"(macro)[over runs with micro average] Recall for null: {np.mean([null_evaluation.loc['TP', :].sum() / (null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FN', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "print(f\"(macro)[over runs with micro average] F1 score for null: {np.mean([2 * null_evaluation.loc['TP', :].sum() / (2 * null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FP', :].sum() + null_evaluation.loc['FN', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "\n",
    "print(f\"(macro)[over runs with macro over keys] F1 score for null: {np.mean([null_scores_by_key['f1'].mean() for null_scores_by_key in null_scores_by_key_runs])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that micro and macro average of the  are quite close together because as we saw in the beginning, almost all keys are given in the data set. There is no big \"class\" (key) imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifications, collisions, lenient accuracy and looking at repetetiveness (all on subdocument level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Unification\n",
    "A unification is whenever there are two or more non-null values coming from the subdoc predictions for the same key. If there is only one non-null value coming from the subdocs (which is always the case if there only is one subdoc but can also happen with more than one subdoc) then it's not a unification.\n",
    "\n",
    "A trivial unification is a unification where all values are the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Collision\n",
    "Given a unification, we describe two or more different values for the same key as the unification having a collision. \n",
    "\n",
    "So a unification is either trivial (all values the same) or it has a collision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a Full Collision\n",
    "Same as a collision but with the constraint that *all* values are different (not just any two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivo/Library/Caches/pypoetry/virtualenvs/uni-kie-JrmAaldC-py3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/ivo/Library/Caches/pypoetry/virtualenvs/uni-kie-JrmAaldC-py3.8/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty string predictions: 11100\n",
      "Processing run 1\n",
      "Number of empty string predictions: 11069\n",
      "Processing run 2\n",
      "Number of empty string predictions: 11227\n"
     ]
    }
   ],
   "source": [
    "empty_key_dict = {\n",
    "    \"Address (post town)\": None,\n",
    "    \"Address (post code)\": None,\n",
    "    \"Address (street)\": None,\n",
    "    \"Charity Name\": None,\n",
    "    \"Charity Number\": None,\n",
    "    \"Annual Income\": None,\n",
    "    \"Period End Date\": None,\n",
    "    \"Annual Spending\": None,\n",
    "}\n",
    "\n",
    "runs_prediction_stats_dict = []\n",
    "runs_predictions_dict = []\n",
    "\n",
    "for i, log_lines in enumerate(prediction_runs_logs):\n",
    "    print(f\"Processing run {i}\")\n",
    "    single_run_prediction_stats_dict = []\n",
    "    single_run_predictions_dict = []\n",
    "\n",
    "    single_run_num_empty_string_predictions = 0\n",
    "    for line in log_lines:\n",
    "        if \"Predicting document\" in line: # this is the beginning of a prediction\n",
    "            # create a new dictionary for this document\n",
    "            single_run_prediction_stats_dict.append({\n",
    "                \"num_subdocs\": None,\n",
    "                \"num_unifications\": 0,\n",
    "                \"collision_per_key\": empty_key_dict.copy(),\n",
    "                \"full_collision_per_key\": empty_key_dict.copy(),\n",
    "                \"num_unified_values_per_key\": empty_key_dict.copy(),\n",
    "                \"correct_in_any_subdoc_per_key\": empty_key_dict.copy(),\n",
    "                \"collision_percentage\": None,\n",
    "                \"full_collision_percentage\": None,\n",
    "                \"correct_in_any_subdoc_percentage\": None,\n",
    "            })\n",
    "            single_run_predictions_dict.append(empty_key_dict.copy())\n",
    "\n",
    "        elif \"Final prediction for document\" in line: # this is the end of a prediction\n",
    "            # calculate the percentages\n",
    "            single_run_prediction_stats_dict[-1][\"collision_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"collision_per_key\"].values() if x is not None])\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"full_collision_per_key\"].values() if x is not None])\n",
    "            single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"].values() if x is not None])\n",
    "    \n",
    "        elif \"No subdocs necessary\" in line:\n",
    "            single_run_prediction_stats_dict[-1][\"num_subdocs\"] = 1\n",
    "\n",
    "        elif \"Split document into\" in line:\n",
    "            num_subdocs = int(re.search(\"into (\\d+) subdocuments\", line).group(1))\n",
    "            single_run_prediction_stats_dict[-1][\"num_subdocs\"] = num_subdocs\n",
    "\n",
    "        elif \"- parse_model_output() - Key:\" in line:\n",
    "            key = re.search(\"- parse_model_output\\(\\) - Key: (.*):\", line).group(1)\n",
    "            # the prediction is always in the next line (unless the key was not predicted at all) or the prediction is an empty string\n",
    "            try:\n",
    "                prediction = re.search(\"- parse_model_output\\(\\) - Stripped value: (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "                \n",
    "\n",
    "            except AttributeError:\n",
    "                single_run_num_empty_string_predictions += 1\n",
    "                prediction = \"[METADATA]: EMPTY_STRING_PREDICTION\"\n",
    "                \n",
    "            if single_run_predictions_dict[-1][key] is None:\n",
    "                    single_run_predictions_dict[-1][key] = [prediction]\n",
    "            else:\n",
    "                single_run_predictions_dict[-1][key].append(prediction)\n",
    "\n",
    "        elif \"Unification necessary for key\" in line:\n",
    "            key = re.search(\"Unification necessary for key (.*)\", line).group(1)\n",
    "            single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "            values = re.search(\"Unifying \\d+ \\(lowered\\) values (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "\n",
    "            # values is the string representation of a list, so we can use eval to turn it into a list\n",
    "            values = eval(values)\n",
    "            unified_values = int(re.search(\"Unifying (\\d+) \\(lowered\\) values\", log_lines[log_lines.index(line)+1]).group(1))\n",
    "\n",
    "            assert unified_values == len(values) # sanity check\n",
    "\n",
    "            # if there is more than 1 value, then it's a unification (because these values don't include null values)\n",
    "            if len(values) > 1:\n",
    "                single_run_prediction_stats_dict[-1][\"num_unifications\"] += 1\n",
    "\n",
    "                # if there are more than 1 different values, then it's a collision\n",
    "                if len(set(values)) > 1:\n",
    "                    single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = True\n",
    "\n",
    "                # if the length of the set is equal to the length of the list, then it's a full collision\n",
    "                if len(set(values)) == len(values):\n",
    "                    single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = True\n",
    "\n",
    "            \n",
    "            single_run_prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = unified_values\n",
    "\n",
    "            # which document are we in?\n",
    "            doc_num = len(single_run_prediction_stats_dict) - 1\n",
    "\n",
    "            # get the correct value for this key\n",
    "            correct_value = expected.iloc[doc_num][key]\n",
    "\n",
    "            # if it's NaN, then we were not supposed to predict anything for this key but we did (in at least one subdoc)\n",
    "            if pd.isna(correct_value):\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False # TODO: ask if this is correct\n",
    "                continue\n",
    "\n",
    "            # we have to transform the values in the list to the same format as the correct value\n",
    "            values = [x.replace(\" \", \"_\").replace(\":\", \"_\").upper() for x in values]\n",
    "\n",
    "            # also transform the correct value to uppercase\n",
    "            correct_value = str(correct_value).upper()\n",
    "\n",
    "            # if the correct value is in the list of values, then it's correct in at least one subdoc\n",
    "            if correct_value in values:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "\n",
    "            else:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "        elif \"Key not found in any subdoc\" in line: # null was predicted in all subdocs\n",
    "            key = re.search(\"Key not found in any subdoc (.*)\", line).group(1)\n",
    "            single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = 0\n",
    "\n",
    "            # get the correct value for this key\n",
    "            correct_value = expected.iloc[len(single_run_prediction_stats_dict) - 1][key]\n",
    "\n",
    "            if pd.isna(correct_value):\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "            else:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "    print(\"Number of empty string predictions:\", single_run_num_empty_string_predictions)\n",
    "    runs_prediction_stats_dict.append(single_run_prediction_stats_dict)\n",
    "    runs_predictions_dict.append(single_run_predictions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -> all docs with x number of subdocs should have x predictions for each key (also for the keys that have an empty string prediction\n",
    "# because we added [METADATA]: EMPTY_STRING_PREDICTION to the list of predictions\n",
    "for i in range(len(runs_prediction_stats_dict)):\n",
    "    for j in range (len(runs_prediction_stats_dict[i])):\n",
    "        for key in single_run_predictions_dict[j].keys():\n",
    "            assert len(single_run_predictions_dict[j][key]) == single_run_prediction_stats_dict[j][\"num_subdocs\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdocs</th>\n",
       "      <th>num_unifications</th>\n",
       "      <th>collision_per_key</th>\n",
       "      <th>full_collision_per_key</th>\n",
       "      <th>num_unified_values_per_key</th>\n",
       "      <th>correct_in_any_subdoc_per_key</th>\n",
       "      <th>collision_percentage</th>\n",
       "      <th>full_collision_percentage</th>\n",
       "      <th>correct_in_any_subdoc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': None, 'Address (post c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': 6, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': 7, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>{'Address (post town)': 0, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': True, 'Address (post c...</td>\n",
       "      <td>{'Address (post town)': 3, 'Address (post code...</td>\n",
       "      <td>{'Address (post town)': False, 'Address (post ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdocs  num_unifications  \\\n",
       "0            1                 0   \n",
       "1           18                 8   \n",
       "2           18                 8   \n",
       "3            3                 0   \n",
       "4            6                 8   \n",
       "\n",
       "                                   collision_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': True, 'Address (post c...   \n",
       "2  {'Address (post town)': True, 'Address (post c...   \n",
       "3  {'Address (post town)': False, 'Address (post ...   \n",
       "4  {'Address (post town)': True, 'Address (post c...   \n",
       "\n",
       "                              full_collision_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': True, 'Address (post c...   \n",
       "2  {'Address (post town)': True, 'Address (post c...   \n",
       "3  {'Address (post town)': False, 'Address (post ...   \n",
       "4  {'Address (post town)': True, 'Address (post c...   \n",
       "\n",
       "                          num_unified_values_per_key  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...   \n",
       "1  {'Address (post town)': 6, 'Address (post code...   \n",
       "2  {'Address (post town)': 7, 'Address (post code...   \n",
       "3  {'Address (post town)': 0, 'Address (post code...   \n",
       "4  {'Address (post town)': 3, 'Address (post code...   \n",
       "\n",
       "                       correct_in_any_subdoc_per_key  collision_percentage  \\\n",
       "0  {'Address (post town)': None, 'Address (post c...                   NaN   \n",
       "1  {'Address (post town)': False, 'Address (post ...                   1.0   \n",
       "2  {'Address (post town)': False, 'Address (post ...                   1.0   \n",
       "3  {'Address (post town)': False, 'Address (post ...                   0.0   \n",
       "4  {'Address (post town)': False, 'Address (post ...                   1.0   \n",
       "\n",
       "   full_collision_percentage  correct_in_any_subdoc_percentage  \n",
       "0                        NaN                               NaN  \n",
       "1                       0.75                             0.250  \n",
       "2                       1.00                             0.000  \n",
       "3                       0.00                             0.000  \n",
       "4                       0.75                             0.375  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_prediction_stats_df =[pd.DataFrame(runs_prediction_stats_dict[i]) for i in range(len(runs_prediction_stats_dict))]\n",
    "runs_prediction_stats_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in column `correct_in_any_subdoc_percentage` with the accuracy as per official_evaluation (this is not recorded in the log traversal)\n",
    "for i in range(len(runs_prediction_stats_df)):\n",
    "    runs_prediction_stats_df[i][\"correct_in_any_subdoc_percentage\"] = runs_prediction_stats_df[i][\"correct_in_any_subdoc_percentage\"].fillna(official_evaluations[i].mean(axis=1, skipna=True))\n",
    "\n",
    "# # replace None values in column `correct_in_any_subdoc_per_key` of the rows with 1 subdoc with True/False as taken from the official_evaluation\n",
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    for j in range(len(single_run_prediction_stats_df)):\n",
    "        if single_run_prediction_stats_df.iloc[j][\"num_subdocs\"] == 1:\n",
    "            for key in single_run_prediction_stats_df.iloc[j][\"correct_in_any_subdoc_per_key\"].keys():\n",
    "                single_run_prediction_stats_df.iloc[j][\"correct_in_any_subdoc_per_key\"][key] = bool(official_evaluations[i].iloc[j][key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenient Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the case that not in any subdoc a value was predicted (all null) we check if the correct solution is in fact null and then consider that in any subdoc the corect value (which is null) was found (in reality it was correctly identified in all). We cannot do the opposite (check if it was null in any subdoc) and then say it was correctly identified (if it was indeed null) because of the subdoc structure where any given subdoc is not guaranteed to have all key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "(official) (macro)[average over the keys] Lenient accuracy: 0.16960227272727274\n",
      "(official) (macro)[average over documents] Lenient accuracy: 0.16960227272727274\n",
      "Run 1\n",
      "(official) (macro)[average over the keys] Lenient accuracy: 0.1971590909090909\n",
      "(official) (macro)[average over documents] Lenient accuracy: 0.19715909090909092\n",
      "Run 2\n",
      "(official) (macro)[average over the keys] Lenient accuracy: 0.17727272727272728\n",
      "(official) (macro)[average over documents] Lenient accuracy: 0.17727272727272728\n",
      "Combined runs:\n",
      "(official) (macro)[average over the runs] Lenient accuracy by key:\n",
      "                         mean       min       max  <lambda>\n",
      "Address (post town)  0.228788  0.220455  0.240909  0.020455\n",
      "Address (post code)  0.052273  0.043182  0.068182  0.025000\n",
      "Address (street)     0.056818  0.040909  0.072727  0.031818\n",
      "Charity Name         0.406061  0.390909  0.436364  0.045455\n",
      "Charity Number       0.218182  0.204545  0.245455  0.040909\n",
      "Annual Income        0.012121  0.006818  0.018182  0.011364\n",
      "Period End Date      0.470455  0.436364  0.488636  0.052273\n",
      "Annual Spending      0.006061  0.004545  0.006818  0.002273\n",
      "(official) (macro)[average over the runs and the keys] Lenient accuracy: 0.144\n"
     ]
    }
   ],
   "source": [
    "runs_lenient_accuracy_by_key = []\n",
    "\n",
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "\n",
    "    # a single entry is a dictionary with the correctness for each key (True, False, or None)\n",
    "    lenient_correctness_over_docs = single_run_prediction_stats_df[\"correct_in_any_subdoc_per_key\"].tolist()\n",
    "\n",
    "    single_run_lenient_accuracy_by_key = pd.DataFrame()\n",
    "\n",
    "    for key in lenient_correctness_over_docs[0].keys():\n",
    "        correctness = [x[key] for x in lenient_correctness_over_docs]\n",
    "        avg_lenient_accuracy = np.mean([x for x in correctness if x is not None])\n",
    "        single_run_lenient_accuracy_by_key[key] = [avg_lenient_accuracy]\n",
    "\n",
    "    runs_lenient_accuracy_by_key.append(single_run_lenient_accuracy_by_key)\n",
    "    print(\"Run\", i)\n",
    "    print(f\"(official) (macro)[average over the keys] Lenient accuracy: {single_run_lenient_accuracy_by_key.mean(axis=1).mean()}\")\n",
    "    print(f\"(official) (macro)[average over documents] Lenient accuracy: {single_run_prediction_stats_df['correct_in_any_subdoc_percentage'].mean()}\")\n",
    "\n",
    "print(\"Combined runs:\")\n",
    "# combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the lenient accuracies by key of each run\n",
    "avg_lenient_accuracy_by_key = pd.concat([lenient_accuracy_by_key for lenient_accuracy_by_key in runs_lenient_accuracy_by_key], axis=0).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=0)\n",
    "print(f\"(official) (macro)[average over the runs] Lenient accuracy by key:\\n{avg_lenient_accuracy_by_key.T}\")\n",
    "print(f\"(official) (macro)[average over the runs and the keys] Lenient accuracy: {round(avg_lenient_accuracy_by_key.mean(axis=1).mean(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'official_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy_by_key \u001b[39m=\u001b[39m official_evaluation\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m(official) Relative improvement in accuracy (lenient vs. strict):\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m accuracy_by_key\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'official_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_by_key = official_evaluation.mean(axis=0, skipna=True)\n",
    "print(\"(official) Relative improvement in accuracy (lenient vs. strict):\")\n",
    "for key in accuracy_by_key.keys():\n",
    "    print(f\"{key}: {round((single_run_lenient_accuracy_by_key[key].values[0] - accuracy_by_key[key]) / accuracy_by_key[key] * 100, 3)}%\")\n",
    "\n",
    "print(\"(macro) (official) Relative improvement in accuracy (lenient vs. strict) (average over the keys):\", round((single_run_lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean()) / accuracy_by_key.mean() * 100, 3), \"%\")\n",
    "print(\"(macro) (official) Absolute improvement in accuracy (lenient vs. strict) (average over the keys):\", round(single_run_lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean(), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and presence penalty motivation for not looking at those parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check how often the same value for different keys is predicted to see if it's generally correct to assume that values shouldn't be repeated (which could be combated by e.g. increasing the freq. and presence penalties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_values_for_different_keys(row):\n",
    "    return len(row) == len(set(row))\n",
    "\n",
    "num_of_same_value_rows = 0\n",
    "for i in range(len(expected)):\n",
    "    if not has_two_values_for_different_keys(expected.iloc[i]):\n",
    "        num_of_same_value_rows += 1\n",
    "\n",
    "print(f\"Found {num_of_same_value_rows} rows that have the same value for different keys, that's {round(num_of_same_value_rows / len(expected) * 100, 3)}% of the dataset.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, there shouldn't be the same value for different keys.\n",
    "A repetition of the same value would also be correct in the case of a document having more than 1 null value, where a correct generation would contain `\"null\"` multiple times. Let's look at how often this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents that have 2 or more NaN values for any two keys:\")\n",
    "expected[expected.isna().sum(axis=1) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_missing_value_in_solution_pct = round(len(expected[expected.isna().sum(axis=1) >= 2]) / len(expected) * 100, 3)\n",
    "print(f\"Percentage of documents that have 2 or more NaN values for any two keys: {more_than_one_missing_value_in_solution_pct}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, we shouldn't have more than 1 null in our generations, so we can now generally assume that repetition of the same value in the generations is incorrect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check how often we have repetition (i.e. the same value (including null's) for different keys) in the (subdoc) predictions. If this is very high then it would make sense to look into tuning the presence and frequency penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(single_run_predictions_dict)):\n",
    "    single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"] = {} # this is min 0.125 (1/8) in the case that all predictions are the same (e.g. all 'null') and max 1.0 (all predictions are unique)\n",
    "    single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"] = {} # this is nan if all predictions are null, and also min 0.125\n",
    "    for subdoc in range(single_run_prediction_stats_dict[i][\"num_subdocs\"]):\n",
    "        # get the predictions for each key\n",
    "        predictions_for_keys = [single_run_predictions_dict[i][key][subdoc] for key in single_run_predictions_dict[i].keys()]\n",
    "        # get the number of unique predictions\n",
    "        num_unique_predictions = len(set(predictions_for_keys)) # this is at least 1, cannot be 0\n",
    "        # add the percentage of unique predictions to the dictionary\n",
    "        single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] = num_unique_predictions / len(predictions_for_keys)\n",
    "\n",
    "        # get the number of unique predictions ignoring null values\n",
    "        num_unique_predictions_ignore_null = len(set([x for x in predictions_for_keys if x.upper() != \"NULL\" and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]))\n",
    "\n",
    "        num_predictions_ignore_null = len([x for x in predictions_for_keys if x.upper() != \"NULL\" and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]) # this is 0 if all the predictions are null or empty string\n",
    "        # in this case, we set the number of unique predictions to nan\n",
    "        if num_unique_predictions_ignore_null == 0:\n",
    "            single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = np.nan\n",
    "            continue\n",
    "        # add the percentage of unique predictions (ignoring null) to the dictionary\n",
    "        single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = num_unique_predictions_ignore_null / num_predictions_ignore_null\n",
    "\n",
    "print(\"(micro) average percentage of unique predictions (ignoring null) per subdoc:\", np.nanmean([single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for i in range(len(single_run_predictions_dict)) for subdoc in range(single_run_prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "print(\"(micro) average percentage of unique predictions per subdoc:\", np.nanmean([single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for i in range(len(single_run_predictions_dict)) for subdoc in range(single_run_prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "\n",
    "print(\"(macro)[over subdocs for each doc] percentage of unique predictions (ignoring null) per subdoc:\", np.nanmean([np.nanmean([single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for subdoc in range(single_run_prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(single_run_predictions_dict))]))\n",
    "print(\"(macro)[over subdocs for each doc] percentage of unique predictions per subdoc:\", np.nanmean([np.nanmean([single_run_prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for subdoc in range(single_run_prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(single_run_predictions_dict))]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to be expected that there is quite some repetition when not ignoring null values because many times the subdoc will only contain few or even none of the keys. When we ignore null values, repetition is almost non-existent, hence we can reasonably assume that tuning the frequency and presence penalty parameters would not improve results by much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_prediction_stats_df.loc[:, \"num_non_null_values_per_key\"] = None\n",
    "for i in range(len(single_run_predictions_dict)):\n",
    "    runs_prediction_stats_df.at[i, \"num_non_null_values_per_key\"] = {key: len([x for x in single_run_predictions_dict[i][key] if x.upper() != \"NULL\" and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]) for key in single_run_predictions_dict[i].keys()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding no_collision percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column \"no_collision_percentage\" which is the percentage of keys that had no collision\n",
    "runs_prediction_stats_df.loc[:, \"no_collision_percentage\"] = 1 - runs_prediction_stats_df.loc[:, \"collision_percentage\"]\n",
    "\n",
    "for i, row in runs_prediction_stats_df.iterrows():\n",
    "    num_keys = len([x for x in row[\"collision_per_key\"].values() if x is not None])\n",
    "    num_no_collisions = len([x for x in row[\"collision_per_key\"].values() if x is False])\n",
    "    try:\n",
    "        runs_prediction_stats_df.loc[i, \"no_collision_percentage_calculated\"] = num_no_collisions / num_keys\n",
    "    except ZeroDivisionError:\n",
    "        runs_prediction_stats_df.loc[i, \"no_collision_percentage_calculated\"] = np.nan\n",
    "\n",
    "assert np.allclose(runs_prediction_stats_df[\"collision_percentage\"], 1 - runs_prediction_stats_df[\"no_collision_percentage_calculated\"], equal_nan=True) # sanity check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs_with_subdocs = len(runs_prediction_stats_df[runs_prediction_stats_df['num_subdocs'] > 1])\n",
    "print(f\"Number of documents that were split into more than 1 subdocument: {num_of_docs_with_subdocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs_with_no_subdocs = len(runs_prediction_stats_df[runs_prediction_stats_df[\"num_subdocs\"] == 1])\n",
    "print(f\"Number of documents that didn't need to be split: {num_of_docs_with_no_subdocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = runs_prediction_stats_df['num_subdocs'].value_counts().sort_index().index\n",
    "y = runs_prediction_stats_df['num_subdocs'].value_counts().sort_index().values\n",
    "\n",
    "max_subdocs = max(x)\n",
    "\n",
    "# add a 0 for each number of subdocs up to the maximum number of subdocs, so that there is a value for each number of subdocs\n",
    "for i in range(1, max_subdocs+1):\n",
    "    if i not in x:\n",
    "        x = np.append(x, i)\n",
    "        y = np.append(y, 0)\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(0.5, max_subdocs)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments needed\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Histogram of Number of Needed Subdocuments\")\n",
    "\n",
    "# plt.subplots_adjust(top=0.5)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_subdoc_hist.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_values_per_key = {}\n",
    "\n",
    "for i in range(len(runs_prediction_stats_df)):\n",
    "    for key, value in runs_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        if key in non_null_values_per_key:\n",
    "            non_null_values_per_key[key].append(value)\n",
    "        else:\n",
    "            non_null_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "occurence_dict = {}\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "# intersperse keys that are missing with value 0\n",
    "for i in range(0, max(occurence_dict.keys())+1):\n",
    "    if i not in occurence_dict:\n",
    "        occurence_dict[i] = 0\n",
    "\n",
    "# sort the dictionary by the key (number of unifications)\n",
    "occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "x = list(occurence_dict.keys())\n",
    "y = list(occurence_dict.values())\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(-0.5, max(x))\n",
    "ax.set(xlabel=\"Number of non-null values\")\n",
    "ax.set_title(\"Histogram of Number of Non-Null Values per Key\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_non_null_values_vs_occurence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_values_per_key = {}\n",
    "\n",
    "for i in range(len(runs_prediction_stats_df)):\n",
    "    for key, value in runs_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        if key in non_null_values_per_key:\n",
    "            non_null_values_per_key[key].append(value)\n",
    "        else:\n",
    "            non_null_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# one key = one bar plot (histogram)\n",
    "max_val = 0\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    occurence_dict = {}\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "    # intersperse keys that are missing with value 0\n",
    "    for i in range(0, max(occurence_dict.keys())+1):\n",
    "        if i not in occurence_dict:\n",
    "            occurence_dict[i] = 0\n",
    "\n",
    "    # sort the dictionary by the key (number of unifications)\n",
    "    occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "    x = list(occurence_dict.keys())\n",
    "    y = list(occurence_dict.values())\n",
    "    \n",
    "    ax = sns.scatterplot(x=x, y=y, ax=ax, label=key)\n",
    "    max_val = max(max_val, max(x))\n",
    "\n",
    "ax.set(xlabel=\"Number of non-null values\", ylabel=\"Count\")\n",
    "\n",
    "# set the x-ticks to be integers\n",
    "ax.set_xticks(np.arange(0, max_val+1, 1))\n",
    "ax.legend()\n",
    "ax.set_title(\"Number of Non-Null Values by Key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_non_null_values_by_key_vs_count.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(runs_prediction_stats_df['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "y = [] # number of non-null values per key (average over all keys)\n",
    "\n",
    "y = [[] for _ in range(max(x)+1)]\n",
    "\n",
    "for i in range(len(runs_prediction_stats_df)):\n",
    "    num_subdocs = runs_prediction_stats_df.iloc[i]['num_subdocs']\n",
    "\n",
    "\n",
    "    num_non_null_values = 0\n",
    "    num_keys = 0\n",
    "    for key, value in runs_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        num_non_null_values += value\n",
    "        num_keys += 1\n",
    "    if num_keys > 0:\n",
    "        y[num_subdocs].append(num_non_null_values / num_keys)\n",
    "    else:\n",
    "        y[num_subdocs].append(0)\n",
    "\n",
    "y = [np.mean(y[i]) for i in range(len(y))] # average number of non-null values per key for each number of subdocuments\n",
    "\n",
    "ax.plot([0, max(x) + 1], [0, max(x) + 1], color='black', linestyle='--', label='Upper bound')\n",
    "\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line')\n",
    "\n",
    "ax.set(xticks=x, yticks=x)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(right=max_subdocs+0.5)\n",
    "ax.set_ylim(top=max_subdocs+0.5)\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of non-null values per key\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title(\"Number of Subdocuments vs. Average Number of Non-Null Values per Key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_num_subdocs_vs_non_null_values_per_key.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(values)/len(values))\n",
    "\n",
    "y = [round(y[i], 3) for i in range(len(y))]\n",
    "\n",
    "key_stats = pd.DataFrame({'key': x, 'avg_num_of_non_null_values': y})\n",
    "\n",
    "print(key_stats) # avg_num_of_non_null_values is the average number of non-null values per key\n",
    "print(single_run_lenient_accuracy_by_key) # lenient_accuracy_by_key is the accuracy for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (every unification is trivial)\")\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=runs_prediction_stats_df, ax=ax, label=\"Collision percentage\")\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=runs_prediction_stats_df, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "avg_collision_percentage = np.nanmean(runs_prediction_stats_df['collision_percentage'])\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(runs_prediction_stats_df['full_collision_percentage'])\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set_xlim(left=1, right=max(runs_prediction_stats_df['num_subdocs']))\n",
    "ax.set(xticks=np.arange(1, max_subdocs+1, 1))\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_collisions_wrt_subdocs_hist.png\", dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 subdocument == no subdocs (or in other words: the 1 subdoc is the whole document)\n",
    "\n",
    "Note: best case assumes:\n",
    "* perfect OCR\n",
    "* no mistakes in the reports (no typos, no conflicting information on different pages)\n",
    "\n",
    "of course with num_subdocs=2 every collision is also a full_collision :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "\n",
    "collision_percentage_by_key = {}\n",
    "full_collision_percentage_by_key = {}\n",
    "\n",
    "\n",
    "for key in runs_prediction_stats_df['num_unified_values_per_key'].iloc[0].keys():\n",
    "    for i in range(len(runs_prediction_stats_df)):\n",
    "        if key not in collision_percentage_by_key:\n",
    "            collision_percentage_by_key[key] = []\n",
    "\n",
    "        if key not in full_collision_percentage_by_key:\n",
    "            full_collision_percentage_by_key[key] = []\n",
    "\n",
    "        collision_percentage_by_key[key].append(runs_prediction_stats_df['collision_per_key'].iloc[i][key])\n",
    "        full_collision_percentage_by_key[key].append(runs_prediction_stats_df['full_collision_per_key'].iloc[i][key])\n",
    "\n",
    "# filter out None values\n",
    "collision_percentage_by_key = {key: [x for x in collision_percentage_by_key[key] if x is not None] for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: [x for x in full_collision_percentage_by_key[key] if x is not None] for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# calculate average and ignore nan values and round to 3 decimal places\n",
    "collision_percentage_by_key = {key: round(np.nanmean(collision_percentage_by_key[key]), 3) for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: round(np.nanmean(full_collision_percentage_by_key[key]), 3) for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# add those values to the dataframe\n",
    "key_stats['collision_percentage'] = key_stats['key'].map(collision_percentage_by_key)\n",
    "key_stats['full_collision_percentage'] = key_stats['key'].map(full_collision_percentage_by_key)\n",
    "\n",
    "# create a barplot that has grouped bars\n",
    "\n",
    "# we want to use the \"hue\" parameter to group the bars by collision vs. full collision\n",
    "# thus we have to transform the dataframe to have a column for each of the two types of collisions\n",
    "# and a column for the key\n",
    "key_stats_trf = key_stats.melt(id_vars=['key'], value_vars=['collision_percentage', 'full_collision_percentage'], var_name='collision_type', value_name='collision_pct')\n",
    "\n",
    "# rename the collision types to something more readable\n",
    "key_stats_trf['collision_type'] = key_stats_trf['collision_type'].map({'collision_percentage': 'Collision percentage', 'full_collision_percentage': 'Full Collision percentage'})\n",
    "\n",
    "# create the barplot\n",
    "ax = sns.barplot(x=\"key\", y=\"collision_pct\", hue=\"collision_type\", data=key_stats_trf, ax=ax)\n",
    "\n",
    "avg_collision_percentage = np.nanmean(list(collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(list(full_collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set(xlabel=\"Key\", ylabel=\"(Full) collision percentage\", title=\"Collision Percentage by Key\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_collision_percentage_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats['full_collision_over_collision'] = round(key_stats['full_collision_percentage'] / key_stats['collision_percentage'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(single_run_lenient_accuracy_by_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation coefficient between avg_num_of_non_null_values and collision percentage: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], key_stats['collision_percentage'])[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], single_run_lenient_accuracy_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (own) accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], avg_own_evaluation_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (official) accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"Correlation coefficient between collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], single_run_lenient_accuracy_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between collision percentage and (own) accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], avg_own_evaluation_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between collision percentage and (official) accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"Correlation coefficient between full collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], single_run_lenient_accuracy_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between full collision percentage and (own) accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], avg_own_evaluation_by_key)[0, 1], 3)))\n",
    "print(\"Correlation coefficient between full collision percentage and (official) accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "sns.heatmap(key_stats.iloc[:, 2:], annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax)\n",
    "ax.set(xlabel=\"Metric\", ylabel=\"Key\", title=\"Collision statistics by key\")\n",
    "\n",
    "ax.set_xticklabels(['Collision Percentage', 'Full Collision Percentage', '% Full Collision of Collision'])\n",
    "\n",
    "# set the yticks to the key names\n",
    "ax.set_yticklabels(key_stats['key'], rotation=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_collision_stats_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the own_evaluation.mean(axis=1, skipna=True) to the prediction_stats dataframe\n",
    "runs_prediction_stats_df['own_evaluation_accuracy'] = own_evaluation.mean(axis=1, skipna=True)\n",
    "runs_prediction_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(runs_prediction_stats_df['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "y = [] # avg. accuracy for each number of subdocs\n",
    "# yerr = [] # standard deviation for each number of subdocs\n",
    "\n",
    "# scatter plot of the avg. own_evaluation accuracy vs num_subdocs\n",
    "for i in range(0, max_subdocs+1):\n",
    "    y.append(runs_prediction_stats_df[runs_prediction_stats_df['num_subdocs'] == i]['own_evaluation_accuracy'].mean())\n",
    "    #yerr.append(prediction_stats[prediction_stats['num_subdocs'] == i]['own_evaluation_accuracy'].std())\n",
    "\n",
    "# scatter plot with error bars\n",
    "#ax.errorbar(x, y, yerr=yerr, fmt='o', ecolor='red', capsize=5, elinewidth=2, capthick=2)\n",
    "\n",
    "# scatter plot without error bars\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line')\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "# set the labels\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Accuracy\", title=\"Accuracy by number of subdocuments\")\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(\"Number of Subdocuments vs. Accuracy (own evaluation)\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_own_eval_accuracy_vs_num_subdocs.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
