{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET DESIRED EVALUATION PARAMS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 'dev' # 'dev' or 'test'\n",
    "TEMPERATURE = '1' # '0', '0.1' or '1'\n",
    "MODEL = 'neox' # 'flan-t5', 'neox' or 'davinci'\n",
    "ONE_SHOT = False # True or False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Levenshtein import distance\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT == 'dev':\n",
    "    if MODEL == 'flan-t5':\n",
    "        if ONE_SHOT:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T23-40-17_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T23-40-19_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T23-40-22_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/flan-t5/2023-01-19T23-40-17_flan-t5_temp_0.log',\n",
    "                    '../logs/flan-t5/2023-01-19T23-40-19_flan-t5_temp_0.log',\n",
    "                    '../logs/flan-t5/2023-01-19T23-40-22_flan-t5_temp_0.log',\n",
    "                ]\n",
    "        else:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T15-01-08_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T15-01-09_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T15-01-10_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/flan-t5/2023-01-19T15-01-08_flan-t5_temp_0.log',\n",
    "                    '../logs/flan-t5/2023-01-19T15-01-09_flan-t5_temp_0.log',\n",
    "                    '../logs/flan-t5/2023-01-19T15-01-10_flan-t5_temp_0.log',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T15-02-03_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T22-18-34_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T22-22-59_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/flan-t5/2023-01-19T15-02-03_flan-t5_temp_0.1.log',\n",
    "                    '../logs/flan-t5/2023-01-19T22-18-34_flan-t5_temp_0.1.log',\n",
    "                    '../logs/flan-t5/2023-01-19T22-22-59_flan-t5_temp_0.1.log',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T17-41-36_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T17-51-37_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/flan-t5/2023-01-19T22-13-13_LLMPipeline(prompt_variant=NeutralPrompt, model=Flan_T5(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40), parser=KleisterCharityParser, shots=False).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/flan-t5/2023-01-19T17-41-36_flan-t5_temp_1.log',\n",
    "                    '../logs/flan-t5/2023-01-19T17-51-37_flan-t5_temp_1.log',\n",
    "                    '../logs/flan-t5/2023-01-19T22-13-13_flan-t5_temp_1.log',\n",
    "                ]\n",
    "                \n",
    "    elif MODEL == 'neox':\n",
    "        if ONE_SHOT:\n",
    "            if TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2023-01-15T23-35-37_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2023-01-15T23-44-53_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2023-01-15T23-45-00_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/neox/2023-01-15T23-35-37_neox_oneshot_temp_1.log',\n",
    "                    '../logs/neox/2023-01-15T23-44-53_neox_oneshot_temp_1.log',\n",
    "                    '../logs/neox/2023-01-15T23-45-00_neox_oneshot_temp_1.log',\n",
    "                ]\n",
    "\n",
    "        else:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-14T18-27-21_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-44-54_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-15_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/neox/2022-12-14T18-27-21_neox_temp_0.log',\n",
    "                    '../logs/neox/2022-12-15T21-44-54_neox_temp_0.log',\n",
    "                    '../logs/neox/2022-12-15T21-45-15_neox_temp_0.log',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-14T18-27-46_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-42_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-15T21-45-54_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=0.1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/neox/2022-12-14T18-27-46_neox_temp_0.1.log',\n",
    "                    '../logs/neox/2022-12-15T21-45-42_neox_temp_0.1.log',\n",
    "                    '../logs/neox/2022-12-15T21-45-54_neox_temp_0.1.log',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-11T00-56-12_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/neox/2022-12-12T02-26-28_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/neox/2022-12-11T00-25-32_neox_temp_1.log',\n",
    "                    '../logs/neox/2022-12-11T00-56-12_neox_temp_1.log',\n",
    "                    '../logs/neox/2022-12-12T02-26-28_neox_temp_1.log',\n",
    "                ]\n",
    "\n",
    "    elif MODEL == 'davinci':\n",
    "        if ONE_SHOT:\n",
    "            if TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2023-01-16T00-05-06_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2023-01-16T00-05-10_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2023-01-16T00-05-13_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser, shots=True).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/davinci/2023-01-16T00-05-06_davinci_oneshot_temp_0.1.log',\n",
    "                    '../logs/davinci/2023-01-16T00-05-10_davinci_oneshot_temp_0.1.log',\n",
    "                    '../logs/davinci/2023-01-16T00-05-13_davinci_oneshot_temp_0.1.log',\n",
    "                ]\n",
    "        else:\n",
    "            if TEMPERATURE == '0':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-14T18-30-56_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-36-13_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-36-16_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/davinci/2022-12-14T18-30-56_davinci_temp_0.log',\n",
    "                    '../logs/davinci/2022-12-15T16-36-13_davinci_temp_0.log',\n",
    "                    '../logs/davinci/2022-12-15T16-36-16_davinci_temp_0.log',\n",
    "                ]\n",
    "\n",
    "            elif TEMPERATURE == '0.1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-14T23-49-26_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-34-49_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-35-20_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0.1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/davinci/2022-12-14T23-49-26_davinci_temp_0.1.log',\n",
    "                    '../logs/davinci/2022-12-15T16-34-49_davinci_temp_0.1.log',\n",
    "                    '../logs/davinci/2022-12-15T16-35-20_davinci_temp_0.1.log',\n",
    "                ]\n",
    "            \n",
    "            elif TEMPERATURE == '1':\n",
    "                PREDICTION_RUNS_PATHS = [\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-11-24T01-57-18_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-09T20-16-34_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                    'datasets/kleister_charity/dev-0/predictions/davinci/2022-12-12T02-27-07_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=1, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv',\n",
    "                ]\n",
    "                LOG_PATHS = [\n",
    "                    '../logs/davinci/2022-11-24T01-57-18_davinci_temp_1.log',\n",
    "                    '../logs/davinci/2022-12-09T20-16-34_davinci_temp_1.log',\n",
    "                    '../logs/davinci/2022-12-12T02-27-07_davinci_temp_1.log',\n",
    "                ]\n",
    "\n",
    "elif SPLIT == 'test':\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading solution (expected.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT == 'dev':\n",
    "    expected = pd.read_csv('datasets/kleister_charity/dev-0/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "elif SPLIT == 'test':\n",
    "    expected = pd.read_csv('datasets/kleister_charity/test-A/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Address (street)\", \"Annual Income\",  \"Annual Spending\"]\n",
    "expected = expected[COLUMN_ORDER]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of keys that actually have a value (are not NaN):\")\n",
    "expected.count() / len(expected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading predictions (three runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_dfs = []\n",
    "for prediction_run_path in PREDICTION_RUNS_PATHS:\n",
    "    prediction_run_df = pd.read_csv(prediction_run_path, sep='\\t', header=None, names=['raw'], skip_blank_lines=False)\n",
    "\n",
    "    for raw_prediction in prediction_run_df[\"raw\"]:\n",
    "        if raw_prediction is np.nan:\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction] = np.nan\n",
    "            continue\n",
    "        key_value_pairs = raw_prediction.split(\" \")\n",
    "        for key_value in key_value_pairs:\n",
    "            key, value = key_value.split(\"=\", 1)\n",
    "            prediction_run_df.loc[prediction_run_df[\"raw\"] == raw_prediction, key] = value\n",
    "\n",
    "    # how many columns are there?\n",
    "    num_columns = len(prediction_run_df.columns)\n",
    "\n",
    "    column_order = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Annual Income\", \"Annual Spending\"]\n",
    "\n",
    "    # rename columns\n",
    "    prediction_run_df.columns = column_order[:num_columns]\n",
    "\n",
    "    # add the missing columns and fill them with NaN\n",
    "    for column in column_order[num_columns:]:\n",
    "        prediction_run_df[column] = np.nan\n",
    "    \n",
    "    prediction_run_df = prediction_run_df[COLUMN_ORDER]\n",
    "    prediction_run_df = prediction_run_df.drop(columns=[\"raw\"])\n",
    "    prediction_runs_dfs.append(prediction_run_df)\n",
    "\n",
    "assert len(prediction_runs_dfs) == 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading associated logs of predictions (three runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_runs_logs = []\n",
    "for log_path in LOG_PATHS:\n",
    "    log_file = open(log_path, \"r\")\n",
    "    log_lines = [line.strip() for line in log_file.readlines() if line.strip() and line.startswith(\"20\") and \"Raw value:\" not in line]\n",
    "    log_file.close()\n",
    "    prediction_runs_logs.append(log_lines)\n",
    "\n",
    "assert len(prediction_runs_logs) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_runs_logs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations (answering RH1)\n",
    "## RH1\n",
    "> Unimodal approach can reach satisfactory performance while being more cost-efficient than current state-of-the-art multi-modal approaches\n",
    "\n",
    "Where we define satisfactory performance as:\n",
    "> 80% of the values for given and findable keys are correctly found (no distinction for the other 20%, they can be either wrong or missing (which is of course also wrong)). Correctness is defined as a case-insensitive (upper-casing everything) string match with some normalisation (details below)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise values for three keys as listed below. We don't normalise values of `Address (post code)`, `Charity Number`, `Period End Date`, `Annual Income`, `Annual Spending`.\n",
    "\n",
    "\n",
    "### Address (post town)\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "OTTERY_ST_MARY | Ottery_St._Mary\n",
    "Lichfield | City_of_Lichfield\n",
    "Liverpool | City_of_Liverpool\n",
    "\n",
    "Normalisation:\n",
    "* `<Solution City>` vs. `City of <Solution City>` are both correct\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition, but not a substitution) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary)\n",
    "\n",
    "### Address (street)\n",
    "Examples: \n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "36_BELLINGHAM_DRIVE | Unit_36_Bellingham_Drive\n",
    "34_DECIMA_STREET | Sherborne_House,_34_Decima_Street\n",
    "190_LONG_LANE | Scout_Centre,_Rear_190_Long_Lane\n",
    "13_ROSSLYN_ROAD | Room_16,_ETNA_Community_Centre,_13_Rosslyn_Road\n",
    "FURNIVAL_GATE | 2_Floor,_Midcity_House,_Furnival_Gate\n",
    "7-14_Great_Dover_Street | 7_-_14_Great_Dover_Street\n",
    "BROWNBERRIE_LANE | Leeds_Trinity_University,_Brownberrie_Lane\n",
    "\n",
    "Normalisation: \n",
    "* Delete Spaces around \"-\" in both solution and prediction\n",
    "\n",
    "Was considering generally cutting off at ,_ before or after the street but ultimately decided against it because it cannot be generally stated that having something in front or after the correct street would still make mail arrive at the destination.\n",
    "\n",
    "Also: Levenshtein edit distance of 1 doesn't make sense here as getting the number wrong (e.g. 13 instead of 1) is a clear mistake.\n",
    "\n",
    "### Charity Name\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "Cheltenham Ladies College | Cheltenham Ladies' College\n",
    "Battersea_Dogs'_and_Cats'_Home | Battersea_Dogs'_&_Cats'_Home\n",
    "Beer_Shmuel_Ltd. | Beer_Shmuel_Limited\n",
    "Catch_22_Charity_Ltd. | Catch22\n",
    "Richard_Hicks | Richard_Hicks_Charity\n",
    "King's_Schools_Taunton_Ltd. | King's_Schools_Taunton_Limited\n",
    "KEY_ENTERPRISES_(1983)_LTD. | KEY_ENTERPRISES_(1983)_LIMITED\n",
    "Louth_Playgoers_Society_Ltd. | Louth_Playgoers_Society_Limited\n",
    "Boxgrove_Village_Hall_and_Community_Centre | BOXGROVE_VILLAGE_HALL_&_COMMUNITY_CENTRE_CIO\n",
    "London_Transport_Museum | London_Transport_Museum_Ltd.\n",
    "The_Momc-Leigh_Park_Crafts_Initiative_Trust_Ltd. | THE_MOMC_-_LEIGH_PARK_CRAFTS_INITIATIVE_TRUST_LIMITED\n",
    "King_Edward_Vi's_School_At_Chelmsford | King_Edward_VI_School_at_Chelmsford\n",
    "The_Hope_Foundation_Ltd. | The_Hope_Foundation\n",
    "Nottingham_Women's_Counselling_Service | The_Nottingham_Women's_Counselling_Service\n",
    "\n",
    "Normalisation (+ give stats for how many values this applies):\n",
    "* Cut off Ltd, Ltd. and Limited from the end of both prediction and solution \n",
    "* Replace \"&\" with \"and\" in both prediction and solution\n",
    "* Delete Spaces around \"-\" in both prediction and solution\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary, King_Edward_VI's_School vs. King_Edward_VI_School)\n",
    "\n",
    "\n",
    "### Other Normalisations\n",
    "Replaced uncommon character: ’ (U+2019) with ' (in the predictions and the solution) (applies to 4 values in the whole solution of the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    count = replace_quotation_mark(prediction_run_df)\n",
    "    print(f\"Replaced {count} quotation marks in prediction run {i}.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating according to own definition of \"correctness\" and official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(3)]\n",
    "own_evaluations = [pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) for _ in range(3)]\n",
    "null_evaluations = [pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) for _ in range(3)]\n",
    "\n",
    "for i, prediction_run_df in enumerate(prediction_runs_dfs):\n",
    "    for index, row in expected.iterrows():\n",
    "        for column in expected.columns:\n",
    "            if pd.notnull(row[column]): # because during parsing we look at the generations and if all subdocs are \"null\" or empty strings, it will not appear in the output and hence be NaN\n",
    "                # FP: we predicted null and it was not null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"FP\", column] += 1\n",
    "\n",
    "                # TN: we predicted not null (i.e. we predicted something) and it was not null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"TN\", column] += 1\n",
    "                if is_correct(column, row[column], prediction_run_df.loc[index, column]):\n",
    "                    own_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    own_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "                if str(row[column]).upper() == str(prediction_run_df.loc[index, column]).upper():\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0\n",
    "\n",
    "            else: # we don't care about the prediction in our own evaluation if the expected value is null\n",
    "                # TP: we predicted null and it was null\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    null_evaluations[i].loc[\"TP\", column] += 1\n",
    "                # FN: we predicted not null and it was null\n",
    "                else:\n",
    "                    null_evaluations[i].loc[\"FN\", column] += 1\n",
    "\n",
    "                if pd.isnull(prediction_run_df.loc[index, column]):\n",
    "                    official_evaluations[i].loc[index, column] = 1\n",
    "                else:\n",
    "                    official_evaluations[i].loc[index, column] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general we take macro average over the runs as it gives equal importance to each run (on the other hand, micro would give more importance to runs that have more predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own evaluation: only looks at the keys that are actually present in the document\n",
    "\n",
    "# combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the own evaluations by key of each run\n",
    "avg_own_evaluation_by_key = pd.concat([own_evaluation.mean(axis=0, skipna=True) for own_evaluation in own_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (own) evaluation by key:\\n{avg_own_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (own) average of correctly predicted values: {round(avg_own_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (own) range of correctly predicted values: {round(avg_own_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")\n",
    "\n",
    "# official evaluation (same as above but with the official evaluation)\n",
    "avg_official_evaluation_by_key = pd.concat([official_evaluation.mean(axis=0, skipna=True) for official_evaluation in official_evaluations], axis=1).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=1)\n",
    "print(f\"(macro)[over runs] (official) evaluation by key:\\n{avg_official_evaluation_by_key}\")\n",
    "print(f\"(macro)[over runs and keys] (official) average of correctly predicted values: {round(avg_official_evaluation_by_key['mean'].agg('mean'), 3)}\")\n",
    "print(f\"(macro)[over runs and keys] (official) range of correctly predicted values: {round(avg_official_evaluation_by_key['<lambda>'].agg('mean'), 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that the official evaluation is actually higher than our evaluation because it rewards correctly identifying null values whereas we don't care about the key-value pairs that are expected to be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"(macro)[over runs] Correlation coefficient between own and official evaluation (by key): {round(np.corrcoef(avg_own_evaluation_by_key['mean'], avg_official_evaluation_by_key['mean'])[0, 1], 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we want to answer here is how good we are with predicting `null` for a key (not in subdocs, but for whole document), specifically we also look at the F1 for only `null` → `F_1_{null}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_scores_by_key_runs = []\n",
    "for null_evaluation in null_evaluations:\n",
    "    # micro average\n",
    "    precision = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FP\", :].sum())\n",
    "    recall = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FN\", :].sum())\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # aggregating by key for later macro average\n",
    "    null_scores_by_key = {}\n",
    "    for key in null_evaluation.columns:   \n",
    "        precision = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FP\", key]) \n",
    "        recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        null_scores_by_key[key] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    null_scores_by_key = pd.DataFrame(null_scores_by_key).T\n",
    "    null_scores_by_key_runs.append(null_scores_by_key)\n",
    "\n",
    "print(f\"(macro)[over runs] Precision for null by key:\\n{pd.concat([null_scores_by_key['precision'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "print(f\"(macro)[over runs] Recall for null by key:\\n{pd.concat([null_scores_by_key['recall'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "print(f\"(macro)[over runs] F1 score for null by key:\\n{pd.concat([null_scores_by_key['f1'] for null_scores_by_key in null_scores_by_key_runs], axis=1).agg(['mean', 'min', 'max'], axis=1)}\")\n",
    "\n",
    "\n",
    "print(f\"(macro)[over runs with micro average] Precision for null: {np.mean([null_evaluation.loc['TP', :].sum() / (null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FP', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "print(f\"(macro)[over runs with micro average] Recall for null: {np.mean([null_evaluation.loc['TP', :].sum() / (null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FN', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "print(f\"(macro)[over runs with micro average] F1 score for null: {np.mean([2 * null_evaluation.loc['TP', :].sum() / (2 * null_evaluation.loc['TP', :].sum() + null_evaluation.loc['FP', :].sum() + null_evaluation.loc['FN', :].sum()) for null_evaluation in null_evaluations])}\")\n",
    "\n",
    "print(f\"(macro)[over runs with macro over keys] F1 score for null: {np.mean([null_scores_by_key['f1'].mean() for null_scores_by_key in null_scores_by_key_runs])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that micro and macro average of the  are quite close together because as we saw in the beginning, almost all keys are given in the data set. There is no big \"class\" (key) imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifications, collisions, lenient accuracy and looking at repetetiveness (all on subdocument level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Unification\n",
    "A unification is whenever there are two or more non-null values coming from the subdoc predictions for the same key. If there is only one non-null value coming from the subdocs (which is always the case if there only is one subdoc but can also happen with more than one subdoc) then it's not a unification.\n",
    "\n",
    "A trivial unification is a unification where all values are the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Collision\n",
    "Given a unification, we describe two or more different values for the same key as the unification having a collision. \n",
    "\n",
    "So a unification is either trivial (all values the same) or it has a collision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a Full Collision\n",
    "Same as a collision but with the constraint that *all* values are different (not just any two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_key_dict = {\n",
    "    \"Address (post town)\": None,\n",
    "    \"Address (post code)\": None,\n",
    "    \"Address (street)\": None,\n",
    "    \"Charity Name\": None,\n",
    "    \"Charity Number\": None,\n",
    "    \"Annual Income\": None,\n",
    "    \"Period End Date\": None,\n",
    "    \"Annual Spending\": None,\n",
    "}\n",
    "\n",
    "runs_predictions_stats_dict = []\n",
    "runs_predictions_dict = []\n",
    "\n",
    "for i, log_lines in enumerate(prediction_runs_logs):\n",
    "    print(f\"Processing run {i}\")\n",
    "    single_run_prediction_stats_dict = []\n",
    "    single_run_predictions_dict = []\n",
    "\n",
    "    single_run_num_empty_string_predictions = 0\n",
    "    for line in log_lines:\n",
    "        if \"Predicting document\" in line: # this is the beginning of a prediction\n",
    "            # create a new dictionary for this document\n",
    "            single_run_prediction_stats_dict.append({\n",
    "                \"num_subdocs\": None,\n",
    "                \"num_unifications\": 0,\n",
    "                \"collision_per_key\": empty_key_dict.copy(),\n",
    "                \"full_collision_per_key\": empty_key_dict.copy(),\n",
    "                \"num_unified_values_per_key\": empty_key_dict.copy(),\n",
    "                \"correct_in_any_subdoc_per_key\": empty_key_dict.copy(),\n",
    "                \"collision_percentage\": None,\n",
    "                \"full_collision_percentage\": None,\n",
    "                \"correct_in_any_subdoc_percentage\": None,\n",
    "            })\n",
    "            single_run_predictions_dict.append(empty_key_dict.copy())\n",
    "\n",
    "        elif \"Final prediction for document\" in line: # this is the end of a prediction\n",
    "            # calculate the percentages\n",
    "            single_run_prediction_stats_dict[-1][\"collision_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"collision_per_key\"].values() if x is not None])\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"full_collision_per_key\"].values() if x is not None])\n",
    "            single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_percentage\"] = np.mean([x for x in single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"].values() if x is not None])\n",
    "    \n",
    "        elif \"No subdocs necessary\" in line:\n",
    "            single_run_prediction_stats_dict[-1][\"num_subdocs\"] = 1\n",
    "\n",
    "        elif \"Split document into\" in line:\n",
    "            num_subdocs = int(re.search(\"into (\\d+) subdocuments\", line).group(1))\n",
    "            single_run_prediction_stats_dict[-1][\"num_subdocs\"] = num_subdocs\n",
    "\n",
    "        elif \"- parse_model_output() - Key:\" in line:\n",
    "            key = re.search(\"- parse_model_output\\(\\) - Key: (.*):\", line).group(1)\n",
    "            # the prediction is always in the next line (unless the key was not predicted at all) or the prediction is an empty string\n",
    "            try:\n",
    "                prediction = re.search(\"- parse_model_output\\(\\) - Stripped value: (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "                \n",
    "\n",
    "            except AttributeError:\n",
    "                single_run_num_empty_string_predictions += 1\n",
    "                prediction = \"[METADATA]: EMPTY_STRING_PREDICTION\"\n",
    "                \n",
    "            if single_run_predictions_dict[-1][key] is None:\n",
    "                    single_run_predictions_dict[-1][key] = [prediction]\n",
    "            else:\n",
    "                single_run_predictions_dict[-1][key].append(prediction)\n",
    "\n",
    "        elif \"Unification necessary for key\" in line:\n",
    "            key = re.search(\"Unification necessary for key (.*)\", line).group(1)\n",
    "            single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "            values = re.search(\"Unifying \\d+ \\(lowered\\) values (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "\n",
    "            # values is the string representation of a list, so we can use eval to turn it into a list\n",
    "            values = eval(values)\n",
    "            unified_values = int(re.search(\"Unifying (\\d+) \\(lowered\\) values\", log_lines[log_lines.index(line)+1]).group(1))\n",
    "\n",
    "            assert unified_values == len(values) # sanity check\n",
    "\n",
    "            # if there is more than 1 value, then it's a unification (because these values don't include null values)\n",
    "            if len(values) > 1:\n",
    "                single_run_prediction_stats_dict[-1][\"num_unifications\"] += 1\n",
    "\n",
    "                # if there are more than 1 different values, then it's a collision\n",
    "                if len(set(values)) > 1:\n",
    "                    single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = True\n",
    "\n",
    "                # if the length of the set is equal to the length of the list, then it's a full collision\n",
    "                if len(set(values)) == len(values):\n",
    "                    single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = True\n",
    "\n",
    "            \n",
    "            single_run_prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = unified_values\n",
    "\n",
    "            # which document are we in?\n",
    "            doc_num = len(single_run_prediction_stats_dict) - 1\n",
    "\n",
    "            # get the correct value for this key\n",
    "            correct_value = expected.iloc[doc_num][key]\n",
    "\n",
    "            # if it's NaN, then we were not supposed to predict anything for this key but we did (in at least one subdoc)\n",
    "            if pd.isna(correct_value):\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False # TODO: ask if this is correct\n",
    "                continue\n",
    "\n",
    "            # we have to transform the values in the list to the same format as the correct value\n",
    "            values = [x.replace(\" \", \"_\").replace(\":\", \"_\").upper() for x in values]\n",
    "\n",
    "            # also transform the correct value to uppercase\n",
    "            correct_value = str(correct_value).upper()\n",
    "\n",
    "            # if the correct value is in the list of values, then it's correct in at least one subdoc\n",
    "            if correct_value in values:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "\n",
    "            else:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "        elif \"Key not found in any subdoc\" in line: # null was predicted in all subdocs\n",
    "            key = re.search(\"Key not found in any subdoc (.*)\", line).group(1)\n",
    "            single_run_prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "            single_run_prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = 0\n",
    "\n",
    "            # get the correct value for this key\n",
    "            correct_value = expected.iloc[len(single_run_prediction_stats_dict) - 1][key]\n",
    "\n",
    "            if pd.isna(correct_value):\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "            else:\n",
    "                single_run_prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "    print(\"Number of empty string predictions:\", single_run_num_empty_string_predictions)\n",
    "    runs_predictions_stats_dict.append(single_run_prediction_stats_dict)\n",
    "    runs_predictions_dict.append(single_run_predictions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -> all docs with x number of subdocs should have x predictions for each key (also for the keys that have an empty string prediction\n",
    "# because we added [METADATA]: EMPTY_STRING_PREDICTION to the list of predictions\n",
    "for _, single_run_predictions_stats_dict in enumerate(runs_predictions_stats_dict):\n",
    "    for j, document in enumerate(single_run_predictions_stats_dict):\n",
    "        for key in single_run_predictions_dict[j].keys():\n",
    "            assert len(single_run_predictions_dict[j][key]) == single_run_prediction_stats_dict[j][\"num_subdocs\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_prediction_stats_df =[pd.DataFrame(runs_predictions_stats_dict[i]) for i in range(len(runs_predictions_stats_dict))]\n",
    "runs_prediction_stats_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in column `correct_in_any_subdoc_percentage` with the accuracy as per official_evaluation (this is not recorded in the log traversal)\n",
    "for i in range(len(runs_prediction_stats_df)):\n",
    "    runs_prediction_stats_df[i][\"correct_in_any_subdoc_percentage\"] = runs_prediction_stats_df[i][\"correct_in_any_subdoc_percentage\"].fillna(official_evaluations[i].mean(axis=1, skipna=True))\n",
    "\n",
    "# # replace None values in column `correct_in_any_subdoc_per_key` of the rows with 1 subdoc with True/False as taken from the official_evaluation\n",
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    for j in range(len(single_run_prediction_stats_df)):\n",
    "        if single_run_prediction_stats_df.iloc[j][\"num_subdocs\"] == 1:\n",
    "            for key in single_run_prediction_stats_df.iloc[j][\"correct_in_any_subdoc_per_key\"].keys():\n",
    "                single_run_prediction_stats_df.iloc[j][\"correct_in_any_subdoc_per_key\"][key] = bool(official_evaluations[i].iloc[j][key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenient Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the case that not in any subdoc a value was predicted (all null) we check if the correct solution is in fact null and then consider that in any subdoc the corect value (which is null) was found (in reality it was correctly identified in all). We cannot do the opposite (check if it was null in any subdoc) and then say it was correctly identified (if it was indeed null) because of the subdoc structure where any given subdoc is not guaranteed to have all key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_lenient_accuracy_by_key = []\n",
    "\n",
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "\n",
    "    # a single entry is a dictionary with the correctness for each key (True, False, or None)\n",
    "    lenient_correctness_over_docs = single_run_prediction_stats_df[\"correct_in_any_subdoc_per_key\"].tolist()\n",
    "\n",
    "    single_run_lenient_accuracy_by_key = pd.DataFrame()\n",
    "\n",
    "    for key in lenient_correctness_over_docs[0].keys():\n",
    "        correctness = [x[key] for x in lenient_correctness_over_docs]\n",
    "        avg_lenient_accuracy = np.mean([x for x in correctness if x is not None])\n",
    "        single_run_lenient_accuracy_by_key[key] = [avg_lenient_accuracy]\n",
    "\n",
    "    runs_lenient_accuracy_by_key.append(single_run_lenient_accuracy_by_key)\n",
    "    print(\"Run\", i)\n",
    "    print(f\"(official) (macro)[average over the keys] Lenient accuracy: {round(single_run_lenient_accuracy_by_key.mean(axis=1).mean(), 3)}\")\n",
    "    print(f\"(official) (macro)[average over documents] Lenient accuracy: {round(single_run_prediction_stats_df['correct_in_any_subdoc_percentage'].mean(), 3)}\")\n",
    "\n",
    "print(\"Combined runs:\")\n",
    "# combine the three runs into one by taking the mean (together with the range around the mean (e.g. if we have [1.0, 0.3, 1.7] we get 1.0 as the mean and the range is from 0.3 to 1.7)) of the lenient accuracies by key of each run\n",
    "avg_lenient_accuracy_by_key = pd.concat([lenient_accuracy_by_key for lenient_accuracy_by_key in runs_lenient_accuracy_by_key], axis=0).agg([\"mean\", \"min\", \"max\", lambda x: x.max() - x.min()], axis=0)\n",
    "print(f\"(official) (macro)[average over the runs] Lenient accuracy by key:\\n{avg_lenient_accuracy_by_key.T}\")\n",
    "\n",
    "print(f\"(official) (macro)[average over the runs and the keys] Lenient accuracy: {round(avg_lenient_accuracy_by_key.T['mean'].mean(), 3)}\")\n",
    "print(f\"(official) (macro)[average over the runs and keys] Range of lenient accuracy by key: {round(avg_lenient_accuracy_by_key.T['<lambda>'].mean(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_relative_lenient_accuracy_improvement = []\n",
    "runs_absolute_lenient_accuracy_improvement = []\n",
    "runs_accuracies_by_key = []\n",
    "\n",
    "for i, official_evaluation in enumerate(official_evaluations):\n",
    "    print(\"Run\", i)\n",
    "    accuracy_by_key = official_evaluation.mean(axis=0, skipna=True)\n",
    "    runs_accuracies_by_key.append(accuracy_by_key)\n",
    "    print(\"(official) Relative improvement in accuracy (lenient vs. strict):\")\n",
    "    for key in accuracy_by_key.keys():\n",
    "        print(f\"{key}: {round((runs_lenient_accuracy_by_key[i][key].values[0] - accuracy_by_key[key]) / accuracy_by_key[key] * 100, 3)}%\")\n",
    "\n",
    "\n",
    "    macro_relative_improvement = (single_run_lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean()) / accuracy_by_key.mean() * 100\n",
    "    print(f\"(macro)[average over keys] (official) Relative improvement in accuracy (lenient vs. strict): {round(macro_relative_improvement, 3)}%\")\n",
    "    runs_relative_lenient_accuracy_improvement.append(macro_relative_improvement)\n",
    "\n",
    "    macro_absolute_improvement = single_run_lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean()\n",
    "    print(f\"(macro)[average over keys] (official) Absolute improvement in accuracy (lenient vs. strict): {round(macro_absolute_improvement, 3)}\")\n",
    "    runs_absolute_lenient_accuracy_improvement.append(macro_absolute_improvement)\n",
    "\n",
    "print(\"Combined runs:\")\n",
    "print(f\"(macro)[average over runs] (official) Relative improvement in accuracy (lenient vs. strict): {round(np.mean(runs_relative_lenient_accuracy_improvement), 3)}%\")\n",
    "print(f\"(macro)[average over runs] (official) Absolute improvement in accuracy (lenient vs. strict): {round(np.mean(runs_absolute_lenient_accuracy_improvement), 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and presence penalty motivation for not looking at those parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check how often the same value for different keys is predicted to see if it's generally correct to assume that values shouldn't be repeated (which could be combated by e.g. increasing the freq. and presence penalties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_values_for_different_keys(row):\n",
    "    return len(row) == len(set(row))\n",
    "\n",
    "num_of_same_value_rows = 0\n",
    "for i in range(len(expected)):\n",
    "    if not has_two_values_for_different_keys(expected.iloc[i]):\n",
    "        num_of_same_value_rows += 1\n",
    "\n",
    "print(f\"Found {num_of_same_value_rows} rows that have the same value for different keys, that's {round(num_of_same_value_rows / len(expected) * 100, 3)}% of the dataset.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, there shouldn't be the same value for different keys.\n",
    "A repetition of the same value would also be correct in the case of a document having more than 1 null value, where a correct generation would contain `\"null\"` multiple times. Let's look at how often this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents that have 2 or more NaN values for any two keys:\")\n",
    "expected[expected.isna().sum(axis=1) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_missing_value_in_solution_pct = round(len(expected[expected.isna().sum(axis=1) >= 2]) / len(expected) * 100, 3)\n",
    "print(f\"Percentage of documents that have 2 or more NaN values for any two keys: {more_than_one_missing_value_in_solution_pct}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, we shouldn't have more than 1 null in our generations, so we can now generally assume that repetition of the same value in the generations is incorrect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check how often we have repetition (i.e. the same value (including null's) for different keys) in the (subdoc) predictions. If this is very high then it would make sense to look into tuning the presence and frequency penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, single_run_prediction_stats_dict in enumerate(runs_predictions_stats_dict): # for each run\n",
    "    print(\"Run\", i)\n",
    "    for j in range(len(runs_predictions_stats_dict[i])): # for each document\n",
    "        single_run_prediction_stats_dict[j][\"pct_unique_predictions_per_subdoc\"] = {} # this is min 0.125 (1/8) in the case that all predictions are the same (e.g. all 'null') and max 1.0 (all predictions are unique)\n",
    "        single_run_prediction_stats_dict[j][\"pct_unique_predictions_per_subdoc_ignore_null\"] = {} # this is nan if all predictions are null, and also min 0.125\n",
    "        for subdoc in range(single_run_prediction_stats_dict[j][\"num_subdocs\"]): # for each subdoc            \n",
    "            # get the predictions for each key\n",
    "            predictions_for_keys = [runs_predictions_dict[i][j][key][subdoc] for key in runs_predictions_dict[i][j].keys()]\n",
    "\n",
    "            # get the number of unique predictions\n",
    "            num_unique_predictions = len(set(predictions_for_keys)) # this is at least 1, cannot be 0\n",
    "\n",
    "            # add the percentage of unique predictions to the dictionary\n",
    "            single_run_prediction_stats_dict[j][\"pct_unique_predictions_per_subdoc\"][subdoc] = num_unique_predictions / len(predictions_for_keys)\n",
    "\n",
    "            # get the number of unique predictions ignoring null values\n",
    "            num_unique_predictions_ignore_null = len(set([x for x in predictions_for_keys if not x.upper().startswith(\"NULL\") and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]))\n",
    "\n",
    "            num_predictions_ignore_null = len([x for x in predictions_for_keys if not x.upper().startswith(\"NULL\") and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]) # this is 0 if all the predictions are null or empty string\n",
    "\n",
    "            # in this case, we set the number of unique predictions to nan\n",
    "            if num_unique_predictions_ignore_null == 0:\n",
    "                single_run_prediction_stats_dict[j][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = np.nan\n",
    "                continue\n",
    "            # add the percentage of unique predictions (ignoring null) to the dictionary\n",
    "            single_run_prediction_stats_dict[j][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = num_unique_predictions_ignore_null / num_predictions_ignore_null\n",
    "\n",
    "    # (micro) average percentage of unique predictions (ignoring null) per subdoc:\"\n",
    "    # every doc has some number of subdocs, and every subdoc has some percentage of unique predictions (ignoring null)\n",
    "    # we want to average over all the subdocs in all the docs\n",
    "\n",
    "    # first, we get the list of all the percentages of unique predictions (ignoring null) per subdoc\n",
    "    all_pct_unique_predictions_per_subdoc_ignore_null = []\n",
    "    for k in range(len(runs_predictions_stats_dict[i])):\n",
    "        all_pct_unique_predictions_per_subdoc_ignore_null.extend(list(single_run_prediction_stats_dict[k][\"pct_unique_predictions_per_subdoc_ignore_null\"].values()))\n",
    "\n",
    "    # then we average over all the percentages of unique predictions (ignoring null) per subdoc\n",
    "    micro_average_pct_unique_predictions_per_subdoc_ignore_null = np.nanmean(all_pct_unique_predictions_per_subdoc_ignore_null)\n",
    "    print(f\"(micro) average percentage of unique predictions (ignoring null) per subdoc: {round(micro_average_pct_unique_predictions_per_subdoc_ignore_null, 3)}\")\n",
    "\n",
    "    # now we do the same thing, but for the percentage of unique predictions (not ignoring null)\n",
    "    all_pct_unique_predictions_per_subdoc = []\n",
    "    for k in range(len(runs_predictions_stats_dict[i])):\n",
    "        all_pct_unique_predictions_per_subdoc.extend(list(single_run_prediction_stats_dict[k][\"pct_unique_predictions_per_subdoc\"].values()))\n",
    "\n",
    "    micro_average_pct_unique_predictions_per_subdoc = np.nanmean(all_pct_unique_predictions_per_subdoc)\n",
    "    print(f\"(micro) average percentage of unique predictions per subdoc: {round(micro_average_pct_unique_predictions_per_subdoc, 3)}\")\n",
    "\n",
    "    # we do the same thing, but for the macro average (over the subdocs for each doc)\n",
    "    macro_average_pct_unique_predictions_per_subdoc_ignore_null = np.nanmean([np.nanmean(list(single_run_prediction_stats_dict[k][\"pct_unique_predictions_per_subdoc_ignore_null\"].values())) for k in range(len(runs_predictions_stats_dict[i]))])\n",
    "    print(f\"(macro)[over subdocs of each doc] average percentage of unique predictions (ignoring null) per subdoc: {round(macro_average_pct_unique_predictions_per_subdoc_ignore_null, 3)}\")\n",
    "\n",
    "    macro_average_pct_unique_predictions_per_subdoc = np.nanmean([np.nanmean(list(single_run_prediction_stats_dict[k][\"pct_unique_predictions_per_subdoc\"].values())) for k in range(len(runs_predictions_stats_dict[i]))])\n",
    "    print(f\"(macro)[over subdocs of each doc] average percentage of unique predictions per subdoc: {round(macro_average_pct_unique_predictions_per_subdoc, 3)}\")\n",
    "\n",
    "print(\"Combined runs:\")\n",
    "# finally we take the macro average over the runs\n",
    "# we do this by taking the average of the macro averages for each run\n",
    "macro_average_pct_unique_predictions_per_subdoc_ignore_null = np.nanmean([np.nanmean([np.nanmean(list(runs_predictions_stats_dict[i][k][\"pct_unique_predictions_per_subdoc_ignore_null\"].values())) for k in range(len(runs_predictions_stats_dict[i]))]) for i in range(len(runs_predictions_stats_dict))])\n",
    "print(f\"(macro)[over runs and subdocs] average percentage of unique predictions (ignoring null) per subdoc: {round(macro_average_pct_unique_predictions_per_subdoc_ignore_null, 3)}\")\n",
    "\n",
    "# and the same but not ignoring null\n",
    "macro_average_pct_unique_predictions_per_subdoc = np.nanmean([np.nanmean([np.nanmean(list(runs_predictions_stats_dict[i][k][\"pct_unique_predictions_per_subdoc\"].values())) for k in range(len(runs_predictions_stats_dict[i]))]) for i in range(len(runs_predictions_stats_dict))])\n",
    "print(f\"(macro)[over runs and subdocs] average percentage of unique predictions per subdoc: {round(macro_average_pct_unique_predictions_per_subdoc, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to be expected that there is quite some repetition when not ignoring null values because many times the subdoc will only contain few or even none of the keys. When we ignore null values, repetition is almost non-existent, hence we can reasonably assume that tuning the frequency and presence penalty parameters would not improve results by much. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding number of non-null values per key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df): # for each run\n",
    "    single_run_prediction_stats_df.loc[:, \"num_non_null_values_per_key\"] = None\n",
    "    for j in range(len(runs_predictions_dict[i])): # for each doc\n",
    "        single_run_prediction_stats_df.at[j, \"num_non_null_values_per_key\"] = {key: len([x for x in runs_predictions_dict[i][j][key] if not x.upper().startswith(\"NULL\") and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]) for key in runs_predictions_dict[i][j].keys()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding no_collision percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df): # for each run\n",
    "    # adding column \"no_collision_percentage\" which is the percentage of keys that had no collision\n",
    "    single_run_prediction_stats_df.loc[:, \"no_collision_percentage\"] = 1 - single_run_prediction_stats_df.loc[:, \"collision_percentage\"]\n",
    "\n",
    "    for j, row in single_run_prediction_stats_df.iterrows():\n",
    "        num_keys = len([x for x in row[\"collision_per_key\"].values() if x is not None])\n",
    "        num_no_collisions = len([x for x in row[\"collision_per_key\"].values() if x is False])\n",
    "        try:\n",
    "            single_run_prediction_stats_df.loc[j, \"no_collision_percentage_calculated\"] = num_no_collisions / num_keys\n",
    "        except ZeroDivisionError:\n",
    "            single_run_prediction_stats_df.loc[j, \"no_collision_percentage_calculated\"] = np.nan\n",
    "\n",
    "    assert np.allclose(single_run_prediction_stats_df[\"collision_percentage\"], 1 - single_run_prediction_stats_df[\"no_collision_percentage_calculated\"], equal_nan=True) # sanity check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_num_of_docs_with_subdocs = []\n",
    "for single_run_prediction_stats_df in runs_prediction_stats_df:\n",
    "    runs_num_of_docs_with_subdocs.append(len(single_run_prediction_stats_df[single_run_prediction_stats_df[\"num_subdocs\"] > 1]))\n",
    "print(f\"Number of docs with subdocs: {runs_num_of_docs_with_subdocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_num_of_docs_with_no_subdocs = []\n",
    "for single_run_prediction_stats_df in runs_prediction_stats_df:\n",
    "    runs_num_of_docs_with_no_subdocs.append(len(single_run_prediction_stats_df[single_run_prediction_stats_df[\"num_subdocs\"] == 1]))\n",
    "print(f\"Number of docs with no subdocs: {runs_num_of_docs_with_no_subdocs}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course the same for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = runs_prediction_stats_df[0]['num_subdocs'].value_counts().sort_index().index\n",
    "y = runs_prediction_stats_df[0]['num_subdocs'].value_counts().sort_index().values\n",
    "\n",
    "max_subdocs = max(x)\n",
    "\n",
    "# add a 0 for each number of subdocs up to the maximum number of subdocs, so that there is a value for each number of subdocs\n",
    "for i in range(1, max_subdocs+1):\n",
    "    if i not in x:\n",
    "        x = np.append(x, i)\n",
    "        y = np.append(y, 0)\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(0.5, max_subdocs)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments needed\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Histogram of Number of Needed Subdocuments\")\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(0, 51, 5))\n",
    "ax.set_xlim(0.5, 51.5)\n",
    "\n",
    "ax.set_yticks(np.arange(20, 150, 20))\n",
    "ax.set_ylim(0, 145)\n",
    "\n",
    "# plt.subplots_adjust(top=0.5)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_subdoc_hist.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_counts = []\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    single_run_non_null_values_per_key = {}\n",
    "\n",
    "    for i in range(len(single_run_prediction_stats_df)):\n",
    "        for key, value in single_run_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "            if key in single_run_non_null_values_per_key:\n",
    "                single_run_non_null_values_per_key[key].append(value)\n",
    "            else:\n",
    "                single_run_non_null_values_per_key[key] = [value]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    occurence_dict = {}\n",
    "    for key, values in single_run_non_null_values_per_key.items():\n",
    "        for value in values:\n",
    "            if value in occurence_dict:\n",
    "                occurence_dict[value] += 1\n",
    "            else:\n",
    "                occurence_dict[value] = 1\n",
    "\n",
    "    # intersperse keys that are missing with value 0\n",
    "    for i in range(0, max(occurence_dict.keys())+1):\n",
    "        if i not in occurence_dict:\n",
    "            occurence_dict[i] = 0\n",
    "\n",
    "    # sort the dictionary by the key (number of non-null values)\n",
    "    occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "    x = list(occurence_dict.keys())\n",
    "    y = list(occurence_dict.values())\n",
    "\n",
    "    run_counts.append(y)\n",
    "\n",
    "    sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "    ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "    # median calculated manually\n",
    "    z = []\n",
    "    for i in range(len(x)):\n",
    "        z.extend([x[i]] * y[i])\n",
    "\n",
    "    ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "\n",
    "    ax.set_xlim(-0.5, 24)\n",
    "    ax.set_xticks(np.arange(0, 25, 1))\n",
    "\n",
    "    ax.set_ylim(0, 3000)\n",
    "    ax.set_yticks(np.arange(0, 3000, 200))\n",
    "\n",
    "    ax.set(xlabel=\"Number of non-null values\")\n",
    "    ax.set_title(\"Histogram of Number of Non-Null Values per Key\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_RUN{run_idx}_non_null_values_vs_occurence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the run_counts into one list\n",
    "average_counts = []\n",
    "ranges = [] # for the error bars\n",
    "# the runs don't necessarily have the same length\n",
    "# so we need to find the maximum length\n",
    "max_length = max([len(counts) for counts in run_counts])\n",
    "\n",
    "for i in range(max_length):\n",
    "    counts = []\n",
    "    for run_count in run_counts:\n",
    "        if i < len(run_count):\n",
    "            counts.append(run_count[i])\n",
    "    average_counts.append(np.average(counts))\n",
    "    ranges.append((np.min(counts), np.max(counts)))\n",
    "\n",
    "ranges = np.array(ranges).T\n",
    "\n",
    "# subtract the minimum from the averages (lower error)\n",
    "ranges[0] = average_counts - ranges[0]\n",
    "\n",
    "# add the maximum to the averages (upper error)\n",
    "ranges[1] = ranges[1] - average_counts\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = list(range(len(average_counts)))\n",
    "\n",
    "sns.histplot(x=x, weights=average_counts, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "# error bars\n",
    "ax.errorbar(x, average_counts, yerr=ranges, fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "\n",
    "# mean\n",
    "ax.axvline(x=np.average(x, weights=average_counts), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * round(average_counts[i]))\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set_xlim(-0.5, 24)\n",
    "ax.set_xticks(np.arange(0, 25, 1))\n",
    "\n",
    "ax.set_ylim(0, 3000)\n",
    "ax.set_yticks(np.arange(0, 3000, 200))\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Number of non-null values\")\n",
    "ax.set_title(\"Histogram of Number of Non-Null Values per Key\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_non_null_values_vs_occurence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_non_null_values_by_key = []\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "\n",
    "    non_null_values_by_key = {}\n",
    "\n",
    "    for i in range(len(single_run_prediction_stats_df)):\n",
    "        for key, value in single_run_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "            if key in non_null_values_by_key:\n",
    "                non_null_values_by_key[key].append(value)\n",
    "            else:\n",
    "                non_null_values_by_key[key] = [value]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # one key = one scatter plot\n",
    "    max_val = 0\n",
    "    for key, values in non_null_values_by_key.items():\n",
    "        occurence_dict = {}\n",
    "        for value in values:\n",
    "            if value in occurence_dict:\n",
    "                occurence_dict[value] += 1\n",
    "            else:\n",
    "                occurence_dict[value] = 1\n",
    "\n",
    "        # intersperse keys that are missing with value 0\n",
    "        for i in range(0, max(occurence_dict.keys())+1):\n",
    "            if i not in occurence_dict:\n",
    "                occurence_dict[i] = 0\n",
    "\n",
    "        # sort the dictionary by the key (number of unifications)\n",
    "        occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "        x = list(occurence_dict.keys())\n",
    "        y = list(occurence_dict.values())\n",
    "\n",
    "        print(max(y))\n",
    "        \n",
    "        ax = sns.scatterplot(x=x, y=y, ax=ax, label=key)\n",
    "        max_val = max(max_val, max(x))\n",
    "\n",
    "    runs_non_null_values_by_key.append(non_null_values_by_key)\n",
    "\n",
    "    ax.set(xlabel=\"Number of non-null values\", ylabel=\"Count\")\n",
    "\n",
    "\n",
    "    ax.set_xlim(-0.5, 24.5)\n",
    "    # set the x-ticks to be integers\n",
    "    ax.set_xticks(np.arange(0, 25, 1))\n",
    "    \n",
    "    ax.set_ylim(0, 240)\n",
    "    ax.set_yticks(np.arange(0, 240, 20))\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Number of Non-Null Values by Key\")\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_RUN{run_idx}_non_null_values_by_key_vs_count.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_y = []\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    max_subdocs = max(single_run_prediction_stats_df['num_subdocs'])\n",
    "\n",
    "    x = np.arange(0, max_subdocs+1)\n",
    "    y = [] # number of non-null values per key (average over all keys)\n",
    "\n",
    "    y = [[] for _ in range(max(x)+1)]\n",
    "\n",
    "    for i in range(len(single_run_prediction_stats_df)):\n",
    "        num_subdocs = single_run_prediction_stats_df.iloc[i]['num_subdocs']\n",
    "\n",
    "\n",
    "        num_non_null_values = 0\n",
    "        num_keys = 0\n",
    "        for key, value in single_run_prediction_stats_df.iloc[i]['num_non_null_values_per_key'].items():\n",
    "            num_non_null_values += value\n",
    "            num_keys += 1\n",
    "        if num_keys > 0:\n",
    "            y[num_subdocs].append(num_non_null_values / num_keys)\n",
    "        else:\n",
    "            y[num_subdocs].append(0)\n",
    "\n",
    "    y = [np.mean(y[i]) for i in range(len(y))] # average number of non-null values per key for each number of subdocuments\n",
    "\n",
    "    runs_y.append(y)\n",
    "\n",
    "    ax.plot([0, 52], [0, 52], color='black', linestyle='--', label='Upper bound')\n",
    "\n",
    "    ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "    # regression line (continuing also for higher number of subdocuments than are present in the dataset)\n",
    "    sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line', truncate=False)\n",
    "\n",
    "    \n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.set_xlim(right=51.5)\n",
    "    ax.set_ylim(top=51.5)\n",
    "\n",
    "    ax.set_xticks(np.arange(5, 51, 5))\n",
    "    ax.set_yticks(np.arange(5, 51, 5))\n",
    "\n",
    "    # remove nan values from x and y\n",
    "    x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "    y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "\n",
    "    ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of non-null values per key\")\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title(\"Number of Subdocuments vs. Average Number of Non-Null Values per Key\")\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_RUN{run_idx}_num_subdocs_vs_non_null_values_per_key.png\", dpi=300)\n",
    "\n",
    "    print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average of all runs with error bars\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "max_subdocs = max(runs_prediction_stats_df[0]['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "\n",
    "# y is now the average over the runs in runs_y\n",
    "y = [np.mean([runs_y[run_idx][i] for run_idx in range(len(runs_y))]) for i in range(len(runs_y[0]))]\n",
    "\n",
    "# yerr is the min and max of the runs in runs_y subtracted (for min) and added (for max)\n",
    "yerr = [[y[i] - min([runs_y[run_idx][i] for run_idx in range(len(runs_y))]) for i in range(len(runs_y[0]))], [max([runs_y[run_idx][i] for run_idx in range(len(runs_y))]) - y[i] for i in range(len(runs_y[0]))]]\n",
    "\n",
    "ax.plot([0, 52], [0, 52], color='black', linestyle='--', label='Upper bound')\n",
    "\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "# replace negative values in yerr with 0\n",
    "yerr = [[0 if yerr[0][i] < 0 else yerr[0][i] for i in range(len(yerr[0]))], [0 if yerr[1][i] < 0 else yerr[1][i] for i in range(len(yerr[1]))]]\n",
    "\n",
    "# error bars\n",
    "ax.errorbar(x, y, yerr=yerr, fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line', truncate=False)\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(right=51.5)\n",
    "ax.set_ylim(top=51.5)\n",
    "\n",
    "ax.set_xticks(np.arange(5, 51, 5))\n",
    "ax.set_yticks(np.arange(5, 51, 5))\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of non-null values per key\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title(\"Number of Subdocuments vs. Average Number of Non-Null Values per Key\")\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_num_subdocs_vs_non_null_values_per_key.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_key_stats = []\n",
    "\n",
    "for run_idx, non_null_values_by_key in enumerate(runs_non_null_values_by_key):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for key, values in non_null_values_by_key.items():\n",
    "        x.append(key)\n",
    "        y.append(sum(values)/len(values))\n",
    "\n",
    "    y = [round(y[i], 3) for i in range(len(y))]\n",
    "\n",
    "    key_stats = pd.DataFrame({'key': x, 'avg_num_of_non_null_values': y})\n",
    "    runs_key_stats.append(key_stats)\n",
    "\n",
    "    print(f\"RUN{run_idx}\")\n",
    "    print(key_stats) # avg_num_of_non_null_values is the average number of non-null values per key\n",
    "    print(runs_lenient_accuracy_by_key[run_idx]) # lenient_accuracy_by_key is the accuracy for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "    ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (every unification is trivial)\")\n",
    "\n",
    "    single_run_prediction_stats_df.loc[single_run_prediction_stats_df['num_subdocs'] == 1, 'collision_percentage'] = 0.0\n",
    "    single_run_prediction_stats_df.loc[single_run_prediction_stats_df['num_subdocs'] == 1, 'full_collision_percentage'] = 0.0\n",
    "    \n",
    "    ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=single_run_prediction_stats_df, ax=ax, label=\"Collision percentage\")\n",
    "    \n",
    "    \n",
    "    ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=single_run_prediction_stats_df, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "    avg_collision_percentage = np.nanmean(single_run_prediction_stats_df['collision_percentage'])\n",
    "    ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "    avg_full_collision_percentage = np.nanmean(single_run_prediction_stats_df['full_collision_percentage'])\n",
    "    ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "    ax.set_xlim(left=0, right=51.5)\n",
    "    # x-ticks from 1 to 5 with a step of 1 and then 5 to 50 with a step of 5\n",
    "    ax.set_xticks(np.arange(1, 6, 1).tolist() + np.arange(5, 51, 5).tolist())\n",
    "\n",
    "    ax.set_ylim(bottom=-0.05, top=1.05)\n",
    "    ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "    ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_RUN{run_idx}_collisions_wrt_subdocs_hist.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_runs_prediction_stats_df = pd.concat(runs_prediction_stats_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (every unification is trivial)\")\n",
    "\n",
    "# add 0.0 for num_subdocs == 1\n",
    "combined_runs_prediction_stats_df.loc[combined_runs_prediction_stats_df['num_subdocs'] == 1, 'collision_percentage'] = 0.0\n",
    "combined_runs_prediction_stats_df.loc[combined_runs_prediction_stats_df['num_subdocs'] == 1, 'full_collision_percentage'] = 0.0\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=combined_runs_prediction_stats_df, ax=ax, label=\"Collision percentage\")\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=combined_runs_prediction_stats_df, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "avg_collision_percentage = np.nanmean(combined_runs_prediction_stats_df['collision_percentage'])\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(combined_runs_prediction_stats_df['full_collision_percentage'])\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set_xlim(left=0, right=51.5)\n",
    "# x-ticks from 1 to 5 with a step of 1 and then 5 to 50 with a step of 5\n",
    "ax.set_xticks(np.arange(1, 6, 1).tolist() + np.arange(5, 51, 5).tolist())\n",
    "\n",
    "ax.set_ylim(bottom=-0.05, top=1.05)\n",
    "ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_collisions_wrt_subdocs_hist.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the same as above but with x_lim (right) = 10\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (every unification is trivial)\")\n",
    "\n",
    "# add 0.0 for num_subdocs == 1\n",
    "combined_runs_prediction_stats_df.loc[combined_runs_prediction_stats_df['num_subdocs'] == 1, 'collision_percentage'] = 0.0\n",
    "combined_runs_prediction_stats_df.loc[combined_runs_prediction_stats_df['num_subdocs'] == 1, 'full_collision_percentage'] = 0.0\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=combined_runs_prediction_stats_df, ax=ax, label=\"Collision percentage\")\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=combined_runs_prediction_stats_df, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "avg_collision_percentage = np.nanmean(combined_runs_prediction_stats_df['collision_percentage'])\n",
    "\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(combined_runs_prediction_stats_df['full_collision_percentage'])\n",
    "\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set_xlim(left=0, right=10.5)\n",
    "# x-ticks from 1 to 5 with a step of 1 and then 5 to 50 with a step of 5\n",
    "ax.set_xticks(np.arange(1, 11, 1))\n",
    "\n",
    "ax.set_ylim(bottom=-0.05, top=1.05)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.0, 0.04))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_collisions_wrt_subdocs_hist_zoomed.png\", dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 subdocument == no subdocs (or in other words: the 1 subdoc is the whole document)\n",
    "\n",
    "Note: best case assumes:\n",
    "* perfect OCR\n",
    "* no mistakes in the reports (no typos, no conflicting information on different pages)\n",
    "\n",
    "of course with num_subdocs=2 every collision is also a full_collision :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "runs_key_stats_trf = []\n",
    "runs_avg_collision_percentage = []\n",
    "runs_avg_full_collision_percentage = []\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "\n",
    "    collision_percentage_by_key = {}\n",
    "    full_collision_percentage_by_key = {}\n",
    "\n",
    "\n",
    "    for key in single_run_prediction_stats_df['num_unified_values_per_key'].iloc[0].keys():\n",
    "        for i in range(len(single_run_prediction_stats_df)):\n",
    "            if key not in collision_percentage_by_key:\n",
    "                collision_percentage_by_key[key] = []\n",
    "\n",
    "            if key not in full_collision_percentage_by_key:\n",
    "                full_collision_percentage_by_key[key] = []\n",
    "\n",
    "            collision_percentage_by_key[key].append(single_run_prediction_stats_df['collision_per_key'].iloc[i][key])\n",
    "            full_collision_percentage_by_key[key].append(single_run_prediction_stats_df['full_collision_per_key'].iloc[i][key])\n",
    "\n",
    "    # filter out None values\n",
    "    collision_percentage_by_key = {key: [x for x in collision_percentage_by_key[key] if x is not None] for key in collision_percentage_by_key.keys()}\n",
    "    full_collision_percentage_by_key = {key: [x for x in full_collision_percentage_by_key[key] if x is not None] for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "    # calculate average and ignore nan values and round to 3 decimal places\n",
    "    collision_percentage_by_key = {key: round(np.nanmean(collision_percentage_by_key[key]), 3) for key in collision_percentage_by_key.keys()}\n",
    "    full_collision_percentage_by_key = {key: round(np.nanmean(full_collision_percentage_by_key[key]), 3) for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "    # add those values to the dataframe\n",
    "    runs_key_stats[run_idx]['collision_percentage'] = runs_key_stats[run_idx]['key'].map(collision_percentage_by_key)\n",
    "    runs_key_stats[run_idx]['full_collision_percentage'] = runs_key_stats[run_idx]['key'].map(full_collision_percentage_by_key)\n",
    "\n",
    "    # create a barplot that has grouped bars\n",
    "\n",
    "    # we want to use the \"hue\" parameter to group the bars by collision vs. full collision\n",
    "    # thus we have to transform the dataframe to have a column for each of the two types of collisions\n",
    "    # and a column for the key\n",
    "    key_stats_trf = runs_key_stats[run_idx].melt(id_vars=['key'], value_vars=['collision_percentage', 'full_collision_percentage'], var_name='collision_type', value_name='collision_pct')\n",
    "\n",
    "    # rename the collision types to something more readable\n",
    "    key_stats_trf['collision_type'] = key_stats_trf['collision_type'].map({'collision_percentage': 'Collision percentage', 'full_collision_percentage': 'Full Collision percentage'})\n",
    "\n",
    "    runs_key_stats_trf.append(key_stats_trf)\n",
    "\n",
    "    # create the barplot\n",
    "    ax = sns.barplot(x=\"key\", y=\"collision_pct\", hue=\"collision_type\", data=key_stats_trf, ax=ax)\n",
    "\n",
    "    avg_collision_percentage = np.nanmean(list(collision_percentage_by_key.values()))\n",
    "    ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "    runs_avg_collision_percentage.append(avg_collision_percentage)\n",
    "\n",
    "    avg_full_collision_percentage = np.nanmean(list(full_collision_percentage_by_key.values()))\n",
    "    ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "    runs_avg_full_collision_percentage.append(avg_full_collision_percentage)\n",
    "\n",
    "    ax.set(xlabel=\"Key\", ylabel=\"(Full) collision percentage\", title=\"Collision Percentage by Key\")\n",
    "\n",
    "    ax.set_ylim(bottom=0, top=1.05)\n",
    "    ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_RUN{run_idx}_collision_percentage_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "\n",
    "ax = sns.barplot(x=\"key\", y=\"collision_pct\", hue=\"collision_type\", data=pd.concat(runs_key_stats_trf), ax=ax, errorbar=(\"pi\", 100))\n",
    "avg_collision_percentage = np.nanmean(runs_avg_collision_percentage)\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "runs_avg_collision_percentage.append(avg_collision_percentage)\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(runs_avg_full_collision_percentage)\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "runs_avg_full_collision_percentage.append(avg_full_collision_percentage)\n",
    "\n",
    "ax.set(xlabel=\"Key\", ylabel=\"(Full) collision percentage\", title=\"Collision Percentage by Key\")\n",
    "\n",
    "ax.set_ylim(bottom=0, top=1.05)\n",
    "ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_collision_stats_by_key_barplot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of full collision percentage to collision percentage for each run and key and attach it as a column to the dataframe\n",
    "for run_idx in range(len(runs_key_stats)):\n",
    "    runs_key_stats[run_idx]['full_collision_over_collision'] = round(runs_key_stats[run_idx]['full_collision_percentage'] / runs_key_stats[run_idx]['collision_percentage'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_runs_key_stats = pd.concat(runs_key_stats)\n",
    "combined_runs_lenient_accuracy_by_key = pd.concat(runs_lenient_accuracy_by_key)\n",
    "combined_own_evaluation_by_key = pd.concat([own_evaluation.mean(axis=0, skipna=True) for own_evaluation in own_evaluations], axis=1)\n",
    "combined_runs_accuracies_by_key = pd.concat(runs_accuracies_by_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_runs_lenient_accuracy_by_key.reset_index(drop=True, inplace=True)\n",
    "combined_runs_lenient_accuracy_by_key = combined_runs_lenient_accuracy_by_key.T\n",
    "\n",
    "# put the columns below each other (instead of next to each other)\n",
    "combined_runs_lenient_accuracy_by_key = combined_runs_lenient_accuracy_by_key.stack().reset_index()\n",
    "combined_runs_lenient_accuracy_by_key.columns = ['key', 'run', 'lenient_accuracy']\n",
    "\n",
    "# copy the index into a column\n",
    "combined_runs_lenient_accuracy_by_key['index'] = combined_runs_lenient_accuracy_by_key.index\n",
    "\n",
    "# change the order so that the keys of a single run are next to each other. sort by run and then by index of the row (not a real column)\n",
    "combined_runs_lenient_accuracy_by_key = combined_runs_lenient_accuracy_by_key.sort_values(['run', 'index']).reset_index(drop=True)\n",
    "\n",
    "# drop the index column\n",
    "combined_runs_lenient_accuracy_by_key.drop(columns=['index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the columns below each other (instead of next to each other)\n",
    "combined_own_evaluation_by_key = combined_own_evaluation_by_key.stack().reset_index()\n",
    "\n",
    "# rename the columns\n",
    "combined_own_evaluation_by_key.columns = ['key', 'run', 'score']\n",
    "\n",
    "# copy the index into a column\n",
    "combined_own_evaluation_by_key['index'] = combined_own_evaluation_by_key.index\n",
    "\n",
    "# change the order so that the keys of a single run are next to each other. sort by run and then by index of the row (not a real column)\n",
    "combined_own_evaluation_by_key = combined_own_evaluation_by_key.sort_values(['run', 'index']).reset_index(drop=True)\n",
    "\n",
    "# drop the index column\n",
    "combined_own_evaluation_by_key.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[By key] Correlation coefficient between avg_num_of_non_null_values and collision percentage: \" + str(round(np.corrcoef(combined_runs_key_stats['avg_num_of_non_null_values'], combined_runs_key_stats['collision_percentage'])[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"[By key] Correlation coefficient between avg_num_of_non_null_values and (own) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['avg_num_of_non_null_values'], combined_own_evaluation_by_key['score'])[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between avg_num_of_non_null_values and (official) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['avg_num_of_non_null_values'], combined_runs_accuracies_by_key)[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between avg_num_of_non_null_values and (official) lenient accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['avg_num_of_non_null_values'], combined_runs_lenient_accuracy_by_key['lenient_accuracy'])[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"[By key] Correlation coefficient between collision percentage and (own) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['collision_percentage'], combined_own_evaluation_by_key['score'])[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between collision percentage and (official) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['collision_percentage'], combined_runs_accuracies_by_key)[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['collision_percentage'], combined_runs_lenient_accuracy_by_key['lenient_accuracy'])[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"[By key] Correlation coefficient between full collision percentage and (own) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['full_collision_percentage'], combined_own_evaluation_by_key['score'])[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between full collision percentage and (official) accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['full_collision_percentage'], combined_runs_accuracies_by_key)[0, 1], 3)))\n",
    "print(\"[By key] Correlation coefficient between full collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(combined_runs_key_stats['full_collision_percentage'], combined_runs_lenient_accuracy_by_key['lenient_accuracy'])[0, 1], 3)))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_key_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_runs_key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the rows by key and leave the order of the rows intact\n",
    "combined_runs_key_stats_grouped = combined_runs_key_stats.groupby('key', as_index=False).first()\n",
    "\n",
    "# re-calculate full_collision_over_collision by using the new collision_percentage and full_collision_percentage\n",
    "combined_runs_key_stats_grouped['full_collision_over_collision'] = combined_runs_key_stats_grouped['full_collision_percentage'] / combined_runs_key_stats_grouped['collision_percentage']\n",
    "\n",
    "# reorder the rows\n",
    "# COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n",
    "combined_runs_key_stats_grouped = combined_runs_key_stats_grouped.reindex([1, 0, 2, 5, 6, 3, 7, 4]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# draw a heatmap and fix the colorbar from 0 to 1\n",
    "sns.heatmap(combined_runs_key_stats_grouped.iloc[:, 2:], annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, vmin=0, vmax=1)\n",
    "ax.set(xlabel=\"Metric\", ylabel=\"Key\", title=\"Collision Statistics by Key\")\n",
    "\n",
    "ax.set_xticklabels(['Collision Percentage', 'Full Collision Percentage', 'Full Collision over Collision'])\n",
    "\n",
    "# set the yticks to the key names\n",
    "ax.set_yticklabels(combined_runs_key_stats_grouped['key'], rotation=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_collision_stats_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the own_evaluation.mean(axis=1, skipna=True) to the prediction_stats dataframe\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "    single_run_prediction_stats_df['own_evaluation_accuracy'] = own_evaluations[run_idx].mean(axis=1, skipna=True)\n",
    "\n",
    "runs_prediction_stats_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(runs_prediction_stats_df[0]['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "runs_y = []\n",
    "\n",
    "for run_idx, single_run_prediction_stats_df in enumerate(runs_prediction_stats_df):\n",
    "\n",
    "    single_run_y = [] # avg. accuracy for each number of subdocs\n",
    "    # yerr = [] # standard deviation for each number of subdocs\n",
    "\n",
    "    # scatter plot of the avg. own_evaluation accuracy vs num_subdocs\n",
    "    for i in range(0, max_subdocs+1):\n",
    "        single_run_y.append(runs_prediction_stats_df[run_idx][runs_prediction_stats_df[run_idx]['num_subdocs'] == i]['own_evaluation_accuracy'].mean())\n",
    "        #yerr.append(prediction_stats[prediction_stats['num_subdocs'] == i]['own_evaluation_accuracy'].std())\n",
    "\n",
    "    runs_y.append(single_run_y)\n",
    "\n",
    "\n",
    "# combined y is the average of the runs_y\n",
    "y = np.mean(runs_y, axis=0)\n",
    "\n",
    "# yerr is the min and max of the runs in runs_y subtracted (for min) and added (for max)\n",
    "yerr = [[y[i] - min([runs_y[run_idx][i] for run_idx in range(len(runs_y))]) for i in range(len(runs_y[0]))], [max([runs_y[run_idx][i] for run_idx in range(len(runs_y))]) - y[i] for i in range(len(runs_y[0]))]]\n",
    "\n",
    "# replace very small negative values (due to rounding errors) with 0\n",
    "yerr = [[0 if yerr[0][i] > -0.0001 else yerr[0][i] for i in range(len(yerr[0]))], [0 if yerr[1][i] < 0.0001 else yerr[1][i] for i in range(len(yerr[1]))]]\n",
    "\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "# plot the error bars\n",
    "ax.errorbar(x, y, yerr=yerr, fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "mpl.rcParams['axes.xmargin'] = 1  # set very wide margins: 100% of the x-axis range so that the regression line is not cut off\n",
    "\n",
    "# regression line\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line', truncate=False)\n",
    "\n",
    "# set the labels\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Accuracy\", title=\"Accuracy by number of subdocuments\")\n",
    "\n",
    "ax.set_xlim(0, 52)\n",
    "ax.set_xticks(np.arange(0, 52, 5))\n",
    "\n",
    "ax.set_ylim(0, 0.75)\n",
    "ax.set_yticks(np.arange(0, 0.76, 0.05))\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(\"Number of Subdocuments vs. Accuracy (own evaluation)\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL}_{TEMPERATURE}_average_own_eval_accuracy_vs_num_subdocs.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
