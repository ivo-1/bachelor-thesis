{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Levenshtein import distance\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTIONS_PATH = \"datasets/kleister_charity/dev-0/predictions/davinci/2022-12-15T16-36-13_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT3_Davinci(max_input_tokens=3840, temperature=0, top_p=0.9, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv\"\n",
    "PREDICTIONS_PATH = \"datasets/kleister_charity/dev-0/predictions/neox/2022-12-11T00-25-32_LLMPipeline(prompt_variant=NeutralPrompt, model=GPT_NeoX(max_input_tokens=1792, temperature=1, top_p=0.9, top_k=40, presence_penalty=0, frequency_penalty=0), parser=KleisterCharityParser).tsv\"\n",
    "# LOG_PATH = \"../logs/davinci/2022-12-15T16-36-05_davinci_temp_0.log\"\n",
    "LOG_PATH = \"../logs/neox/2022-12-11T00-25-32_neox_temp_1.log\"\n",
    "\n",
    "SPLIT = re.search(r'datasets/kleister_charity/(.*?)/', PREDICTIONS_PATH).group(1)\n",
    "\n",
    "if SPLIT == 'dev-0':\n",
    "    SPLIT = 'dev'\n",
    "\n",
    "elif SPLIT == 'test-A':\n",
    "    SPLIT = 'test' \n",
    "\n",
    "MODEL_NAME = re.search(r'model=(.*)\\(', PREDICTIONS_PATH).group(1)\n",
    "if MODEL_NAME == 'GPT3_Davinci':\n",
    "    MODEL_NAME = 'davinci' \n",
    "\n",
    "elif MODEL_NAME == 'GPT_NeoX':\n",
    "    MODEL_NAME = 'neox'\n",
    "\n",
    "TEMPERATURE = re.search(r'temperature=(.*),', PREDICTIONS_PATH).group(1).split(',')[0]\n",
    "COLUMN_ORDER = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Annual Income\", \"Period End Date\", \"Annual Spending\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading solution (expected.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROADWAY</td>\n",
       "      <td>WR12_7NL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WESTCLIFF-ON-SEA</td>\n",
       "      <td>SS0_8HX</td>\n",
       "      <td>47_SECOND_AVENUE</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>1022119</td>\n",
       "      <td>10348000.00</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>9415000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHELTENHAM</td>\n",
       "      <td>GL50_3EP</td>\n",
       "      <td>BAYSHILL_ROAD</td>\n",
       "      <td>Cheltenham_Ladies_College</td>\n",
       "      <td>311722</td>\n",
       "      <td>32168000.00</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>27972000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHREWSBURY</td>\n",
       "      <td>SY3_7PQ</td>\n",
       "      <td>58_TRINITY_STREET</td>\n",
       "      <td>The_Sanata_Charitable_Trust</td>\n",
       "      <td>1132766</td>\n",
       "      <td>255653.00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>258287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WARE</td>\n",
       "      <td>SG11_2DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>122836.00</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>124446.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Address (post town) Address (post code)   Address (street)  \\\n",
       "0            BROADWAY            WR12_7NL                NaN   \n",
       "1    WESTCLIFF-ON-SEA             SS0_8HX   47_SECOND_AVENUE   \n",
       "2          CHELTENHAM            GL50_3EP      BAYSHILL_ROAD   \n",
       "3          SHREWSBURY             SY3_7PQ  58_TRINITY_STREET   \n",
       "4                WARE            SG11_2DY                NaN   \n",
       "\n",
       "                  Charity Name Charity Number Annual Income Period End Date  \\\n",
       "0   Wormington_Village_Society        1155074           NaN      2018-07-31   \n",
       "1     Havens_Christian_Hospice        1022119   10348000.00      2016-03-31   \n",
       "2    Cheltenham_Ladies_College         311722   32168000.00      2016-07-31   \n",
       "3  The_Sanata_Charitable_Trust        1132766     255653.00      2015-12-31   \n",
       "4          Cantate_Youth_Choir        1039369     122836.00      2013-12-31   \n",
       "\n",
       "  Annual Spending  \n",
       "0             NaN  \n",
       "1      9415000.00  \n",
       "2     27972000.00  \n",
       "3       258287.00  \n",
       "4       124446.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = pd.read_csv('datasets/kleister_charity/dev-0/expected.tsv', sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in expected[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\")\n",
    "        expected.loc[expected[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "# renaming and sorting for better readability\n",
    "expected.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Address (street)\", \"Annual Income\",  \"Annual Spending\"]\n",
    "expected = expected[COLUMN_ORDER]\n",
    "\n",
    "expected = expected.drop(columns=[\"raw\"])\n",
    "expected.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of keys that actually have a value (are not NaN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Address (post town)    0.959091\n",
       "Address (post code)    0.968182\n",
       "Address (street)       0.886364\n",
       "Charity Name           1.000000\n",
       "Charity Number         0.993182\n",
       "Annual Income          0.986364\n",
       "Period End Date        1.000000\n",
       "Annual Spending        0.986364\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage of keys that actually have a value (are not NaN):\")\n",
    "expected.count() / len(expected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Predictions (single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>Address (post town)</th>\n",
       "      <th>Address (post code)</th>\n",
       "      <th>Address (street)</th>\n",
       "      <th>Charity Name</th>\n",
       "      <th>Charity Number</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Period End Date</th>\n",
       "      <th>Annual Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>address__post_town=N/A address__postcode=N/A a...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Wormington_Village_Society</td>\n",
       "      <td>1155074</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>.00</td>\n",
       "      <td>.00......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>address__post_town=Havens_Hospices address__po...</td>\n",
       "      <td>Havens_Hospices</td>\n",
       "      <td>E20_5SX</td>\n",
       "      <td>Fair_Havens_Hospital</td>\n",
       "      <td>Havens_Christian_Hospice</td>\n",
       "      <td>12345678</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>16848000.00</td>\n",
       "      <td>34747000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>address__post_town=Swansea address__postcode=S...</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>SA1_1BL</td>\n",
       "      <td>College</td>\n",
       "      <td>Cheltenham_Ladies_College_Charitable_Trust</td>\n",
       "      <td>566101</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>10001.00</td>\n",
       "      <td>20221210.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spending_annually_in_british_pounds=2015312015.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015312015.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>address__post_town=Bishop’s_Stortford,_Hertfor...</td>\n",
       "      <td>Bishop’s_Stortford,_Hertfordshire,_CM23_3UZ</td>\n",
       "      <td>CM23_3UZ</td>\n",
       "      <td>The_Gables</td>\n",
       "      <td>Cantate_Youth_Choir</td>\n",
       "      <td>1039369</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>21900.00</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw  \\\n",
       "0  address__post_town=N/A address__postcode=N/A a...   \n",
       "1  address__post_town=Havens_Hospices address__po...   \n",
       "2  address__post_town=Swansea address__postcode=S...   \n",
       "3  spending_annually_in_british_pounds=2015312015.00   \n",
       "4  address__post_town=Bishop’s_Stortford,_Hertfor...   \n",
       "\n",
       "                           Address (post town) Address (post code)  \\\n",
       "0                                          N/A                 N/A   \n",
       "1                              Havens_Hospices             E20_5SX   \n",
       "2                                      Swansea             SA1_1BL   \n",
       "3                                          NaN                 NaN   \n",
       "4  Bishop’s_Stortford,_Hertfordshire,_CM23_3UZ            CM23_3UZ   \n",
       "\n",
       "       Address (street)                                Charity Name  \\\n",
       "0                   N/A                  Wormington_Village_Society   \n",
       "1  Fair_Havens_Hospital                    Havens_Christian_Hospice   \n",
       "2               College  Cheltenham_Ladies_College_Charitable_Trust   \n",
       "3                   NaN                                         NaN   \n",
       "4            The_Gables                         Cantate_Youth_Choir   \n",
       "\n",
       "  Charity Number Annual Income Period End Date Annual Spending  \n",
       "0        1155074    2017-08-01             .00       .00......  \n",
       "1       12345678    2016-03-31     16848000.00     34747000.00  \n",
       "2         566101    2020-07-31        10001.00     20221210.00  \n",
       "3            NaN           NaN             NaN   2015312015.00  \n",
       "4        1039369    2013-12-31        21900.00        10000.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.read_csv(PREDICTIONS_PATH, sep='\\t', header=None, names=['raw'])\n",
    "\n",
    "for key_value_pair in predictions[\"raw\"]:\n",
    "    for key_value in key_value_pair.split(\" \"):\n",
    "        key, value = key_value.split(\"=\", 1)\n",
    "        predictions.loc[predictions[\"raw\"] == key_value_pair, key] = value\n",
    "\n",
    "predictions.columns = [\"raw\", \"Address (post town)\", \"Address (post code)\", \"Address (street)\", \"Charity Name\", \"Charity Number\", \"Period End Date\", \"Annual Income\", \"Annual Spending\"]\n",
    "predictions = predictions[COLUMN_ORDER]\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.drop(columns=[\"raw\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Log of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(LOG_PATH, \"r\")\n",
    "log_lines = [line.strip() for line in log_file.readlines() if line.strip() and line.startswith(\"20\") and \"Raw value:\" not in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61145\n"
     ]
    }
   ],
   "source": [
    "print(len(log_lines))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations (answering RH1)\n",
    "## RH1\n",
    "> Unimodal approach can reach satisfactory performance while being more cost-efficient than current state-of-the-art multi-modal approaches\n",
    "\n",
    "Where we define satisfactory performance as:\n",
    "> 80% of the values for given and findable keys are correctly found (no distinction for the other 20%, they can be either wrong or missing (which is of course also wrong)). Correctness is defined as a case-insensitive (upper-casing everything) string match with some normalisation (details below)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisations and evaluating according to own definition of \"correctness\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address (post town)\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "OTTERY_ST_MARY | Ottery_St._Mary\n",
    "Lichfield | City_of_Lichfield\n",
    "Liverpool | City_of_Liverpool\n",
    "\n",
    "Normalisation:\n",
    "*  `<Solution City>` vs. `City of <Solution City>` are both correct\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition, but not a substitution) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary)\n",
    "\n",
    "### Address (post code)\n",
    "NO NORMALISATION\n",
    "\n",
    "### Address (street)\n",
    "Examples: \n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "36_BELLINGHAM_DRIVE | Unit_36_Bellingham_Drive\n",
    "34_DECIMA_STREET | Sherborne_House,_34_Decima_Street\n",
    "190_LONG_LANE | Scout_Centre,_Rear_190_Long_Lane\n",
    "13_ROSSLYN_ROAD | Room_16,_ETNA_Community_Centre,_13_Rosslyn_Road\n",
    "FURNIVAL_GATE | 2_Floor,_Midcity_House,_Furnival_Gate\n",
    "7-14_Great_Dover_Street | 7_-_14_Great_Dover_Street\n",
    "BROWNBERRIE_LANE | Leeds_Trinity_University,_Brownberrie_Lane\n",
    "\n",
    "Normalisation: \n",
    "* Delete Spaces around \"-\" in both solution and prediction\n",
    "\n",
    "Was considering generally cutting off at ,_ before or after the street but ultimately decided against it because it cannot be generally stated that having something in front or after the correct street would still make mail arrive at the destination.\n",
    "\n",
    "Also: Levenshtein edit distance of 1 doesn't make sense here as getting the number wrong (e.g. 13 instead of 1) is a clear mistake.\n",
    "\n",
    "### Charity Name\n",
    "Examples:\n",
    "Solution | Predicted\n",
    "| --- | --- |\n",
    "Cheltenham Ladies College | Cheltenham Ladies' College\n",
    "Battersea_Dogs'_and_Cats'_Home | Battersea_Dogs'_&_Cats'_Home\n",
    "Beer_Shmuel_Ltd. | Beer_Shmuel_Limited\n",
    "Catch_22_Charity_Ltd. | Catch22\n",
    "Richard_Hicks | Richard_Hicks_Charity\n",
    "King's_Schools_Taunton_Ltd. | King's_Schools_Taunton_Limited\n",
    "KEY_ENTERPRISES_(1983)_LTD. | KEY_ENTERPRISES_(1983)_LIMITED\n",
    "Louth_Playgoers_Society_Ltd. | Louth_Playgoers_Society_Limited\n",
    "Boxgrove_Village_Hall_and_Community_Centre | BOXGROVE_VILLAGE_HALL_&_COMMUNITY_CENTRE_CIO\n",
    "London_Transport_Museum | London_Transport_Museum_Ltd.\n",
    "The_Momc-Leigh_Park_Crafts_Initiative_Trust_Ltd. | THE_MOMC_-_LEIGH_PARK_CRAFTS_INITIATIVE_TRUST_LIMITED\n",
    "King_Edward_Vi's_School_At_Chelmsford | King_Edward_VI_School_at_Chelmsford\n",
    "The_Hope_Foundation_Ltd. | The_Hope_Foundation\n",
    "Nottingham_Women's_Counselling_Service | The_Nottingham_Women's_Counselling_Service\n",
    "\n",
    "Normalisation (+ give stats for how many values this applies):\n",
    "* Cut off Ltd, Ltd. and Limited from the end of both prediction and solution \n",
    "* Replace \"&\" with \"and\" in both prediction and solution\n",
    "* Delete Spaces around \"-\" in both prediction and solution\n",
    "* Levenshtein: edit distance of 1 (1 deletion or 1 addition) counts as correct (e.g. St.Mary vs. St._Mary or St_Mary vs St._Mary, King_Edward_VI's_School vs. King_Edward_VI_School)\n",
    "\n",
    "\n",
    "### Charity Number\n",
    "NO NORMALISATION\n",
    "\n",
    "### Period End Date\n",
    "NO NORMALISATION\n",
    "\n",
    "### Annual Income\n",
    "NO NORMALISATION\n",
    "\n",
    "### Annual Spending\n",
    "NO NORMALISATION\n",
    "\n",
    "### Other Normalisations\n",
    "Replaced uncommon character: ’ (U+2019) with ' (in the predictions and the solution) (applies to 4 values in the whole solution of the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_quotation_mark(df):\n",
    "    \"\"\"\n",
    "    Replace U+2019 (right single quotation mark) with U+0027 (apostrophe) in a dataframe and return the number of replacements.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if isinstance(value, str):\n",
    "                if \"’\" in value:\n",
    "                    df.loc[index, column] = value.replace(\"’\", \"'\")\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "count = replace_quotation_mark(predictions)\n",
    "print(f\"Replaced {count} quotation marks in predictions.\")\n",
    "\n",
    "count = replace_quotation_mark(expected)\n",
    "print(f\"Replaced {count} quotation marks in expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(key, expected_value, predicted_value):\n",
    "    \"\"\"\n",
    "    Our definition of correctness for each key with the normalisation rules applied.\n",
    "    \"\"\"\n",
    "    upper_cased_expected = str(expected_value).upper()\n",
    "    upper_cased_predicted = str(predicted_value).upper()\n",
    "\n",
    "    if key == \"Address (post town)\":\n",
    "        if upper_cased_expected.startswith(\"CITY_OF_\") or upper_cased_expected.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_expected = upper_cased_expected[8:]\n",
    "        if upper_cased_predicted.startswith(\"CITY_OF_\") or upper_cased_predicted.startswith(\"TOWN_OF_\"):\n",
    "            upper_cased_predicted = upper_cased_predicted[8:]\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "\n",
    "    elif key == \"Address (street)\":\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return upper_cased_expected == upper_cased_predicted\n",
    "    \n",
    "    elif key == \"Charity Name\":\n",
    "        upper_cased_expected = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_LTD|_LTD.|_LIMITED)$\", \"\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(&)\", \"and\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(&)\", \"and\", upper_cased_predicted)\n",
    "\n",
    "        upper_cased_expected = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_expected)\n",
    "        upper_cased_predicted = re.sub(r\"(_)(-)(_)\", r\"\\2\", upper_cased_predicted)\n",
    "        return distance(upper_cased_expected, upper_cased_predicted, weights=(1, 1, 2)) <= 1\n",
    "    \n",
    "    else:\n",
    "        return upper_cased_expected == upper_cased_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_evaluation = pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) # empty dataframe with NaNs that will be filled with 0s (wrong) and 1s (correct), e.g. document 42, key \"Charity Name\": 1\n",
    "own_evaluation = pd.DataFrame(np.nan, index=expected.index, columns=expected.columns) # empty dataframe with NaNs that will be filled with 0s (wrong) and 1s (correct), e.g. document 42, key \"Charity Name\": 1\n",
    "null_evaluation = pd.DataFrame(np.zeros((4, len(expected.columns))), index=[\"TP\", \"FP\", \"FN\", \"TN\"], columns=expected.columns) # empty dataframe with zeros that will be filled with the number of true positives, false positives, false negatives and true negatives *w.r.t. null's*, e.g. key \"Charity Name\": 42\n",
    "\n",
    "for index, row in expected.iterrows():\n",
    "    for column in expected.columns:\n",
    "        if pd.notnull(row[column]):\n",
    "            # FP: we predicted null and it was not null\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                null_evaluation.loc[\"FP\", column] += 1\n",
    "\n",
    "            # TN: we predicted not null and it was not null\n",
    "            else:\n",
    "                null_evaluation.loc[\"TN\", column] += 1\n",
    "            \n",
    "            if is_correct(column, row[column], predictions.loc[index, column]):\n",
    "                own_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                own_evaluation.loc[index, column] = 0\n",
    "\n",
    "            if str(row[column]).upper() == str(predictions.loc[index, column]).upper():\n",
    "                official_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                official_evaluation.loc[index, column] = 0\n",
    "\n",
    "        else:\n",
    "            # TP: we predicted null and it was null\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                null_evaluation.loc[\"TP\", column] += 1\n",
    "            # FN: we predicted not null and it was null\n",
    "            else:\n",
    "                null_evaluation.loc[\"FN\", column] += 1\n",
    "\n",
    "            if pd.isnull(predictions.loc[index, column]):\n",
    "                official_evaluation.loc[index, column] = 1\n",
    "            else:\n",
    "                official_evaluation.loc[index, column] = 0\n",
    "\n",
    "# only looks at the keys that are actually present in the document\n",
    "print(f\"(own) (macro) average (over the keys) of correctly predicted values: {own_evaluation.mean(axis=0, skipna=True).mean()}\")\n",
    "print(f\"(own) (micro) average (over all predictions) of correct values: {own_evaluation.mean(axis=1, skipna=True).mean()}\")\n",
    "\n",
    "print(f\"Own evaluation by key:\\n{own_evaluation.mean(axis=0, skipna=True)}\")\n",
    "\n",
    "# official evaluation\n",
    "print(f\"(official) (macro)[over keys] average  of correctly predicted values: {official_evaluation.mean(axis=0, skipna=True).mean()}\")\n",
    "print(f\"(official) (micro)[over all predictions] average of correctly predicted values: {official_evaluation.mean(axis=1, skipna=True).mean()}\")\n",
    "\n",
    "print(f\"Official evaluation by key:\\n{official_evaluation.mean(axis=0, skipna=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation coefficient between own and official evaluation (by key):\\n{str(np.corrcoef(official_evaluation.mean(axis=0, skipna=True), own_evaluation.mean(axis=0, skipna=True))[0, 1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often does it say null for a key (not in subdocs, but for whole document) (wrongly vs. correctly) → F_1_{null}\n",
    "print(f\"Null evaluation:\\n{null_evaluation}\")\n",
    "\n",
    "# micro f1 score for null\n",
    "precision = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FP\", :].sum()) # micro average\n",
    "recall = null_evaluation.loc[\"TP\", :].sum() / (null_evaluation.loc[\"TP\", :].sum() + null_evaluation.loc[\"FN\", :].sum()) # micro average\n",
    "f1 = 2 * precision * recall / (precision + recall) # micro average\n",
    "print(f\"(micro)[over all predictions] Precision for null: {precision}\")\n",
    "print(f\"(micro)[over all predictions] Recall for null: {recall}\")\n",
    "print(f\"(micro)[over all predictions] F1 score for null: {f1}\")\n",
    "\n",
    "# macro f1 score for null\n",
    "# per key and then average\n",
    "null_scores_by_key = {}\n",
    "for key in null_evaluation.columns:\n",
    "    precision = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FP\", key])\n",
    "    recall = null_evaluation.loc[\"TP\", key] / (null_evaluation.loc[\"TP\", key] + null_evaluation.loc[\"FN\", key])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    null_scores_by_key[key] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "null_scores_by_key = pd.DataFrame(null_scores_by_key).T\n",
    "print(null_scores_by_key)\n",
    "print(f\"(macro)[over the keys] Precision for null: {null_scores_by_key['precision'].mean()}\")\n",
    "print(f\"(macro)[over the keys] Recall for null: {null_scores_by_key['recall'].mean()}\")\n",
    "print(f\"(macro)[over the keys] F1 score for null: {null_scores_by_key['f1'].mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that micro and macro average of the  are quite close together because as we saw in the beginning, almost all keys are given in the data set. There is no big \"class\" (key) imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifications, collisions, lenient accuracy and looking at repetetiveness (all on subdocument level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Unification\n",
    "A unification is whenever there are two or more non-null values coming from the subdoc predictions for the same key. If there is only one non-null value coming from the subdocs (which is always the case if there only is one subdoc but can also happen with more than one subdoc) then it's not a unification.\n",
    "\n",
    "A trivial unification is a unification where all values are the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Collision\n",
    "Given a unification, we describe two or more different values for the same key as the unification having a collision. \n",
    "\n",
    "So a unification is either trivial (all values the same) or it has a collision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a Full Collision\n",
    "Same as a collision but with the constraint that *all* values are different (not just any two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_stats_dict = []\n",
    "predictions_dict = []\n",
    "empty_key_dict = {\n",
    "    \"Address (post town)\": None,\n",
    "    \"Address (post code)\": None,\n",
    "    \"Address (street)\": None,\n",
    "    \"Charity Name\": None,\n",
    "    \"Charity Number\": None,\n",
    "    \"Annual Income\": None,\n",
    "    \"Period End Date\": None,\n",
    "    \"Annual Spending\": None,\n",
    "}\n",
    "\n",
    "\n",
    "for line in log_lines:\n",
    "    if \"Predicting document\" in line: # this is the beginning of a prediction\n",
    "        # create a new dictionary for this document\n",
    "        prediction_stats_dict.append({\n",
    "            \"num_subdocs\": None,\n",
    "            \"num_unifications\": 0,\n",
    "            \"collision_per_key\": empty_key_dict.copy(),\n",
    "            \"full_collision_per_key\": empty_key_dict.copy(),\n",
    "            \"num_unified_values_per_key\": empty_key_dict.copy(),\n",
    "            \"correct_in_any_subdoc_per_key\": empty_key_dict.copy(),\n",
    "            \"collision_percentage\": None,\n",
    "            \"full_collision_percentage\": None,\n",
    "            \"correct_in_any_subdoc_percentage\": None,\n",
    "        })\n",
    "        predictions_dict.append(empty_key_dict.copy())\n",
    "\n",
    "    elif \"Final prediction for document\" in line: # this is the end of a prediction\n",
    "        # calculate the percentages\n",
    "        prediction_stats_dict[-1][\"collision_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"collision_per_key\"].values() if x is not None])\n",
    "        prediction_stats_dict[-1][\"full_collision_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"full_collision_per_key\"].values() if x is not None])\n",
    "        prediction_stats_dict[-1][\"correct_in_any_subdoc_percentage\"] = np.mean([x for x in prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"].values() if x is not None])\n",
    " \n",
    "    elif \"No subdocs necessary\" in line:\n",
    "        prediction_stats_dict[-1][\"num_subdocs\"] = 1\n",
    "\n",
    "    elif \"Split document into\" in line:\n",
    "        num_subdocs = int(re.search(\"into (\\d+) subdocuments\", line).group(1))\n",
    "        prediction_stats_dict[-1][\"num_subdocs\"] = num_subdocs\n",
    "\n",
    "    elif \"- parse_model_output() - Key:\" in line:\n",
    "        key = re.search(\"- parse_model_output\\(\\) - Key: (.*):\", line).group(1)\n",
    "        # the prediction is always in the next line (unless the key was not predicted at all) or the prediction is an empty string\n",
    "        try:\n",
    "            prediction = re.search(\"- parse_model_output\\(\\) - Stripped value: (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "            \n",
    "\n",
    "        except AttributeError:\n",
    "            print(f\"Key {key} was not predicted at all (empty string prediction).\")\n",
    "            prediction = \"[METADATA]: EMPTY_STRING_PREDICTION\"\n",
    "            \n",
    "        if predictions_dict[-1][key] is None:\n",
    "                predictions_dict[-1][key] = [prediction]\n",
    "        else:\n",
    "            predictions_dict[-1][key].append(prediction)\n",
    "\n",
    "    elif \"Unification necessary for key\" in line:\n",
    "        key = re.search(\"Unification necessary for key (.*)\", line).group(1)\n",
    "        prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "        values = re.search(\"Unifying \\d+ \\(lowered\\) values (.*)\", log_lines[log_lines.index(line)+1]).group(1)\n",
    "\n",
    "        # values is the string representation of a list, so we can use eval to turn it into a list\n",
    "        values = eval(values)\n",
    "        unified_values = int(re.search(\"Unifying (\\d+) \\(lowered\\) values\", log_lines[log_lines.index(line)+1]).group(1))\n",
    "\n",
    "        assert unified_values == len(values) # sanity check\n",
    "\n",
    "        # if there is more than 1 value, then it's a unification (because these values don't include null values)\n",
    "        if len(values) > 1:\n",
    "            prediction_stats_dict[-1][\"num_unifications\"] += 1\n",
    "\n",
    "            # if there are more than 1 different values, then it's a collision\n",
    "            if len(set(values)) > 1:\n",
    "                prediction_stats_dict[-1][\"collision_per_key\"][key] = True\n",
    "\n",
    "            # if the length of the set is equal to the length of the list, then it's a full collision\n",
    "            if len(set(values)) == len(values):\n",
    "                prediction_stats_dict[-1][\"full_collision_per_key\"][key] = True\n",
    "\n",
    "        \n",
    "        prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = unified_values\n",
    "\n",
    "        # which document are we in?\n",
    "        doc_num = len(prediction_stats_dict) - 1\n",
    "\n",
    "        # get the correct value for this key\n",
    "        correct_value = expected.iloc[doc_num][key]\n",
    "\n",
    "        # if it's NaN, then we were not supposed to predict anything for this key but we did (in at least one subdoc)\n",
    "        if pd.isna(correct_value):\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False # TODO: ask if this is correct\n",
    "            continue\n",
    "\n",
    "        # we have to transform the values in the list to the same format as the correct value\n",
    "        values = [x.replace(\" \", \"_\").replace(\":\", \"_\").upper() for x in values]\n",
    "\n",
    "        # also transform the correct value to uppercase\n",
    "        correct_value = str(correct_value).upper()\n",
    "\n",
    "        # if the correct value is in the list of values, then it's correct in at least one subdoc\n",
    "        if correct_value in values:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "\n",
    "        else:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False\n",
    "\n",
    "    elif \"Key not found in any subdoc\" in line: # null was predicted in all subdocs\n",
    "        key = re.search(\"Key not found in any subdoc (.*)\", line).group(1)\n",
    "        prediction_stats_dict[-1][\"collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"full_collision_per_key\"][key] = False\n",
    "        prediction_stats_dict[-1][\"num_unified_values_per_key\"][key] = 0\n",
    "\n",
    "        # get the correct value for this key\n",
    "        correct_value = expected.iloc[len(prediction_stats_dict) - 1][key]\n",
    "\n",
    "        if pd.isna(correct_value):\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = True\n",
    "        else:\n",
    "            prediction_stats_dict[-1][\"correct_in_any_subdoc_per_key\"][key] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -> all docs with x number of subdocs should have x predictions for each key (also for the keys that have an empty string prediction\n",
    "# because we added [METADATA]: EMPTY_STRING_PREDICTION to the list of predictions\n",
    "for i in range (len(prediction_stats_dict)):\n",
    "    for key in predictions_dict[i].keys():\n",
    "        assert len(predictions_dict[i][key]) == prediction_stats_dict[i][\"num_subdocs\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_stats = pd.DataFrame(prediction_stats_dict)\n",
    "prediction_stats.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenient Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"correct_in_any_subdoc_percentage\" does not consider the situation where there are no subdocs. In the case that not in any subdocs a value was predicted (all null) we check if the correct solution is in fact null and then consider that in any subdoc the corect value (which is null) was found (in reality it was correctly identified in all). We cannot do the opposite (check if it was null in any subdoc) and then say it was correctly identified (if it was indeed null) because of the subdoc structure where any given subdoc is not guaranteed to have all key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(official) (macro)[average over documents] Lenient accuracy:\", round(prediction_stats[\"correct_in_any_subdoc_percentage\"].mean(), 3))\n",
    "\n",
    "# a single entry is a dictionary with the correctness for each key (True, False, or None)\n",
    "correctness_per_key = prediction_stats[\"correct_in_any_subdoc_per_key\"].tolist()\n",
    "\n",
    "# create empty df\n",
    "lenient_accuracy_by_key = pd.DataFrame()\n",
    "\n",
    "print(\"(official) Correct value found in any subdoc per key:\")\n",
    "for key in correctness_per_key[0].keys():\n",
    "    correctness = [x[key] for x in correctness_per_key]\n",
    "    avg_correctness = np.mean([x for x in correctness if x is not None])\n",
    "    lenient_accuracy_by_key[key] = [avg_correctness]\n",
    "\n",
    "print(lenient_accuracy_by_key)\n",
    "print(f\"(official) (macro)[average over the keys] Lenient accuracy: {round(lenient_accuracy_by_key.mean(axis=1).mean(), 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the relative differences by key between the lenient accuracy and the actual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_key = official_evaluation.mean(axis=0, skipna=True)\n",
    "print(\"(official) Relative improvement in accuracy (lenient vs. strict):\")\n",
    "for key in accuracy_by_key.keys():\n",
    "    print(f\"{key}: {round((lenient_accuracy_by_key[key].values[0] - accuracy_by_key[key]) / accuracy_by_key[key] * 100, 2)}%\")\n",
    "\n",
    "print(\"(macro) (official) Relative improvement in accuracy (lenient vs. strict) (average over the keys):\", round((lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean()) / accuracy_by_key.mean() * 100, 2), \"%\")\n",
    "print(\"(macro) (official) Absolute improvement in accuracy (lenient vs. strict) (average over the keys):\", round(lenient_accuracy_by_key.mean(axis=1).mean() - accuracy_by_key.mean(), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and presence penalty motivation for not looking at those parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check how often the same value for different keys is predicted to see if it's generally correct to assume that values shouldn't be repeated (which could be combated by e.g. increasing the freq. and presence penalties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_values_for_different_keys(row):\n",
    "    return len(row) == len(set(row))\n",
    "\n",
    "num_of_same_value_rows = 0\n",
    "for i in range(len(expected)):\n",
    "    if not has_two_values_for_different_keys(expected.iloc[i]):\n",
    "        num_of_same_value_rows += 1\n",
    "\n",
    "print(f\"Found {num_of_same_value_rows} rows that have the same value for different keys, that's {round(num_of_same_value_rows / len(expected) * 100, 2)}% of the dataset.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, there shouldn't be the same value for different keys.\n",
    "A repetition of the same value would also be correct in the case of a document having more than 1 null value, where a correct generation would contain `\"null\"` multiple times. Let's look at how often this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents that have 2 or more NaN values for any two keys:\")\n",
    "expected[expected.isna().sum(axis=1) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_missing_value_in_solution_pct = round(len(expected[expected.isna().sum(axis=1) >= 2]) / len(expected) * 100, 2)\n",
    "print(f\"Percentage of documents that have 2 or more NaN values for any two keys: {more_than_one_missing_value_in_solution_pct}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generally speaking, we shouldn't have more than 1 null in our generations, so we can now generally assume that repetition of the same value in the generations is incorrect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check how often we have repetition (i.e. the same value (including null's) for different keys) in the (subdoc) predictions. If this is very high then it would make sense to look into tuning the presence and frequency penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions_dict)):\n",
    "    prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"] = {} # this is min 0.125 (1/8) in the case that all predictions are the same (e.g. all 'null') and max 1.0 (all predictions are unique)\n",
    "    prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"] = {} # this is nan if all predictions are null, and min. \n",
    "    for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"]):\n",
    "        # get the predictions for each key\n",
    "        predictions_for_keys = [predictions_dict[i][key][subdoc] for key in predictions_dict[i].keys()]\n",
    "        # get the number of unique predictions\n",
    "        num_unique_predictions = len(set(predictions_for_keys)) # this is at least 1, cannot be 0\n",
    "        # add the percentage of unique predictions to the dictionary\n",
    "        prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] = num_unique_predictions / len(predictions_for_keys)\n",
    "\n",
    "        # get the number of unique predictions ignoring null values\n",
    "        num_unique_predictions_ignore_null = len(set([x for x in predictions_for_keys if x != \"null\"]))\n",
    "\n",
    "        num_predictions_ignore_null = len([x for x in predictions_for_keys if x != \"null\"]) # this is 0 if all the predictions are null\n",
    "        # in this case, we set the number of unique predictions to nan\n",
    "        if num_unique_predictions_ignore_null == 0:\n",
    "            prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = np.nan\n",
    "            continue\n",
    "        # add the percentage of unique predictions (ignoring null) to the dictionary\n",
    "        prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] = num_unique_predictions_ignore_null / num_predictions_ignore_null\n",
    "\n",
    "print(predictions_dict[0])\n",
    "\n",
    "print(\"Average percentage of unique predictions (ignoring null) per subdoc (micro average):\", np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for i in range(len(predictions_dict)) for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "print(\"Average percentage of unique predictions per subdoc (micro average):\", np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for i in range(len(predictions_dict)) for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]))\n",
    "\n",
    "print(\"Average percentage of unique predictions (ignoring null) per subdoc (macro average over the subdocs for each doc):\", np.nanmean([np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc_ignore_null\"][subdoc] for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(predictions_dict))]))\n",
    "print(\"Average percentage of unique predictions per subdoc (macro average over the subdocs for each doc):\", np.nanmean([np.nanmean([prediction_stats_dict[i][\"pct_unique_predictions_per_subdoc\"][subdoc] for subdoc in range(prediction_stats_dict[i][\"num_subdocs\"])]) for i in range(len(predictions_dict))]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to be expected that there is quite some repetition when not ignoring null values because many times the subdoc will only contain few or even none of the keys. When we ignore null values, repetition is almost non-existent, hence we can reasonably assume that tuning the frequency and presence penalty parameters would not improve results by much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the number of non-null values per key to the dataframe\n",
    "prediction_stats.loc[:, \"num_non_null_values_per_key\"] = None\n",
    "for i in range(len(predictions_dict)):\n",
    "    prediction_stats.at[i, \"num_non_null_values_per_key\"] = {key: len([x for x in predictions_dict[i][key] if x != \"null\" and x != \"[METADATA]: EMPTY_STRING_PREDICTION\"]) for key in predictions_dict[i].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_stats.iloc[0]['num_non_null_values_per_key'])\n",
    "print(prediction_stats.iloc[0]['num_unified_values_per_key'])\n",
    "print(predictions_dict[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding no_collision percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column \"no_collision_percentage\" which is the percentage of keys that had no collision\n",
    "prediction_stats.loc[:, \"no_collision_percentage\"] = 1 - prediction_stats.loc[:, \"collision_percentage\"]\n",
    "\n",
    "for i, row in prediction_stats.iterrows():\n",
    "    num_keys = len([x for x in row[\"collision_per_key\"].values() if x is not None])\n",
    "    num_no_collisions = len([x for x in row[\"collision_per_key\"].values() if x is False])\n",
    "    try:\n",
    "        prediction_stats.loc[i, \"no_collision_percentage_calculated\"] = num_no_collisions / num_keys\n",
    "    except ZeroDivisionError:\n",
    "        prediction_stats.loc[i, \"no_collision_percentage_calculated\"] = np.nan\n",
    "\n",
    "assert np.allclose(prediction_stats[\"collision_percentage\"], 1 - prediction_stats[\"no_collision_percentage_calculated\"], equal_nan=True) # sanity check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs_with_subdocs = len(prediction_stats[prediction_stats['num_subdocs'] > 1])\n",
    "print(f\"Number of documents that were split into more than 1 subdocument: {num_of_docs_with_subdocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs_with_no_subdocs = len(prediction_stats[prediction_stats[\"num_subdocs\"] == 1])\n",
    "print(f\"Number of documents that were not split into subdocuments: {num_of_docs_with_no_subdocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = prediction_stats['num_subdocs'].value_counts().sort_index().index\n",
    "y = prediction_stats['num_subdocs'].value_counts().sort_index().values\n",
    "\n",
    "max_subdocs = max(x)\n",
    "\n",
    "# add a 0 for each number of subdocs up to the maximum number of subdocs, so that there is a value for each number of subdocs\n",
    "for i in range(1, max_subdocs+1):\n",
    "    if i not in x:\n",
    "        x = np.append(x, i)\n",
    "        y = np.append(y, 0)\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(0.5, max_subdocs)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments needed\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Histogram of Number of Needed Subdocuments\")\n",
    "\n",
    "# plt.subplots_adjust(top=0.5)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_subdoc_hist.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_values_per_key = {}\n",
    "\n",
    "for i in range(len(prediction_stats)):\n",
    "    for key, value in prediction_stats.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        if key in non_null_values_per_key:\n",
    "            non_null_values_per_key[key].append(value)\n",
    "        else:\n",
    "            non_null_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "occurence_dict = {}\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "# intersperse keys that are missing with value 0\n",
    "for i in range(0, max(occurence_dict.keys())+1):\n",
    "    if i not in occurence_dict:\n",
    "        occurence_dict[i] = 0\n",
    "\n",
    "# sort the dictionary by the key (number of unifications)\n",
    "occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "x = list(occurence_dict.keys())\n",
    "y = list(occurence_dict.values())\n",
    "\n",
    "sns.histplot(x=x, weights=y, discrete=True, ax=ax, kde=True)\n",
    "\n",
    "ax.axvline(x=np.average(x, weights=y), color='red', linestyle='--', label=\"Mean\")\n",
    "\n",
    "# median calculated manually\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.extend([x[i]] * y[i])\n",
    "\n",
    "ax.axvline(x=np.median(z), color='green', linestyle='--', label=\"Median\")\n",
    "\n",
    "ax.set(xticks=x)\n",
    "ax.set_xlim(-0.5, max(x))\n",
    "ax.set(xlabel=\"Number of non-null values\")\n",
    "ax.set_title(\"Histogram of Number of Non-Null Values per Key\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_non_null_values_vs_occurence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_values_per_key = {}\n",
    "\n",
    "for i in range(len(prediction_stats)):\n",
    "    for key, value in prediction_stats.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        if key in non_null_values_per_key:\n",
    "            non_null_values_per_key[key].append(value)\n",
    "        else:\n",
    "            non_null_values_per_key[key] = [value]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# one key = one bar plot (histogram)\n",
    "max_val = 0\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    occurence_dict = {}\n",
    "    for value in values:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "\n",
    "    # intersperse keys that are missing with value 0\n",
    "    for i in range(0, max(occurence_dict.keys())+1):\n",
    "        if i not in occurence_dict:\n",
    "            occurence_dict[i] = 0\n",
    "\n",
    "    # sort the dictionary by the key (number of unifications)\n",
    "    occurence_dict = dict(sorted(occurence_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "    x = list(occurence_dict.keys())\n",
    "    y = list(occurence_dict.values())\n",
    "    \n",
    "    ax = sns.scatterplot(x=x, y=y, ax=ax, label=key)\n",
    "    max_val = max(max_val, max(x))\n",
    "\n",
    "ax.set(xlabel=\"Number of non-null values\", ylabel=\"Count\")\n",
    "\n",
    "# set the x-ticks to be integers\n",
    "ax.set_xticks(np.arange(0, max_val+1, 1))\n",
    "ax.legend()\n",
    "ax.set_title(\"Number of Non-Null Values by Key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_non_null_values_by_key_vs_count.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(prediction_stats['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "y = [] # number of non-null values per key (average over all keys)\n",
    "\n",
    "y = [[] for _ in range(max(x)+1)]\n",
    "\n",
    "for i in range(len(prediction_stats)):\n",
    "    num_subdocs = prediction_stats.iloc[i]['num_subdocs']\n",
    "\n",
    "\n",
    "    num_non_null_values = 0\n",
    "    num_keys = 0\n",
    "    for key, value in prediction_stats.iloc[i]['num_non_null_values_per_key'].items():\n",
    "        num_non_null_values += value\n",
    "        num_keys += 1\n",
    "    if num_keys > 0:\n",
    "        y[num_subdocs].append(num_non_null_values / num_keys)\n",
    "    else:\n",
    "        y[num_subdocs].append(0)\n",
    "\n",
    "y = [np.mean(y[i]) for i in range(len(y))] # average number of non-null values per key for each number of subdocuments\n",
    "\n",
    "ax.plot([0, max(x) + 1], [0, max(x) + 1], color='black', linestyle='--', label='Upper bound')\n",
    "\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line')\n",
    "\n",
    "ax.set(xticks=x, yticks=x)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(right=max_subdocs+0.5)\n",
    "ax.set_ylim(top=max_subdocs+0.5)\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of non-null values per key\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title(\"Number of Subdocuments vs. Average Number of Non-Null Values per Key\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_num_subdocs_vs_non_null_values_per_key.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for key, values in non_null_values_per_key.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(values)/len(values))\n",
    "\n",
    "# round to 2 decimal places\n",
    "y = [round(y[i], 2) for i in range(len(y))]\n",
    "\n",
    "# create a table with the keys and the average number of non-null values per key\n",
    "key_stats = pd.DataFrame({'key': x, 'avg_num_of_non_null_values': y})\n",
    "\n",
    "print(key_stats) # avg_num_of_non_null_values is the average number of non-null values per key\n",
    "print(lenient_accuracy_by_key) # lenient_accuracy_by_key is the accuracy for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "ax.axhline(y=0, color=\"green\", linestyle=\"--\", label=\"Best case (every unification is trivial)\")\n",
    "\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"collision_percentage\", data=prediction_stats, ax=ax, label=\"Collision percentage\")\n",
    "ax = sns.lineplot(x=\"num_subdocs\", y=\"full_collision_percentage\", data=prediction_stats, ax=ax, label=\"Full collision percentage\")\n",
    "\n",
    "avg_collision_percentage = np.nanmean(prediction_stats['collision_percentage'])\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(prediction_stats['full_collision_percentage'])\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set_xlim(left=1, right=max(prediction_stats['num_subdocs']))\n",
    "ax.set(xticks=np.arange(1, max_subdocs+1, 1))\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Average number of collisions per key\", title=\"Average Number of Collisions per Key vs. Number of Subdocuments\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_collisions_wrt_subdocs_hist.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 subdocument == no subdocs (or in other words: the 1 subdoc is the whole document)\n",
    "\n",
    "Note: best case assumes:\n",
    "* perfect OCR\n",
    "* no mistakes in the reports (no typos, no conflicting information on different pages)\n",
    "\n",
    "of course with num_subdocs=2 every collision is a full_collision :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.axhline(y=1, color=\"red\", linestyle=\"--\", label=\"Worst case (every unification has a collision)\")\n",
    "\n",
    "collision_percentage_by_key = {}\n",
    "full_collision_percentage_by_key = {}\n",
    "\n",
    "# line plot of the collision percentage by key\n",
    "for key in prediction_stats['num_unified_values_per_key'].iloc[0].keys():\n",
    "    for i in range(len(prediction_stats)):\n",
    "        if key not in collision_percentage_by_key:\n",
    "            collision_percentage_by_key[key] = []\n",
    "\n",
    "        if key not in full_collision_percentage_by_key:\n",
    "            full_collision_percentage_by_key[key] = []\n",
    "\n",
    "        collision_percentage_by_key[key].append(prediction_stats['collision_per_key'].iloc[i][key])\n",
    "        full_collision_percentage_by_key[key].append(prediction_stats['full_collision_per_key'].iloc[i][key])\n",
    "\n",
    "# filter out None values\n",
    "collision_percentage_by_key = {key: [x for x in collision_percentage_by_key[key] if x is not None] for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: [x for x in full_collision_percentage_by_key[key] if x is not None] for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# calculate average and ignore nan values and round to 2 decimal places\n",
    "collision_percentage_by_key = {key: round(np.nanmean(collision_percentage_by_key[key]), 2) for key in collision_percentage_by_key.keys()}\n",
    "full_collision_percentage_by_key = {key: round(np.nanmean(full_collision_percentage_by_key[key]), 2) for key in full_collision_percentage_by_key.keys()}\n",
    "\n",
    "# add those values to the dataframe\n",
    "key_stats['collision_percentage'] = key_stats['key'].map(collision_percentage_by_key)\n",
    "key_stats['full_collision_percentage'] = key_stats['key'].map(full_collision_percentage_by_key)\n",
    "\n",
    "# create a barplot that has grouped bars\n",
    "\n",
    "# we want to use the \"hue\" parameter to group the bars by collision vs. full collision\n",
    "# thus we have to transform the dataframe to have a column for each of the two types of collisions\n",
    "# and a column for the key\n",
    "key_stats_trf = key_stats.melt(id_vars=['key'], value_vars=['collision_percentage', 'full_collision_percentage'], var_name='collision_type', value_name='collision_pct')\n",
    "\n",
    "# rename the collision types to something more readable\n",
    "key_stats_trf['collision_type'] = key_stats_trf['collision_type'].map({'collision_percentage': 'Collision percentage', 'full_collision_percentage': 'Full Collision percentage'})\n",
    "\n",
    "# create the barplot\n",
    "ax = sns.barplot(x=\"key\", y=\"collision_pct\", hue=\"collision_type\", data=key_stats_trf, ax=ax)\n",
    "\n",
    "avg_collision_percentage = np.nanmean(list(collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_collision_percentage, color=\"black\", linestyle=\"-\", label=\"Average collision percentage\")\n",
    "\n",
    "avg_full_collision_percentage = np.nanmean(list(full_collision_percentage_by_key.values()))\n",
    "ax.axhline(y=avg_full_collision_percentage, color=\"black\", linestyle=\"-.\", label=\"Average full collision percentage\")\n",
    "\n",
    "ax.set(xlabel=\"Key\", ylabel=\"(Full) collision percentage\", title=\"Collision Percentage by Key\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_collision_percentage_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the part of collisions that are full collisions\n",
    "key_stats['full_collision_over_collision'] = round(key_stats['full_collision_percentage'] / key_stats['collision_percentage'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_evaluation_by_key = own_evaluation.mean(axis=0, skipna=True)\n",
    "print(own_evaluation_by_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lenient_accuracy_by_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the correlation between the avg_num_of_non_null_values and the collision percentage?\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and collision percentage: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], key_stats['collision_percentage'])[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "# what's the correlation between the avg_num_of_non_null_values and the lenient accuracy of the key?\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], lenient_accuracy_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the avg_num_of_non_null_values and our own evaluation of the accuracy?\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (own) accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], own_evaluation_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the avg_num_of_non_null_values and the official accuracy of the key?\n",
    "print(\"Correlation coefficient between avg_num_of_non_null_values and (official) accuracy: \" + str(round(np.corrcoef(key_stats['avg_num_of_non_null_values'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "# what's the correlation between the collision_percentage and the lenient accuracy of the key?\n",
    "print(\"Correlation coefficient between collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], lenient_accuracy_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the collision_percentage and our own evaluation of the accuracy?\n",
    "print(\"Correlation coefficient between collision percentage and (own) accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], own_evaluation_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the collision_percentage and the official accuracy of the key?\n",
    "print(\"Correlation coefficient between collision percentage and (official) accuracy: \" + str(round(np.corrcoef(key_stats['collision_percentage'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")\n",
    "\n",
    "# what's the correlation between the full_collision_percentage and the lenient accuracy of the key?\n",
    "print(\"Correlation coefficient between full collision percentage and (official) lenient accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], lenient_accuracy_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the full_collision_percentage and our own evaluation of the accuracy?\n",
    "print(\"Correlation coefficient between full collision percentage and (own) accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], own_evaluation_by_key)[0, 1], 3)))\n",
    "# what's the correlation between the full_collision_percentage and the official accuracy of the key?\n",
    "print(\"Correlation coefficient between full collision percentage and (official) accuracy: \" + str(round(np.corrcoef(key_stats['full_collision_percentage'], accuracy_by_key)[0, 1], 3)))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataframe key_stats with a heatmap\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# create a heatmap ignore 'key' column but use 'key' column as row labels\n",
    "sns.heatmap(key_stats.iloc[:, 2:], annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax)\n",
    "\n",
    "# set the labels\n",
    "ax.set(xlabel=\"Metric\", ylabel=\"Key\", title=\"Collision statistics by key\")\n",
    "\n",
    "# rename the x-labels from 'collision_percentage' to 'Collision percentage', from 'full_collision_percentage' to 'Full collision percentage' and from 'full_collision_over_collision' to 'Full collision over collision'\n",
    "ax.set_xticklabels(['Collision Percentage', 'Full Collision Percentage', '% Full Collision of Collision'])\n",
    "\n",
    "# set the yticks to the key names\n",
    "ax.set_yticklabels(key_stats['key'], rotation=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save the plot\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_collision_stats_by_key.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the own_evaluation.mean(axis=1, skipna=True) to the prediction_stats dataframe\n",
    "prediction_stats['own_evaluation_accuracy'] = own_evaluation.mean(axis=1, skipna=True)\n",
    "prediction_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the own_evaluation accuract vs num_subdocs using sns\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "max_subdocs = max(prediction_stats['num_subdocs'])\n",
    "\n",
    "x = np.arange(0, max_subdocs+1)\n",
    "y = [] # avg. accuracy for each number of subdocs\n",
    "# yerr = [] # standard deviation for each number of subdocs\n",
    "\n",
    "# scatter plot of the avg. own_evaluation accuracy vs num_subdocs\n",
    "for i in range(0, max_subdocs+1):\n",
    "    y.append(prediction_stats[prediction_stats['num_subdocs'] == i]['own_evaluation_accuracy'].mean())\n",
    "    #yerr.append(prediction_stats[prediction_stats['num_subdocs'] == i]['own_evaluation_accuracy'].std())\n",
    "\n",
    "# scatter plot with error bars\n",
    "#ax.errorbar(x, y, yerr=yerr, fmt='o', ecolor='red', capsize=5, elinewidth=2, capthick=2)\n",
    "\n",
    "# scatter plot without error bars\n",
    "ax = sns.scatterplot(x=x, y=y, ax=ax)\n",
    "\n",
    "sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', label='Regression line')\n",
    "\n",
    "# remove nan values from x and y\n",
    "x = [x[i] for i in range(len(x)) if not np.isnan(y[i])]\n",
    "y = [y[i] for i in range(len(y)) if not np.isnan(y[i])]\n",
    "\n",
    "# set the labels\n",
    "ax.set(xlabel=\"Number of subdocuments\", ylabel=\"Accuracy\", title=\"Accuracy by number of subdocuments\")\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(\"Number of Subdocuments vs. Accuracy (own evaluation)\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"plots/{SPLIT}_{MODEL_NAME}_{TEMPERATURE}_own_eval_accuracy_vs_num_subdocs.png\", dpi=300)\n",
    "\n",
    "print(\"Correlation coefficient: \" + str(round(np.corrcoef(x, y)[0, 1], 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "In order to contextualize the performance of the baselines (general and specific), we want to check how often the key search (with the given fuzziness) yields any result. In case of the general baseline, if no match is found then no key-value is extracted, in case of the specific baseline there is the addition of synonyms for some of the keys which increases the likelihood of finding a key in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(\"datasets/kleister_charity/dev-0/in_extended.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_key_to_gold_key = {\n",
    "    \"Address (post code)\": \"address__postcode\",\n",
    "    \"Address (street)\": \"address__street_line\",\n",
    "    \"Address (post town)\": \"address__post_town\",\n",
    "    \"Charity Name\": \"charity_name\",\n",
    "    \"Charity Number\": \"charity_number\",\n",
    "    \"Annual Income\": \"income_annually_in_british_pounds\",\n",
    "    \"Period End Date\": \"report_date\",\n",
    "    \"Annual Spending\": \"spending_annually_in_british_pounds\",\n",
    "}\n",
    "prompt_keys = list(prompt_key_to_gold_key.keys())\n",
    "gold_keys = list(prompt_key_to_gold_key.values())\n",
    "\n",
    "# for specific baseline we also use these synonyms\n",
    "synonyms = {\n",
    "    \"Charity Name\": [\"Charity Name\"],\n",
    "    \"Charity Number\": [\n",
    "        \"Charity Number\",\n",
    "        \"Charity Registration No\",\n",
    "        \"Charity No\",\n",
    "    ],\n",
    "    \"Annual Income\": [\"Annual Income\", \"Income\", \"Total Income\"],\n",
    "    \"Period End Date\": [\"Period End Date\", \"Period End\", \"Year Ended\"],\n",
    "    \"Annual Spending\": [\n",
    "        \"Annual Spending\",\n",
    "        \"Spending\",\n",
    "        \"Total Spending\",\n",
    "        \"Expenditure\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create dict that will record how often the respective key was actually found\n",
    "key_to_count = {key: 0 for key in prompt_keys}\n",
    "\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has to match the respective configuration of the baseline model that is evaluated\n",
    "type_of_baseline = \"specific\" # or \"specific\" \n",
    "error_percentage = 0.18\n",
    "\n",
    "def get_best_match_span(text: str, key: str):\n",
    "    \"\"\"\n",
    "    Returns the best match for the key in the text with some fuzziness\n",
    "    (i.e. we limit the levenshtein distance) of the best match.\n",
    "\n",
    "    (?b) -> BESTMATCH\n",
    "    (?i) -> IGNORECASE\n",
    "    {e<n} -> up to n errors (subs, inserts, dels). if more -> None\n",
    "    (1) -> the span of the best match\n",
    "    \"\"\"\n",
    "    key_length = len(key)\n",
    "    max_errors = round(key_length * error_percentage)\n",
    "    match_span = regex.search(f\"(?b)(?i)({key}){{e<{max_errors}}}\", text)\n",
    "\n",
    "    if match_span:\n",
    "        return match_span.span(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(in_df)):\n",
    "    text = in_df.loc[in_df[\"filename\"] == in_df.iloc[i][\"filename\"], \"text_best_cleaned\"].values[0]\n",
    "\n",
    "    if type_of_baseline == \"general\":\n",
    "        # check for a match for each key\n",
    "        for i, key in enumerate(prompt_keys):\n",
    "            total += 1\n",
    "            match_span = get_best_match_span(text, key)\n",
    "\n",
    "            if match_span is None:\n",
    "                continue\n",
    "            else:\n",
    "                key_to_count[key] += 1\n",
    "    \n",
    "    elif type_of_baseline == \"specific\":\n",
    "        # check for a match for each synonym\n",
    "        for i, key in enumerate(list(synonyms.keys())):\n",
    "            total += 1\n",
    "            for synonym in synonyms[key]:\n",
    "                match_span = get_best_match_span(text, synonym)\n",
    "    \n",
    "                if match_span is None: # no match for this synonym\n",
    "                    continue\n",
    "                else:\n",
    "                    key_to_count[key] += 1 # found a match for this key\n",
    "                    break # no need to check the other synonyms for this key as we are only interested if any of them matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_of_baseline == \"specific\":\n",
    "    num_keys_considered = len(synonyms.keys())\n",
    "\n",
    "    # remove the keys that we don't use synonyms for\n",
    "    for key in prompt_keys:\n",
    "        if key not in synonyms.keys():\n",
    "            key_to_count.pop(key)\n",
    "else:\n",
    "    num_keys_considered = len(prompt_keys)\n",
    "\n",
    "\n",
    "key_to_count_percentage = {key: count / total * num_keys_considered * 100 for key, count in key_to_count.items()}\n",
    "print(key_to_count_percentage)\n",
    "\n",
    "# macro average\n",
    "print(f\"(macro)[over keys] average Percentage of how often the keys can be found in the docoument: {sum(key_to_count_percentage.values()) / len(key_to_count_percentage)}\")\n",
    "\n",
    "# micro average\n",
    "print(f\"(micro)[over all predictions] average Percentage of how often the keys can be found in the document: {sum(key_to_count.values()) / total * 100}\")\n",
    "\n",
    "# they are the same which makes sense because all classes (keys) are checked the same amount of times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('uni-kie-JrmAaldC-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec8707b55c29234c829cd46c92f0adfa2b741d49905cfffb1cd22fea1c1c224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
